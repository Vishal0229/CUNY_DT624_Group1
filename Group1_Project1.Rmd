---
title: "CUNY DT 624"
author: 'Group 1: Avraham Adler, Vishal Arora, Samuel Bellows, Austin Chan'
date: "Summer 2020"
output:
  word_document:
    toc: yes
    toc_depth: 4
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
subtitle: Project 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readxl)
library(openxlsx)
library(fpp2)
library(urca)
library(ggplot2)
library(scales)
library(data.table)
library(dplyr)
library(imputeTS)
library(MLmetrics)


# Read in Data
DT <- read_xls('./project1data/Data Set for class.xls')
# Set as data table
setDT(DT)
# Separate out groups
n <- length(unique(DT$group))
for (i in seq_len(n)) {
  gName <- paste0('S0', i)
  assign(gName, DT[group == gName])
}

# Following is useful to quickly remove "future" periods from groups
futureobs <- 1623:1762

arimaDisplay <- function(arimaobject) {
  paste0("ARIMA(",
         arimaorder(arimaobject)[["p"]],
         ",",
         arimaorder(arimaobject)[["d"]],
         ",",
         arimaorder(arimaobject)[["q"]],
         ")")
}
```

# Introduction
The purpose of this report is to describe the analysis performed in order to
forecast the following series, each for 140 periods.

 * S01: Forecast Var01 and Var02
 * S02: Forecast Var02 and Var03
 * S03: Forecast Var05 and Var07
 * S04: Forecast Var01 and Var02
 * S05: Forecast Var02 and Var03
 * S06: Forecast Var05 and Var07
 
The forecasts will be provided in a separate Excel document. The code for the
analysis can be found in the attached R markdown document.

The **executive summary** will contain the model selections and forecasts. The
detailed analysis may be found in the **analytical appendix** for those
interested in the findings and decisions applied. The `R` code will be made
available upon request.

# Executive Summary
## Series 1
### Observations
Variables 01 and 02 (V1 & V2 from now on) of Series 01 (S1 from now on) can each
be considered a time series. As few transformations as possible will be applied
to the data to maximize fidelity to the original source.
```{r S1ts, echo=FALSE}
S1ts <- ts(S01[-futureobs, 3:4])
plot(S1ts)
```

Clearly V1 and V2 are negatively correlated with each other, as V1 has an
increasing secular trend whereas V2 has an overall decreasing one. This is even
more apparent when they are plotted after center-scaling and normalization.
```{r S1scaleV, echo=FALSE, fig.width=8L, fig.height=6L}
V1 <- scale(S01$Var01[-futureobs])
V2 <- scale(S01$Var02[-futureobs])
plot(V2, type = 'l', col = 'green4', main = "Series 01 Variables",
     xlab = "Time", ylab = "Scaled Values", ylim = c(-2, 8))
lines(V1, col = 'blue')
legend('topright', legend = c('V1', 'V2'), col = c('blue', 'green4'), lty = 1L)
```
```{r SeriesFitting, include=FALSE}
S1V1 <- na.interp(S1ts[, 1])
S1V2 <- S1ts[, 2]
S1V1O <- tsoutliers(S1V1)
S1V2O <- tsoutliers(S1V2)
S1V2c <- tsclean(S1V2)
AAS1V1 <- auto.arima(S1V1, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                     approximation = FALSE, seasonal = FALSE, parallel = TRUE)
AAS1V2 <- auto.arima(S1V2, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                     approximation = FALSE, seasonal = FALSE, parallel = TRUE)
AAS1V1l <- auto.arima(S1V1, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                     approximation = FALSE, seasonal = FALSE, parallel = TRUE,
                     lambda = 'auto', biasadj = TRUE)
AAS1V2l <- auto.arima(S1V2, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                      approximation = FALSE, seasonal = FALSE, parallel = TRUE,
                      lambda = 'auto', biasadj = TRUE)
ETSS1V1 <- ets(S1V1, model = 'ZZZ', ic = 'aicc')
ETSS1V1l <- ets(S1V1, model = 'ZZZ', ic = 'aicc', lambda = 'auto', biasadj = TRUE)
ETSS1V2 <- ets(S1V2, model = 'ZZZ', ic = 'aicc')
ETSS1V2l <- ets(S1V2, model = 'ZZZ', ic = 'aicc', lambda = 'auto', biasadj = TRUE)
```

### Model Selection
Exponentially smoothed and ARIMA models were reviewed, with details in the
appendix. The selected model for V1 is an `r arimaDisplay(AAS1V1l)` with drift
and a Box-Cox \(\lambda\) of `r AAS1V1l$lambda[[1]]`. For V2 it is an
`r arimaDisplay(AAS1V2l)` with a Box-Cox \(\lambda\) of `r AAS1V2l$lambda[[1]]`.

### Forecasts
The mean forecast values will be supplied in the attached Excel workbook. The
mean forecasts with error bands are plotted below
```{r S1forecastplots, echo=FALSE}
autoplot(forecast(AAS1V1l, h = 140)) + ylab("V1")
autoplot(forecast(AAS1V2l, h = 140)) + ylab("V2")
```
```{r S1Final, include=FALSE}
S1Final <- data.table(Var01 = forecast(AAS1V1l, h = 140)$mean,
                 Var02 = forecast(AAS1V2l, h = 140)$mean)
```

## Series 2            

### Statistical & Exploratory Analysis of series S02            
```{r S2extrVar, echo=FALSE}
S2 <- ts(S02[-futureobs, 4:5])

S2v1 <- ts(S2[-futureobs,1])
S2v2 <- ts(S2[-futureobs,2])

summary(S2v1)
summary(S2v2)
```

Var02 has no missing values and Var03 has 4 missing values.        

### Series Visualization

```{r}
autoplot(S2v1)
autoplot(S2v2)
```

### Applying Models 

#### Using Holt's method for Var02.                 

```{r S2outliRep02-run, echo=FALSE, include = F}

# Replacing outlier using linear interpolation on a time series

S02v2C <- tsclean(S2v1)
S02v3C <- S2v2
```

```{r S2v2HoltModel, echo=FALSE, fig.width=8L, fig.height=6L}
fc2 <- holt(S02v2C, damped=TRUE, h=140)

autoplot(S02v2C) +
  autolayer(fc2, series="Damped Holt's method V02", PI=FALSE) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Holt's Damped Method Var02") +
  guides(colour=guide_legend(title="Forecast"))

checkresiduals(fc2)

autoplot(forecast(fc2))

```

#### Applying Holt's method for Var03.                         

```{r S2v3HoltModel, echo=FALSE, fig.width=8L, fig.height=6L}
# VAr03
fc3 <- holt(S02v3C, damped=TRUE, h=140)
autoplot(S02v3C) +
  autolayer(fc3, series="Damped Holt's method V03", PI=FALSE) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Holt's Damped Method Var03") +
  guides(colour=guide_legend(title="Forecast"))
checkresiduals(fc3)
autoplot(forecast(fc3))

```

#### Applying ETS model to Var02
```{r S2v2ETSModel, echo=FALSE, fig.width=8L, fig.height=6L}
#Var 02
fit_ets <- ets(S02v2C)
autoplot(fit_ets)
checkresiduals(fit_ets)
fit_ets %>% forecast(h=140) %>%
  autoplot() +
  ylab("Forecast for Var02")

```

#### Applying ETS model to Var03
```{r S2v3ETSModel, echo=FALSE, fig.width=8L, fig.height=6L}
#Var 03
fit_ets_V03 <- ets(S02v3C)
autoplot(fit_ets_V03)
checkresiduals(fit_ets_V03)
fit_ets_V03 %>% forecast(h=140) %>%
  autoplot() +
  ylab("Forecast for Var03")

```


#### Applying ARIMA Model to Var02.
```{r S2v2ArimaModel, echo=FALSE, fig.width=8L, fig.height=6L}
kpss_fit <-ur.kpss(S02v2C)
summary(kpss_fit)


diff_kpss_fit <- ur.kpss(diff(S02v2C))
summary(diff_kpss_fit)

ndiffs(S02v2C)


fit4 <- auto.arima(diff(diff(S02v2C)),lambda = NULL)
checkresiduals(fit4)

autoplot(forecast(fit4, h=140))

```
The test statistic is much bigger than the 1% critical value, indicating that the
null hypothesis is rejected. That is, the data are not stationary. We can difference
the data, and apply the test again.After doing the differencing the critical value is
less than 1% which clearly indicates the data is stationary .                     

#### Arima model for Var 03
```{r S2v3ArimaModel, echo=FALSE, fig.width=8L, fig.height=6L}
kpss_fit_V03 <-ur.kpss(S02v3C)
summary(kpss_fit_V03)

arimafit_var03 <- auto.arima(diff(S02v3C), lambda=NULL)
checkresiduals(arimafit_var03)
autoplot(forecast(arimafit_var03, h=140))

```
The test statistic is much bigger than the 1% critical value, indicating that the null
hypothesis is rejected. That is, the data are not stationary. We can difference the data,
and apply the test again.After doing the differencing the critical value is less than 
1% which clearly indicates the data is stationarynow for implementing the Arima model.               

#### MAPE Calculation:

```{r S2v2v3MAPE, echo=FALSE, fig.width=8L, fig.height=6L}
print(paste0("Accuracy for Var 02"))
print(paste0("MAPE for S02 Var02 using Holt's winter model :::   ", MLmetrics::MAPE(fc2$fitted,S02v2C)))
print(paste0("MAPE for S02 Var02 using ETS model           :::   ",  MLmetrics::MAPE(fit_ets$fitted,S02v2C)))
print(paste0("MAPE for S02 Var02 using Auto ARIMA model    :::   ",  MLmetrics::MAPE(fit4$fitted,S02v2C)))

print(paste0("Accuracy for Var 03"))
print(paste0("MAPE for S03 Var03 using Holt's winter model  :::   ", MLmetrics::MAPE(fc3$fitted,S02v3C)))
print(paste0("MAPE for S03 Var03 using ETS model           :::   ", MLmetrics::MAPE(fit_ets_V03$fitted,S02v3C)))
print(paste0("MAPE for S03 Var03 using Auto ARIMA model    :::   ", MLmetrics::MAPE(arimafit_var03$fitted,S02v3C)))

```

Looking at the error(MAPE) for various model, the Holt model is best fit model for 
Var02 variables.And for Var03 ETS model is the best fit.                  

Also looking at the residuals for both models , we can see that both the Holt and
ETS model  for thier respective variables is having constant variance and normal 
distrubution and also residuals are uncorrelated with nearly zero mean.The mean of
the residuals is close to zero and there is no significant correlation in the residuals
series. Also from portmanteau test (Ljung-Box test) both Holt &  ETS method has the
least p-value , thus we can reject the NULL hypothesis i.e. residuals are clearly 
distinguishable from white noise and hence our residuals are free from data information
which if present should have been used for computing forecasts.And also in case var03 
the residuals are uncorrelated and nearly zero.               



```{r S2Final, include=FALSE}

S2Final <- data.table(Var02 = forecast(fc2,h=140)$mean,
                      Var03 = forecast(fit_ets_V03, h=140)$mean)
```


## Series 3
### Series Visualization
```{r s3}
S3 <- ts(S03[-futureobs, 6:7])
autoplot(S3) + ylab('Value') + xlab('Time') + ggtitle('Var05 vs Var07')
```

The two series of interest appear to be extremely similar, although one of the
series appears to be slightly more volatile than the other. Likely the forecasts
for these two series will be almost identical. We can also see on the graph what
appears to be missing values towards the very end of the series.

The data appear to have a strong trend but no clear seasonal pattern, which will
aid us in our modeling in the future.

```{r s3NA-exec, echo=F, include=F}
sum(is.na(S3))

sum(is.na(S3[,1]))
sum(is.na(S3[,2]))

S3 <- ts(sapply(S3, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S3))
```

### Baseline Model
Since the data has a clear trend component but no seasonality component, a
simple random walk with drift will serve as a good baseline model.

```{r s3var1-drift, echo = F}
s3v1 <- S3[,1]
s3v2 <- S3[,2]

rwf(s3v1, h = 140, drift = T) %>% autoplot() + ylab('Var05')
rwf(s3v2, h = 140, drift = T) %>% autoplot() + ylab('Var07')
```

The cumulative error for our baseline model is 20.09, which is meaningless in and
of itself but will be an excellent comparison point for other models.

### Exponential Smoothing model

Due the the nature of the data, a linear trend model is a natural choice as an
attempt to model the data.

```{r, echo = F}
fit <- ets(s3v1, model = 'AAN', damped = F)
fit %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

fit2 <- ets(s3v2, model = 'AAN', damped = F)
fit2 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

Exponential Smoothing models achieved an error of 20.05, a slight improvement on
the baseline model.

### ARIMA Model

The most appropriate ARIMA model is an ARIMA(1,1,0) model as the data is clearly
not stationary and needs to be differenced. This choice of model parameters can
be confirmed both by data visualization and by the auto arima function in R.

```{r, echo = F}
# data indicates an AR 1 model from ACF and PACF
arima_s3v1 <- Arima(s3v1, order = c(1,1,0), include.drift = T)
arima_s3v1 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

arima_s3v2 <- Arima(s3v2, order = c(1,1,0), include.drift = T)
arima_s3v2 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

The ARIMA model has an error of 19.98, the lowest of the 3 models. All 3 models
make very similar looking positions so we will select the ARIMA model as it has
the lowest error across the validation period.

```{r S3Final, include=FALSE}
S3Final <- data.table(Var05 = forecast(arima_s3v1, h = 140)$mean,
                      Var07 = forecast(arima_s3v2, h = 140)$mean)
```
## Series 4
### Series Visualization

```{r echo=FALSE}
S4ts <- ts(S04[-futureobs, 3:4])
plot(S4ts)
```

The graphs above show both series plotted next to each other. According to the
graphs, the series look quite different from each other. The first series
appears to be gradually increasing upward as time increases and then begins to
trend downward after a certain point. This series is not stationary, so some
differencing will be used to create a stationary series. Beyond the
non-stationarity, there does not seem to be any unusual values in the first
series.

The second series appears to be stationary with many large spikes at somewhat
random intervals. These large spikes will most likely have an effect on the
forecasting and should be adjusted for more accurate predictions.

```{r echo=FALSE, fig.width=8L, fig.height=6L}
V1 <- scale(S04$Var01[-futureobs])
V2 <- scale(S04$Var02[-futureobs])
plot(V2, type = 'l', col = 'green4', main = "Series 01 Variables",
     xlab = "Time", ylab = "Scaled Values", ylim = c(-2, 8))
lines(V1, col = 'blue')
legend('topright', legend = c('V1', 'V2'), col = c('blue', 'green4'), lty = 1L)
```

The graph above shows both series overlayed. There does not appear to be an
obvious correlation or pattern between these two series.

### Data Transformation
#### Missing Values
The first series exhibits a couple missing values. The `na.interp` times series
interpolation function replaces the missing values with numbers that match the
pattern of the overall series.

The second series had no missing values.

#### Outliers
The first series exhibited a few outliers, but they were not significant enough
to warrant replacement; so no outlier transformations were performed.

The second series exhibited many outliers that were quite extreme. These
outliers were replaced with more reasonable values using the `tsclean` function.

#### Variance Stabilization
Both series did not exhibit a changes in variance, so a Box-Cox transformation
was deemed unnecessary.

#### Stationarity

A first order difference was applied to both series to induce stationarity. The
differencing significantly improved the stationarity for both series to a
satisfactory level so no further differencing was applied.

#### Seasonality

After differencing both series, neither series exhibited seasonality.

```{r include=FALSE}
sum(is.na(S4ts[,1])) # 2 missing values
sum(is.na(S4ts[,2])) # no missing values

S4V1 = na.interp(S4ts[,1]) #interpolate missing values
S4V1_outliers = tsoutliers(S4V1) #identify outliers

S4V2 = S4ts[,2]
S4V2_outliers = tsoutliers(S4V2) #identify outliers
S4V2_clean = tsclean(S4V2) #replace outliers with more reasonable values
```

```{r include=FALSE}
S4V1_autoarima = auto.arima(S4V1, ic = 'aicc', stepwise = F, approximation = F,
                            allowdrift = F, seasonal = F, parallel = T)
S4V1_boxed_autoarima = auto.arima(S4V1, ic = 'aicc', stepwise = F,
                                  approximation = F, allowdrift = F,
                                  seasonal = F, parallel = T, lambda = 'auto')

S4V2_autoarima = auto.arima(S4V2, ic = 'aicc', stepwise = F, approximation = F,
                            allowdrift = F, seasonal = F, parallel = T)
S4V2_outliers_autoarima = auto.arima(S4V2_clean, ic = 'aicc', stepwise = F,
                                     approximation = F, allowdrift = F,
                                     seasonal = F, parallel = T)
S4V2_boxed_autoarima = auto.arima(S4V2, ic = 'aicc', stepwise = F,
                                  approximation = F, allowdrift = F,
                                  seasonal = F, parallel = T, lambda = 'auto')
S4V2_boxed_outliers_autoarima = auto.arima(S4V2_clean, ic = 'aicc',
                                           stepwise = F, approximation = F,
                                           allowdrift = F, seasonal = F,
                                           parallel = T, lambda = 'auto')
```

#### Forecasting
The forecast for both series can be seen below.

```{r, echo=FALSE}
autoplot(forecast(S4V1_boxed_autoarima, h = 140), main = "Forecast for V1")
autoplot(forecast(S4V2_boxed_outliers_autoarima, h = 140, biasadj = TRUE),
     main = "Forecast for V2")
```
```{r S4Final, include=FALSE}
S4Final <- data.table(Var01 = forecast(S4V1_boxed_autoarima, h = 140)$mean,
                      Var02 = forecast(S4V2_boxed_outliers_autoarima,
                                       h = 140)$mean)
```
## Series 5
### Observations
Variables 02 and 03 (V2 & V3 from now on) of Series 05 (S5 from now on) can each
be considered a time series. As few transformations as possible will be applied
to the data to maximize fidelity to the original source.
```{r S5ts, echo=FALSE}
S5ts <- ts(S05[-futureobs, 4:5])
plot(S5ts, main = "Series 5")
```

There is no immediately apparent relationship between V2 and V3. Both seem to
be without drift. V3, in particular, looks very much like a random walk at first
glance. This remains the case when they are plotted after center-scaling and
normalization. V2 does exhibit some extreme variability and may be a candidate
for Box-Cox transformation.
```{r S5scaleV, echo=FALSE, fig.width=8L, fig.height=6L}
V2 <- scale(S05$Var02[-futureobs])
V3 <- scale(S05$Var03[-futureobs])
plot(V2, type = 'l', col = 'green4', main = "Series 05 Variables",
     xlab = "Time", ylab = "Scaled Values", ylim = c(-3, 12))
lines(V3, col = 'blue')
legend('topright', legend = c('V3', 'V2'), col = c('blue', 'green4'), lty = 1L)
```
```{r S5Fitting, include=FALSE}
S5V3missing <- which(is.na(S5ts[, 2]))
S5V3med <- median(S5ts[, 2], na.rm = TRUE)
S5V2 <- na.interp(S5ts[, 1])
S5V3 <- na.interp(S5ts[, 2])
S5V2O <- tsoutliers(S5V2)
S5V3O <- tsoutliers(S5V3)
AAS5V2 <- auto.arima(S5V2, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE)
AAS5V3 <- auto.arima(S5V3, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE)
AAS5V2l <- auto.arima(S5V2, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE,
                   lambda = 'auto', biasadj = TRUE)
S5V3lam <- BoxCox.lambda(S5V3, lower = -5, upper = 5)
AAS5V3l <- auto.arima(S5V3, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE,
                   lambda = S5V3lam, biasadj = TRUE)
ETSS5V2 <- ets(S5V2, model = 'ZZZ', ic = 'aicc')
ETSS5V2l <- ets(S5V2, model = 'ZZZ', ic = 'aicc', lambda = 'auto',
                biasadj = TRUE)
ETSS5V3 <- ets(S5V3, model = 'ZZZ', ic = 'aicc')
ETSS5V3l <- ets(S5V3, model = 'ZZZ', ic = 'aicc', lambda = S5V3lam,
                biasadj = TRUE)
```

### Model Selection
Exponentially smoothed and ARIMA models were reviewed, with details in the
appendix. The selected model for V2 is an `r arimaDisplay(AAS5V2l)` model with
\(\lambda=\)`r AAS5V2l$lambda[[1]]`. The selected model for V3 is an
`r arimaDisplay(AAS5V3)`.

### Forecasts
The mean forecast values will be supplied in the attached Excel workbook. The
mean forecasts with error bands are plotted below
```{r S5forecastplots, echo=FALSE}
autoplot(forecast(AAS5V2l, h = 140)) + ylab("V2")
autoplot(forecast(AAS5V3, h = 140)) + ylab("V3")
```
```{r S5Final, include=FALSE}
S5Final <- data.table(Var02 = forecast(AAS5V2l, h = 140)$mean,
                      Var03 = forecast(AAS5V3, h = 140)$mean)
```
## Series 6
### Series Visualization

```{r s6NA, echo=F, include=F}
S6 <- ts(S06[-futureobs, 6:7])

sum(is.na(S6))

sum(is.na(S6[,1]))
sum(is.na(S6[,2]))

S6 <- ts(sapply(S6, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S6))
```


```{r s6outlier, echo = F}
S6_orig <- S6
S6 <- ts(sapply(S6, tsclean))
S6 %>% autoplot()
```

Both series have an extremely clear outlier that will has been removed. Both
series also appear to be almost 100% correlated. The data has a clear trend but
no visibly clear seasonal pattern.

### Baseline Model

Once again, since there is no clear seasonal component to the time series, we
are choosing to use a simple random walk with drift model as a baseline.

```{r S6DriftGraph}
s6v5 <- S6[,1]
s6v7 <- S6[,2]

rwf(s6v5, h = 140, drift = T) %>% autoplot() + ylab('Var05')
rwf(s6v7, h = 140, drift = T) %>% autoplot() + ylab('Var07')
```

The baseline model has an error of 16.72. This is not meaningful in and of
itself but rather as a comparison to other models.

### Exponential Smoothing

```{r, echo = F}
ets_s6v5 <- ets(s6v5, model = 'AAN', damped = F)
ets_s6v5 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

ets_s6v7 <- ets(s6v7, model = 'AAN', damped = F)
ets_s6v7 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

This model has an error of 16.69, slightly lower than the baseline.

### ARIMA Model

```{r, echo = F}
# data indicates an AR 1 model from ACF and PACF
arima_s6v5 <- Arima(s6v5, order = c(2,1,2), include.drift = T)
arima_s6v5 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

arima_s6v7 <- Arima(s6v7, order = c(2,1,2), include.drift = T)
arima_s6v7 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')

arima_s6v5
```

The ARIMA model has an error of 16.74, worse than the exponential smoothing
model and even underperforming the baseline. It also has a higher level of
complexity so it makes sense to select the simpler and more accurate model.

```{r S6Final, include=FALSE}
S6Final <- data.table(Var05 = forecast(ets_s6v5, h = 140)$mean,
                      Var07 = forecast(ets_s6v7, h = 140)$mean)
```

## Predictions
The predicted forecasts can be found in the attached Excel workbook named
`DT624_Summer2020_Group1.xlsx`.
```{r createExcel, include=FALSE}
G1P1 <- createWorkbook(creator = "DT624_Summer2020_Group1")
addWorksheet(G1P1, "Set for Class")
writeData(G1P1, "Set for Class", DT)
groups <- unique(DT$group)
for (i in seq_len(6)) {
  gName <- paste0('S0', i)
  addWorksheet(G1P1, gName)
  finalModel <- get(paste0('S', i, 'Final'))
  namesFinal <- names(finalModel)
  DT[group == gName][futureobs, namesFinal] <- finalModel
  writeData(G1P1, gName, DT[group == gName])
}
saveWorkbook(G1P1, file = './DT624_Summer2020_Project1_Group1.xlsx',
             overwrite = TRUE)
```

# Analytical Appendix
## Data Preprocessing
Each of the separate series was investigated for correlation, autocorrelation,
and stationarity. For each data series, the last 140 periods are those needing
the forecast. The first 1622 periods are the data.

There are missing data elements in the series. Excluding the last 140 entries,
the respective series are missing the following data elements:
```{r missingData, echo=FALSE}
knitr::kable(DT[, lapply(.SD, function(x) sum(is.na(x)) - 140),
   .SDcols = c(3:7), keyby = group])
```

These will be addressed in the individual series discussions.

## Series 01
### Variable 01
As mentioned in the text, originally the entire series was fit to ARIMA models.
As becomes clear in the discussion below, this has problems. The technical
narrative will retain the discussion of the original analysis for verification
purposes, but the last section will just describe the training models, the
best-fitting to the test set, and the selected model for forecasting.

#### Missing Data
With no apparent seasonality, the missing 2 variables for out of 1662 values for
V1 were replaced by linear interpolation.

#### Outliers
For V1, five points were identified as outliers using the `tsoutliers` function
in the `forecast` package.

```{r, S1V1Out}
plot(S1V1, main = "Outliers for V1", ylab = "V1")
points(S1V1O$index, S1V1O$replacements, col = 'red', pch = 16L)
points(S1V1O$index, S1V1[S1V1O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

As seen in the plot above, the blue points are the actual values and the maroon
points are the suggested replacements. The outliers and their replacements are
not deemed different enough to substitute interpolated data for real.

#### Stationarity
It is clear that there is non-stationarity in V1 when looking at autocorrelation
plots.

```{r S1V1acf, echo=FALSE, fig.width=10L}
ggAcf(S1V1, type = 'correlation') + ggtitle("Series 1: V1 Autocorrelation")
```

The autocorrelation plot shows slowly decreasing but continually positive
values, a clear sign of the existence of non-stationarity. Using the
*Kwiatkowski-Phillips-Schmidt-Shin* test as suggested by the text implies that a
difference of one lag will be sufficient, as the test statistic falls below the
critical values for any sane level of confidence after
a difference of one lag.

```{r S1V1station, echo=TRUE}
summary(ur.kpss(S1V1))
summary(ur.kpss(diff(S1V1)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S1V1lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S1V1), set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Series 1 Variable 1")
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V1.
```{r S1V1aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S1V1))
```

The positive ACF lag 1 with cutoff after lag 2 implies that V1 will require a MA
component of at least 2. It's more difficult to immediately identify an AR
component, so an exhaustive search using `auto.arima` will be performed on a
set of \(ARIMA(p, 1, q)\) models, using AICc as our goodness-of-fit measure.
While \(d\) has been robustly estimated as 1, it too will be left blank for use
in the automatic trial-and-error fitting procedures. Lastly, as the entirety of
V1 demonstrates a clear upward trend, fitting will be with drift allowed.

For V1, the best model using maximum likelihood is estimated as an
`r arimaDisplay(AAS1V1)`. However, looking at the characteristic roots
gives some pause as the majority of its roots lie near the boundary of the unit
circle. This is despite the author's claim:
"*The `auto.arima()` function is even stricter, and will not select a model*
*with roots close to the unit circle*"
([Section 8.7](https://otexts.com/fpp2/arima-r.html),
Hyndman & Athanasopoulos 2018).

```{r S1V1unitC1}
autoplot(AAS1V1)
```

A number of other models were investigated, but the ones which brought their
roots well inside the circle suffered from severe deterioration in goodness of
fit (Burnham \& Anderson 2012). Therefore, the original model is retained.

The residuals for this model visually exhibit very little heteroskedasticity, 
and are particularly Gaussian in nature.
```{r S1V1resid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAS1V1)
```

#### Box-Cox Adjusted Model
The model was fit with the same exhaustive procedure using a Box-Cox adjustment
with \(\lambda=\)`r AAS1V1l$lambda[[1]]`. The best-fitting model was a much more
parsimonious `r arimaDisplay(AAS1V1l)`. This exhibited completely safe roots and
excellent residuals. 

```{r S1V1BC, echo=FALSE}
ggtsdisplay(diff(BoxCox(S1V1, AAS1V1l$lambda[[1]])))
autoplot(AAS1V1l)
checkresiduals(AAS1V1l)
```

#### Exponential Smoothing State Space
An exponential smoothing state-space model was fit to the data as well and
returned a `r ETSS1V1$method` model without Box-Cox and a `r ETSS1V1l$method`
with \(\lambda=\)`r ETSS1V1l$lambda[[1]]`.

#### Model Comparison and Selection
The accuracy of the models are compared below.

```{r S1V1Acc, echo=FALSE}
accS1V1 <- rbind(data.frame(accuracy(AAS1V1)),
                 data.frame(accuracy(AAS1V1l)),
                 data.frame(accuracy(ETSS1V1)),
                 data.frame(accuracy(ETSS1V1l)))
row.names(accS1V1) <- c('ARIMA', 'ARIMA+BC', 'ETS', 'ETS+BC')
knitr::kable(accS1V1, digits = 3L, format = 'pandoc')
```

The AICc may be compared within untransformed and Box-Cox transformed models,
but not between them.

```{r S1V1AICc, echo=FALSE}
aiccUTS1V1 <- data.frame(Models = c('ARIMA', 'ETS'),
                         AICc = c(AAS1V1$aicc, ETSS1V1$aicc))
aiccBCS1V1 <- data.frame(Models = c('ARIMA+BC', 'ETS+BC'),
                         AICc = c(AAS1V1l$aicc, ETSS1V1l$aicc))
knitr::kable(aiccUTS1V1, digits = 3L, format = 'pandoc')
knitr::kable(aiccBCS1V1, digits = 3L, format = 'pandoc')
```

It is clear that the ARIMA models outperform their state space equivalents for
almost all statistics. Choosing between the ARIMA and ARIMA+BC models is hard,
as most statistics, including the MAPE, are almost exactly the same. However,
there is value in parsimony, so the the `r arimaDisplay(AAS1V1l)` model with
\(\lambda=\)`r AAS1V1l$lambda[[1]]` will be used to forecast Series 01 Variable
01, with the bias adjustment to recover the mean and not the median.

### Variable 02
#### Missing Data
There are no missing values for V2.

#### Outliers
Using the same technique, there are `r length(S1V2O$index)` outliers identified
for V2.
```{r, S1V2aOut}
plot(S1V2, main = "Outliers for V1", ylab = "V1")
points(S1V2O$index, S1V2O$replacements, col = 'red', pch = 16L)
points(S1V2O$index, S1V2[S1V2O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

In this case, the outliers are **clearly** extremes. Replacing the outliers with
the suggestions would result in the series in black becoming the one in green.
```{r, S2V2cOut}
plot(S1V2)
lines(S1V2c, col = 'green4')
```

Consideration was given throughout the process to replacing outliers, but doing
so did not result in any significant improvement.

#### Stationarity
It is clear that there is non-stationarity in V2 when looking at autocorrelation
plots.

```{r S1V2acf, echo=FALSE, fig.width=10L}
ggAcf(S1V2, type = 'correlation') + ggtitle("Series 1: V2 Autocorrelation")
```

The same tests as performed on V1 indicate that a difference of one lag will be
sufficient here as well.

```{r S1V2station, echo=TRUE}
summary(ur.kpss(S1V2))
summary(ur.kpss(diff(S1V2)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S1V2lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S1V2),
          set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Series 1 Variable 2") +
  scale_x_continuous(breaks = breaks_extended(n = 3L),
                     labels = label_scientific(digits = 1L)) 
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V2.
```{r S1V2aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S1V2))
```

Here, there is certainly more uncertainty as to what parameters should be
selected. Therefore, the best option is once again an exhaustive search.

Using the same exhaustive technique as for V1, the best selected model for V2 is
an `r arimaDisplay(AAS1V2)` model. The autoregressive component for V2 makes
sense in the context of the partial autocorrelation plot above; the fourth
partial autocorrelation is larger in magnitude than all that follow. 

However, once again, the characteristic roots raise the specter of instability.
```{r S1V2unitC1, echo=FALSE}
autoplot(AAS1V2) + ggtitle("Inverse Roots for Series 1 V2")
```

The residuals look poor as well.
```{r S1V2Resid, echo=FALSE}
checkresiduals(AAS1V2)
```

Similar to V1, multiple models were investigated including limiting the order of
the ARIMA and replacing the outliers discussed above. However, any model which
showed a meaningful improvement in its roots demonstrated a significant
reduction in goodness of fit.

#### Box-Cox Adjusted Model
The model was fit with the same exhaustive procedure using a Box-Cox adjustment
with \(\lambda=\)`r AAS1V2l$lambda[[1]]`. The best-fitting model was a similar
`r arimaDisplay(AAS1V2l)`. This exhibited slightly safer roots and much better
residuals. 

```{r S1V2BC, echo=FALSE}
ggtsdisplay(diff(BoxCox(S1V2, AAS1V2l$lambda[[1]])))
autoplot(AAS1V2l)
checkresiduals(AAS1V2l)
```

#### Exponential Smoothing State Space
An exponential smoothing state-space model was fit to the data as well and
returned a `r ETSS1V2$method` model without Box-Cox and a `r ETSS1V2l$method`
with \(\lambda=\)`r ETSS1V1l$lambda[[1]]`.

#### Model Comparison and Selection
The accuracy of the models are compared below.

```{r S1V2Acc, echo=FALSE}
accS1V2 <- rbind(data.frame(accuracy(AAS1V2)),
                 data.frame(accuracy(AAS1V2l)),
                 data.frame(accuracy(ETSS1V2)),
                 data.frame(accuracy(ETSS1V2l)))
row.names(accS1V2) <- c('ARIMA', 'ARIMA+BC', 'ETS', 'ETS+BC')
knitr::kable(accS1V2, digits = 3L, format = 'pandoc')
```

The AICc may be compared within untransformed and Box-Cox transformed models,
but not between them.

```{r S1V2AICc, echo=FALSE}
aiccUTS1V2 <- data.frame(Models = c('ARIMA', 'ETS'),
                         AICc = c(AAS1V2$aicc, ETSS1V2$aicc))
aiccBCS1V2 <- data.frame(Models = c('ARIMA+BC', 'ETS+BC'),
                         AICc = c(AAS1V2l$aicc, ETSS1V2l$aicc))
knitr::kable(aiccUTS1V2, digits = 3L, format = 'pandoc')
knitr::kable(aiccBCS1V2, digits = 3L, format = 'pandoc')
```

As usual, the ARIMA models outperform their state space equivalents for almost
all statistics. Choosing between the ARIMA and ARIMA+BC models is easier here,
as most statistics demonstrate the superiority of the Box-Cox transform.
Therefore, the the `r arimaDisplay(AAS1V2l)` model with
\(\lambda=\)`r AAS1V2l$lambda[[1]]` will be used to forecast Series 01 Variable
02, with the bias adjustment to recover the mean and not the median.

### Cross-correlation
While the raw series did exhibit negative correlation, the analysis concluded
that both series needed differencing. Once that occurred, the cross-correlation
shrunk dramatically. Compare the two plots below.

```{r S1CrossCor, echo=FALSE}
ggCcf(S1V1, S1V2) + ggtitle("Cross Correlation V1 & V2: Raw")
ggCcf(diff(S1V1), diff(S1V2)) + ggtitle("Cross Correlation V1 & V2: Differenced")
```

After differencing, the only correlations it would appear worthwhile to
investigate are those at lag 0 (correlation) and lag 1 (cross-correlation). A
future enhancement may be to consider some form of expectation-maximization or
staggered fitting algorithm where the value of \(V_{i_k}\) would also depend on
the value of \(V_{j_{k-1}}\).

## Series 2
### Exploratory Analysis of Var 02

```{r S2ExpAnlV2, echo=FALSE}

autoplot(S2v1)

lag.plot(S2v1)

hist(scale(S2v1),br=100,xlim=c(0,100))

boxplot(S2v1)

qqnorm(S2v1)
qqline(S2v1)
```

Looking at above plots and comparing with the below assumptions for univariate
variable.                                  

Fixed Location: If the fixed location assumption holds, then the run sequence 
plot will be flat and non-drifting.          

Fixed Variation: If the fixed variation assumption holds, then the vertical 
spread in the run sequence plot will be the approximately the same over the
entire horizontal axis.                         

Randomness: If the randomness assumption holds, then the lag plot will be 
structureless and random.                 

Fixed Distribution: If the fixed distribution assumption holds, in particular if 
the fixed normal distribution holds, then the histogram will be bell-shaped, and
the normal probability plot will be linear.                              
In Var02 case, the run sequence plot shows the order that the data are presented
in the file is NOT random and also the variance in the data is eventually not 
much towards end i.e. it is flat and drift is nearly constant. Lag plot has some
structure i.e. mostly a  big clusters is  visible which means it is not random.
Histogram shows the presence of outliers. The histogram is not symmetrical, 
nor does it looks like a bell-curve and it is right skewed. The normal QQ plot 
is not a straight line.

### Exploratory Analysis of Var 03 

```{r S2ExpAnlV3, echo=FALSE}

# interpreting NA values in our variable,
S2v2 <- na_interpolation(S2v2)  

summary(S2v2)

autoplot(S2v2)

lag.plot(S2v2)

hist(scale(S2v2),br=100,xlim=c(0,100) )

boxplot(S2v2)

qqnorm(S2v2)

qqline(S2v2)


```

In Var03 case, histogram shows the data is right skewed, and lag diagram shows a 
big cluster of variables , which means data is randomly distributed.And we can clearly 
see that there is only one outlier from our box plot.so we are not going to do anything
for the outlier as without knowing the background of the data outlier we don't want to 
tinker with that. 

### Outlier replacement for var02

```{r S2outliRep02, echo=FALSE, fig.width=8L, fig.height=6L}

# Replacing outlier using linear interpolation on a time series

S02v2C <- tsclean(S2v1)
S02v3C <- S2v2



autoplot(S02v2C)

hist(scale(S02v2C),br=100,xlim=c(0,100) )
```

Aplying tsclean to replace outlier in the var02 using linear interpolation, it is clear
visible that our run sequence diagram shows a inverse trend and not much of seasonality 
and it has certain constant drift . Histogram is now not rughtly skewed and is now more 
following bell shaped curve. As var03 has only one outlier , so we will give it a skip as
there is not enough data information as to whether this outlier is due to wrong data 
collection is it due to sudden cyclic trend which occured at that time period.

### ACF of Var02 and Var03

```{r S2ACFPlots, echo=FALSE, fig.width=8L, fig.height=6L}

ggAcf(S02v2C)

ggAcf(S02v3C)


```

ACF plots for both var02 & var03 shows that Trend is present with very less or no seasonality.
And also looking at the plots we can say that the correlation is very weak which means there
is very less time dependency in data for both variables.   

## Series 3
### Missing Value Imputation
Below is the code used to linearly impute the missing values.
```{r s3NA, include = T, echo=TRUE}
sum(is.na(S3))

sum(is.na(S3[,1]))
sum(is.na(S3[,2]))

S3 <- ts(sapply(S3, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S3))
```

### Outliers
```{r S3Outliers}
s31out <- tsoutliers(S3[,1])
s32out <- tsoutliers(S3[,2])

data.frame(S3) %>% ggplot() +
  geom_line(aes(x = 1:length(S3[,1]), y = Var05), color = 'green4') +
  geom_point(data = data.frame(s31out), aes(x = index, y = replacements),
             color = 'blue', size = 2) +
  geom_point(aes(x = s31out$index, y = Var05[s31out$index]),
             color = 'red', size = 2) +
  xlab('Time') + ylab('Values') +
  ggtitle('Var05 With Outlier and Replacement Shown')
```

Due to this outlier being one of the defining differences between the two
timeseries we will not be adjusting the outlier to its recommended replacement.

### Decomposition

```{r}
s3v1 <- ts(s3v1, frequency = 365)

s3v1 %>% stl(s.window = 'periodic') %>% autoplot()
Pacf(s3v1, 370)
```

The data may have some seasonal aspect to it on the daily time frame. Since we
are unsure how the data was collected and what it represents we cannot confirm
this. We chose not to model with the seasonal aspect since the there is no
correlation between the data and itself 365 timesteps ago, indicating that last
year's data is not predictive of next years.

### Calculating Error
To calculate the error for each model, we created a model at each timestep using
previous data and predict one timestep ahead. We then compare the predicted
value to the actual value to generate an error at this time step. The overall
error is the sum of squared errors.

### Drift Model Error
```{r S3Drift, echo=TRUE}
drift_sse_v1 <- 0
for(i in 100:(length(s3v1)-1)){
  pred <- rwf(s3v1[1:i], h = 1, drift = T)
  error <- abs((s3v1[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  drift_sse_v1 <- drift_sse_v1 + error
}
drift_sse_v1
```

### Exponential Smoothing Error
```{r S3ExpSmthErr, echo=TRUE}
ets_sse <- 0
for(i in 100:(length(s3v1)-1)){
  model <- ets(s3v1[1:i], model = 'AAN')
  pred <- model %>% forecast(h = 1)
  error <- abs((s3v1[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  ets_sse <- ets_sse + error
}
ets_sse
```

### ARIMA Parameter Selection
The ACF plot indicates a strong trend component to the data. the PACF plot
indicates that there is little to no seasonality as entries far in the past are
not predictive of entries in the future. We also used a PACF plot of length 300
to check for any daily seasonality but it appears there is no strong daily
seasonality on the yearly scale either.

```{r S3ARIMA_1, warning=FALSE}
library(tseries)

kpss.test(s3v1)
kpss.test(diff(s3v1))

diff(s3v1) %>% autoplot()
acf(s3v1)
```

The decreasing ACF values coupled with the large spike in the PACF values at 1
is indicative of an AR(1) model, leading us to believe that the best ARIMA model
would be an ARIMA(1,1,0) model. Also, the PACF values discount the daily
seasonality we found as there is no spike in the PACF graph near 365.

### Differencing Data

The data is clearly non stationary which means it is not appropriate for ARIMA
modeling. We can solve this by taking the first (or potentially second)
difference between observations.

The new differenced data is stationary and therefore appropriate for ARIMA
modeling. Diagnostic tests suggest either an ARIMA(1,1,0) model or an
ARIMA(2,1,0) model.

### ARIMA Error
```{r S3ARIMA_2, echo=TRUE}
arima_sse <- 0
for(i in 100:(length(s3v1)-1)){
  model <- Arima(s3v1[1:i], order = c(1,1,0), include.drift = T)
  pred <- model %>% forecast(h = 1)
  error <- abs((s3v1[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  arima_sse <- arima_sse + error
}
arima_sse
```

### Model Residuals
```{r S3Residuals}
checkresiduals(arima_s3v1)
checkresiduals(fit)
```

Both sets of residuals appear to resemble a normal distribution with some strong
outliers due to the volatile nature of the series. However the exponential
smoothing model does not pass  the Ljung-Box test with a p-value of .03,
indicating that some of the residuals from the exponential smoothing model may
be correlated and that the model could be improved. As such, this further
confirms our decision to use the ARIMA model for this series.


## Series 4
### S4V1
#### Outlier Replacement

```{r S4_1}
#outliers do not look significant enough to be replaced

plot(S4V1, main = "Outliers for V1", ylab = "V1")
points(S4V1_outliers$index, S4V1_outliers$replacements, col = 'red', pch = 16L)
points(S4V1_outliers$index, S4V1[S4V1_outliers$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

The plot above shows the outliers for the first series in blue and their
replacements in red. There does not appear to be a significant difference
between the outliers and the replacements given how close they are to each
other. As a result, we did not choose to use the replacements for the first
series.

#### Stationarity

```{r S4_2}
#nonstationary

ggAcf(S4V1, type = 'correlation') + ggtitle("V1 Autocorrelation")
```

The ACF plot of the first series above shows a very clear indication of
autocorrelation due to significant spikes across the board for every lag.
As further evidence, we conducted a KPSS Unit Root Test on both the standard
series and a first order difference of the series to see if differencing would
improve the stationarity. According to the tests, differencing appears to
improve the stationarity significantly as seen by the value of the test
statistic falling under the 1 percent critical value.

```{r S4_3}
# One lag is good enough because test statistic falls below the 1 percent
# critical value

summary(ur.kpss(S4V1))
summary(ur.kpss(diff(S4V1)))
```

After taking the first difference, we can see a significant improvement in the
stationarity as shown in the lag plots below. None of the lags appear to exhibit
a linear pattern, which means that the data is more stationary now.

```{r S4_4}
#after differencing, autocorrelation is gone as seen by random pattern
gglagplot(diff(S4V1), set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 1")
```

#### Picking ARIMA Parameters
Looking at the ACF and PACF plots, we can see that there is not a clear
indication that an AR or MA model should be used. The only root that is
statistically significant is at lag 17 for both plots, but even then, it is not
significant by a large margin. Also, there appears to be increasing variance as
time increases, which indicates that a Box-Cox transformation may be necessary.

At this point, the candidate model appears to be a Box-Cox transformed
ARIMA(0,1,0) model.

```{r S4_5}
#lag at 17 may indicate something
ggtsdisplay(diff(S4V1))
```

#### Residuals
After fitting the model, we found that an ARIMA(0,1,1) model had a lower AICc
compared to the ARIMA(0,1,0) model. This ARIMA(0,1,1) model exhibited good
residuals with no discernible pattern, constant variance, and a normal
distribution. In other words, the residuals appear to be Gaussian Noise.

```{r S4_6}
#residuals looking good. no discernible patterns, Gaussian noise.
checkresiduals(S4V1_boxed_autoarima)
```

### S4V2
#### Outlier Replacement

The plot below shows the outliers for the second series in blue and their
replacements in red. As we can see, there are many outliers in this series and
that their replacements are much more reasonable values. For this series,
replacing the outliers would create better forecasts.

```{r S4_7}
#there are many outliers that need to be replaced

plot(S4V2, main = "Outliers for V2", ylab = "V2")
points(S4V2_outliers$index, S4V2_outliers$replacements, col = 'red', pch = 16L)
points(S4V2_outliers$index, S4V2[S4V2_outliers$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

#### Stationarity
According to the ACF plot below, the data exhibits very clear non-stationarity
due to the spikes across all lags. Differencing will be necessary to make the
series stationary.

```{r S4_8}
#nonstationary

ggAcf(S4V2_clean, type = 'correlation') + ggtitle("V2 Autocorrelation")
```

Using a KPSS Unit Root Test, we show that one degree of differencing is
sufficient for creating a stationary series. The value of the test statistic
falls below the critical value at 1 percent, which indicates that the series is
stationary.

```{r S4_9}
# One lag is good enough because test statistic falls below the 1 percent
# critical value

summary(ur.kpss(S4V2_clean))
summary(ur.kpss(diff(S4V2_clean)))
```

The lag plot below further reinforces the idea that the once differenced series
is stationary given that there is no linear pattern for any lag.

```{r S4_10}
#lags look more random now after differencing
gglagplot(diff(S4V2_clean),
          set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 2") +
  scale_x_continuous(breaks = breaks_extended(n = 3L),
                     labels = label_scientific(digits = 1L)) 
```

#### Picking ARIMA Parameters
The ACF graph shows two spikes at lag 1 and lag 2. This would suggest that an
ARIMA(p,1,2) model would be appropriate for this series. The PACF graph shows
spikes from lag 1 to lag 9. This would suggest that `q` could be any value
between 3 and 9. Though, most likely, a value of 3 or 4 would be sufficient for
the AR component of this model.

The variance appears to be constant, however, the large spikes in the first half
might indicate that a Box-Cox transformation could improve the model.

Given that there are many moving parts for this series, we decided to use the
`auto.arima` function to fit a variety of different models at various `p` and
`q` values with and without a Box-Cox transformation.

```{r S4_11}
#PACF would suggest an ARIMA(3,1,q) or ARIMA(4,1,q) model
#ACF would suggest an ARIMA(p,1,1) or ARIMA(p,1,2) model

ggtsdisplay(diff(S4V2_clean))
```

#### Roots Analysis
The characteristic roots for our best model are pretty good overall given that
most of the roots are not close to the unit circle. However, there is one MA
root that is close to the circle, which could indicate that the forecast may not
be as accurate as we hope.

```{r S4_12}
# AR roots look fine, MA root is too close to the unit circle, which indicates
# that the model might not be good for forecasting
autoplot(S4V2_boxed_outliers_autoarima)
```

#### Residuals
After running the `auto.arima` function, we found that an ARIMA(3,1,1) model
with smoothed outliers and a Box-Cox transformation gave the best looking
results. As seen in the residual plots below, the variance of the noise is
constant, there is no indication of autocorrelation of the residuals, and the
residuals are normally distributed. All these factors would indicate that the
residuals are white noise.

```{r S4_13}
# The outlier replacement model appears to be the best compared to the other
# models

checkresiduals(S4V2_boxed_outliers_autoarima)
```

## Series 05
### Variable 02
#### Missing Data
With no apparent seasonality, the missing observation for V2 was imputed by
linear interpolation.

#### Outliers
For V2, `r length(S5V2O$index)` points were identified as outliers using the
`tsoutliers` function in the `forecast` package.
```{r, S5V2Out}
plot(S5V2, main = "Outliers for V2", ylab = "V2")
points(S5V2O$index, S5V2O$replacements, col = 'red', pch = 16L)
points(S5V2O$index, S5V2[S5V2O$index], col = 'blue', pch = 16L)
legend('topright', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

As seen in the plot above, the blue points are the actual values and the red
points are the suggested replacements. Making this quantity of significant
changes is something to be avoided. It may be better to perform a transform on
all the data, allowing the relationship between the points to be consistent
under the transform, as opposed to making `r length(S5V2O$index)` individual
changes.

#### Stationarity
It is clear that there is non-stationarity in V1 when looking at autocorrelation
plots.

```{r S5V2acf, echo=FALSE, fig.width=10L}
ggAcf(S5V2, type = 'correlation') + ggtitle("Series 5: V1 Autocorrelation")
```

The autocorrelation plot shows slowly decreasing but continually positive
values, a clear sign of the existence of non-stationarity. Using the
*Kwiatkowski-Phillips-Schmidt-Shin* test as suggested by the text implies that a
difference of one lag will be sufficient, as the test statistic falls below the
critical values for any sane level of confidence after
a difference of one lag.

```{r S5V2station, echo=TRUE}
summary(ur.kpss(S5V2))
summary(ur.kpss(diff(S5V2)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S5V2lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S5V2), set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Series 5 Variable 2")
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V2.
```{r S5V2aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S5V2))
```

The negative ACF lag 1 with cutoff after lag 1 implies that V2 will require a MA
component of at least 1. For the PACF, there are clearly significant lags 
through the fourth. The fifth lag is barely significant, the sixth lag isn't,
and then the next few lags barely are. There is no steady deterioration, though,
so the implication is after differencing there will need to be at least 4 AR
lags. There still is some extreme variability as seen in the top plot, so a
Box-Cox transform should be attempted as well.

With this in mind, it is convenient to use the exhaustive feature of
`auto.arima` on a set of \(ARIMA(p, 1, q)\) models, using AICc as our
goodness-of-fit measure. While \(d\) has been robustly estimated as 1, it too
will be left blank for use in the automatic trial-and-error fitting procedures.

For V1, the best model using maximum likelihood is estimated as an
`r arimaDisplay(AAS5V2)`. Looking at the characteristic roots is mainly
reassuring. The AR roots are all well within the circle. The MA root is close to
the boundary, but is acceptable.

```{r S5V2unitC1}
autoplot(AAS5V2)
```

The residuals for this model visually exhibit very little heteroskedasticity, 
and are relatively Gaussian in nature.
```{r S5V2resid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAS5V2)
```

#### Box-Cox Adjusted Model
The model was fit with the same exhaustive procedure using a Box-Cox adjustment
with \(\lambda=\)`r AAS5V2l$lambda[[1]]`. The best-fitting model was a more
parsimonious `r arimaDisplay(AAS5V2l)`. This too exhibited safe roots and
acceptable residuals. What is of note is that despite a similar PACF graph as to
the untransformed series, the model with the lowest AICc has fewer AR components
than would have been expected.

```{r S5V2BC, echo=FALSE}
ggtsdisplay(diff(BoxCox(S5V2, AAS5V2l$lambda[[1]])))
autoplot(AAS5V2l)
checkresiduals(AAS5V2)
```

#### Exponential Smoothing State Space
An exponential smoothing state-space model was fit to the data as well and
returned a `r ETSS5V2$method` model without Box-Cox and a `r ETSS5V2l$method`
with \(\lambda=\)`r ETSS5V2l$lambda[[1]]`. Note this is, by necessity, the same
value as the ARIMA model, as  \(\lambda\) is a function of the *data* and not
the model.

#### Model Comparison and Selection
The accuracy of the models are compared below.

```{r S5V2Acc, echo=FALSE}
accS5V2 <- rbind(data.frame(accuracy(AAS5V2)),
                 data.frame(accuracy(AAS5V2l)),
                 data.frame(accuracy(ETSS5V2)),
                 data.frame(accuracy(ETSS5V2l)))
row.names(accS5V2) <- c('ARIMA', 'ARIMA+BC', 'ETS', 'ETS+BC')
knitr::kable(accS5V2, digits = 3L, format = 'pandoc')
```

The AICc may be compared within untransformed and Box-Cox transformed models,
but not between them.

```{r S5V2AICc, echo=FALSE}
aiccUTS5V2 <- data.frame(Models = c('ARIMA', 'ETS'),
                         AICc = c(AAS5V2$aicc, ETSS5V2$aicc))
aiccBCS5V2 <- data.frame(Models = c('ARIMA+BC', 'ETS+BC'),
                         AICc = c(AAS5V2l$aicc, ETSS5V2l$aicc))
knitr::kable(aiccUTS5V2, digits = 3L, format = 'pandoc')
knitr::kable(aiccBCS5V2, digits = 3L, format = 'pandoc')
```

It is clear that the ARIMA models outperform their state space equivalents for
almost all statistics. When choosing between the ARIMA and ARIMA+BC models,
most statistics, including the MAPE, point to the Box-Cox adjusted model. Thus,
the `r arimaDisplay(AAS5V2l)` model with \(\lambda=\)`r AAS5V2l$lambda[[1]]`
will be used to forecast Series 05 Variable 02, with the bias adjustment to
recover the mean and not the median.

### Variable 03
#### Missing Data
There are `r length(S5V3missing)` missing values for V2 which are at the
x-axis locations of the purple points below:

```{r S5V3Mis, echo=FALSE}
plot(S5V3)
points(S5V3missing, rep(S5V3med, length(S5V3missing)), col = 'purple')
```

With no apparent seasonality, these were imputed using linear interpolation.
#### Outliers
For V2, `r length(S5V3O$index)` point was identified as an outlier using the
`tsoutliers` function in the `forecast` package.

```{r, S5V3aOut}
plot(S5V3, main = "Outliers for V3", ylab = "V3")
points(S5V3O$index, S5V3O$replacements, col = 'red', pch = 16L)
points(S5V3O$index, S5V3[S5V3O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

With only the one value, it was decided not to adjust the data.
#### Stationarity
It is clear that there is non-stationarity in V2 when looking at autocorrelation
plots.

```{r S5V3acf, echo=FALSE, fig.width=10L}
ggAcf(S5V3, type = 'correlation') + ggtitle("Series 5: V2 Autocorrelation")
```

The same tests as performed on V1 indicate that a difference of one lag will be
sufficient here as well.

```{r S5V3station, echo=TRUE}
summary(ur.kpss(S5V3))
summary(ur.kpss(diff(S5V3)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S5V3lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S5V3),
          set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Series 5 Variable 3")
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V2.
```{r S5V3aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S5V3))
```

Here, with both ACF and PACF positive with sharp cutoffs after 1, there is not
much we can glean from the plots. This is where exhaustive searches are valuable.
To quote Dr. Uwe Ligges of the University of Dortmund and a member of R core for
many decades "*RAM is cheap and thinking hurts*" (Ligges 2007).

Using the same exhaustive technique as for V2, the best selected model for
untransformed V3 is an `r arimaDisplay(AAS5V3)` model. Many of its characteristic
roots are near the boundary, which is somewhat concerning.
```{r S5V3unitC1, echo=FALSE}
autoplot(AAS5V3) + ggtitle("Inverse Roots for Untransformed Series 5 V3")
```

The residuals look good, though.
```{r S5V3Resid, echo=FALSE}
checkresiduals(AAS5V3)
```

#### Box-Cox Adjusted Model
The model was fit with the same exhaustive procedure using a Box-Cox adjustment
with \(\lambda=\)`r S5V3lam`. Interestingly, the best-fitting model is a less
parsimonious `r arimaDisplay(AAS5V3l)`. The added MA root is well within the
circle, the others are similar to the untransformed model.

```{r S5V3l_unitC2, echo=FALSE}
autoplot(AAS5V3l)
```

Its residuals are eminently acceptable as well.
```{r S5V3lresid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAS5V3l)
```

#### Exponential Smoothing State Space
An ETS model with and without Box-Cox was also fit. These returned an
`r ETSS5V3$method` and `r ETSS5V3l$method` respectively.

#### Model Comparison and Selection
The accuracy of the models are compared below.

```{r S5V3Acc, echo=FALSE}
accS5V3 <- rbind(data.frame(accuracy(AAS5V3)),
                 data.frame(accuracy(AAS5V3l)),
                 data.frame(accuracy(ETSS5V3)),
                 data.frame(accuracy(ETSS5V3l)))
row.names(accS5V3) <- c('ARIMA', 'ARIMA+BC', 'ETS', 'ETS+BC')
knitr::kable(accS5V3, digits = 5L, format = 'pandoc')
```

The AICc may be compared within untransformed and Box-Cox transformed models,
but not between them.

```{r S5V3AICc, echo=FALSE}
aiccUTS5V3 <- data.frame(Models = c('ARIMA', 'ETS'),
                         AICc = c(AAS5V3$aicc, ETSS5V3$aicc))
aiccBCS5V3 <- data.frame(Models = c('ARIMA+BC', 'ETS+BC'),
                         AICc = c(AAS5V3l$aicc, ETSS5V3l$aicc))
knitr::kable(aiccUTS5V3, digits = 3L, format = 'pandoc')
knitr::kable(aiccBCS5V3, digits = 3L, format = 'pandoc')
```

In both cases, the ARIMA variants once again outperform their exponentially
smoothed counterparts. This time, in comparable metrics (e.g. RMSE), the Box-Cox
transformation does not show better results. Moreover, the \(\lambda\) is
greater than 2, and serves to exaggerate the outliers, as can be seen from the
plots above. Therefore, in the interests of accuracy, parsimony, and data
fidelity, the `r arimaDisplay(AAS5V3)` model will be used to forecast Variable
03 for Series 05.

## Series 6
### Missing Value Imputation
Below is the code for imputing the missing values. Missing values are imputed
linearly as an average between the last two known values.

```{r s6NA-exec, echo=T, output = F}
sum(is.na(S6))

sum(is.na(S6[,1]))
sum(is.na(S6[,2]))

S6 <- ts(sapply(S6, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S6))
```

### Outliers
Here is the code used to clean the outlier.

```{r}
s61out <- tsoutliers(S6_orig[,1])
s62out <- tsoutliers(S6_orig[,2])

data.frame(S6_orig) %>% ggplot() +
  geom_line(aes(x = 1:length(S6_orig[,1]), y = Var05), color = 'green4') +
  geom_point(data = data.frame(s61out), aes(x = index, y = replacements),
             color = 'blue', size = 2) +
  geom_point(aes(x = s61out$index, y = Var05[s61out$index]),
             color = 'red', size = 2) +
  xlab('Time') + ylab('Values') +
  ggtitle('Var05 With Outlier and Replacement Shown')
```

### Decomposition

```{r S6Decomposition}
s6v5 <- ts(s6v5, frequency = 365)

s6v5 %>% stl(s.window = 'periodic') %>% autoplot()
```

It is unclear what the correct time period for this data should be as we have no
knowledge of what the data is modeling, so we have tested annual, quarterly,
monthly, weekly, and daily seasonality to see if the data fits any of these
models. Although there is some seasonal pattern on the daily timeframe, the data
is not predictive of itself 1 year in advance so we will not model using
seasonality.

### Calculating Error
Error is calculated using hold one out validation where a model is build on all
previous data and then forecasts the next point. Error statistic reported is the
sum of squared errors across all data points.

### Drift Model Error

```{r S6DriftError, echo=TRUE}
drift_sse_v1 <- 0
for(i in 100:(length(s6v5)-1)){
  pred <- rwf(s6v5[1:i], h = 1, drift = T)
  error <- abs((s6v5[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  drift_sse_v1 <- drift_sse_v1 + error
}
drift_sse_v1
```

### Exponential Smoothing Error

```{r S6ExpSmthErr, echo=TRUE}
ets_sse <- 0
for(i in 100:(length(s6v5)-1)){
  model <- ets(s6v5[1:i], model = 'AAN')
  pred <- model %>% forecast(h = 1)
  error <- abs((s6v5[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  ets_sse <- ets_sse + error
}
ets_sse
```

### ARIMA Parameter Selection

```{r S6ACFDiag}
library(tseries)

kpss.test(s6v5)
kpss.test(diff(s6v5))

diff(s6v5) %>% autoplot()
acf(s6v5)
Pacf(s6v5, 20)
Pacf(s6v5, 370)
```

The spike in the PACF at 2 may indicate an AR component of 2 in the ARIMA model.
There also seems to be some alternating nature to the PACF graph with small
spikes at 4 and 6, which may indicate an oscillating nature to the data which
would be well modeled by an MA 2 component.

The data needs a single differencing to become a stationary series.

### ARIMA Model Error

```{r S6ARIMA_2, echo=TRUE}
arima_sse <- 0
for(i in 100:(length(s6v5)-1)){
  model <- Arima(s6v5[1:i], order = c(2,1,2), method = 'CSS', include.drift = T)
  pred <- model %>% forecast(h = 1)
  error <- abs((s6v5[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  arima_sse <- arima_sse + error
}
arima_sse
```

### Residuals

```{r S6Residuals}
checkresiduals(arima_s6v5)
checkresiduals(ets_s6v5)
```

The residuals confirm our suspicions: The exponential smoothing model is likely
a better fit as the residuals from the ARIMA have a significant level of
correlation with each other, meaning the ARIMA model has not fully modeled the
data. The exponential smoothing model however has non correlated residuals and
is therefore the better model choice.

# References
 * Burnham, Kenneth P., and Anderson, David R. (2002).
 *Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach*.
 Second. New York: Springer Science+Business Media, Inc.
 * Hyndman, R.J., & Athanasopoulos, G. (2018)
 *Forecasting: principles and practice*, 2nd edition, OTexts: Melbourne,
 Australia. OTexts.com/fpp2. Accessed on 2020-06-21
 * Ligges, Uwe, (June 23, 2007) E-mail response to R-help
 https://stat.ethz.ch/pipermail/r-help/2007-June/134955.html
 