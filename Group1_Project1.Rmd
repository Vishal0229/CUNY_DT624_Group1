---
title: "CUNY DT 624"
author: 'Group 1: Avraham Adler, Vishal Arora, Samuel Bellows, Austin Chan'
date: "Summer 2020"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: 4
subtitle: Project 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readxl)
library(fpp2)
library(urca)
library(ggplot2)
library(scales)
library(data.table)

# Read in Data
DT <- read_xls('./project1data/Data Set for class.xls')
# Set as data table
setDT(DT)
# Separate out groups
n <- length(unique(DT$group))
for (i in seq_len(n)) {
  gName <- paste0('S0', i)
  assign(gName, DT[group == gName])
}

# Following is useful to quickly remove "future" periods from groups
futureobs <- 1623:1762
```

# Introduction
The purpose of this report is to describe the analysis performed in order to
forecast the following series, each for 140 periods.

 * S01: Forecast Var01 and Var02
 * S02: Forecast Var02 and Var03
 * S03: Forecast Var05 and Var07
 * S04: Forecast Var01 and Var02
 * S05: Forecast Var02 and Var03
 * S06: Forecast Var05 and Var07
 
The forecasts will be provided in a separate Excel document. The code for the
analysis can be found in the attached R markdown document.

The **executive summary** will contain the key findings. The more detailed
analysis may be found in the **analytical appendix** for those interested in
the findings and decisions applied.

# Executive Summary
## Series 01
### Observations
Variables 01 and 02 (V1 & V2 from now on) of Series 01 (S1 from now on) can each
be considered a time series. As few transformations as possible will be applied
to the data to maximize fidelity to the original source.
```{r S1ts, echo=FALSE}
S1ts <- ts(S01[-futureobs, 3:4])
plot(S1ts)
```

Clearly V1 and V2 are negatively correlated with each other, as V1 has an
increasing secular trend whereas V2 has an overall decreasing one. This is even
more apparent when they are plotted after center-scaling and normalization.
```{r S1scaleV, echo=FALSE, fig.width=8L, fig.height=6L}
V1 <- scale(S01$Var01[-futureobs])
V2 <- scale(S01$Var02[-futureobs])
plot(V2, type = 'l', col = 'green4', main = "Series 01 Variables",
     xlab = "Time", ylab = "Scaled Values", ylim = c(-2, 8))
lines(V1, col = 'blue')
legend('topright', legend = c('V1', 'V2'), col = c('blue', 'green4'), lty = 1L)
```
```{r SeriesFitting, include=FALSE}
S1V1 <- na.interp(S1ts[, 1])
S1V1w <- window(S1V1, 1, 1000)
S1V1wp <- window(S1V1, 1001)
S1V2 <- S1ts[, 2]
V1O <- tsoutliers(S1V1)
V2O <- tsoutliers(S1V2)
S1V2c <- tsclean(S1V2)
AAV1 <- auto.arima(S1V1, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE)
AAV2 <- auto.arima(S1V2, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE)
V1p <- AAV1$arma[[1]]
V1q <- AAV1$arma[[2]]
V1d <- AAV1$arma[[5]]
V2p <- AAV2$arma[[1]]
V2q <- AAV2$arma[[2]]
V2d <- AAV2$arma[[5]]
AAV2l <- auto.arima(S1V2, max.order = 8L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, seasonal = FALSE, parallel = TRUE,
                   lambda = 'auto', biasadj = TRUE)
V2lp <- AAV2l$arma[[1]]
V2lq <- AAV2l$arma[[2]]
V2ld <- AAV2l$arma[[5]]
ETSV1 <- ets(S1V1, model = 'ZZZ', ic = 'aicc')
ETSV2 <- ets(S1V2, model = 'ZZZ', ic = 'aicc')
ETSV2l <- ets(S1V2, model = 'ZZZ', ic = 'aicc', lambda = 'auto', biasadj = TRUE)
```

### Stationarity
Both variables, as they stand are non-stationary. Statistical tests indicate
that a difference of lag 1 is sufficient to induce stationarity.

### Seasonality
Once differenced, neither variable exhibits seasonality.

### Missing Data
With no apparent seasonality, the missing 2 variables for out of 1662 values for
V1 were imputed by linear interpolation. There are no missing values for V2.

### Transformations
After investigating the options, V1 was modeled with neither outlier replacement
nor transformation and V2 was modeled without explicit outlier replacement but
included an option with a Box-Cox transform to reduce its variability.

### Model Selection
Exponentially smoothed and ARIMA models were reviewed, with details in the
appendix. The selected model for V1 is an ARIMA(`r V1p`, `r V1d`, `r V1q`)
with drift and for V2 it is an ARIMA(`r V2lp`, `r V2ld`, `r V2lq`) with a
Box-Cox \(\lambda\) of `r AAV2l$lambda[[1]]`

### Forecasts
The mean forecast values will be supplied in the attached Excel workbook. The
mean forecasts with error bands are plotted below
```{r S1forecastplots, echo=FALSE}
autoplot(forecast(AAV1, h = 140)) + ylab("V1")
autoplot(forecast(AAV2l, h = 140, lambda = AAV2l$lambda[[1]], biasadj = TRUE)) +
  ylab("V2")
```

## Series 3
### Series Visualization
```{r s3}
S3 <- ts(S03[-futureobs, 6:7])
autoplot(S3) + ylab('Value') + xlab('Time') + ggtitle('Var05 vs Var07')
```

The two series of interest appear to be extremely similar, although one of the
series appears to be slightly more volatile than the other. Likely the forecasts
for these two series will be almost identical. We can also see on the graph what
appears to be missing values towards the very end of the series.

The data appear to have a strong trend but no clear seasonal pattern, which will
aid us in our modeling in the future.

### Missing Values

missing values were imputed linearly to create a smooth timeseries.

```{r s3NA-exec, echo=F, include=F}
sum(is.na(S3))

sum(is.na(S3[,1]))
sum(is.na(S3[,2]))

S3 <- ts(sapply(S3, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S3))
```

### Outliers
The series have a single outlier between them which is also apparent in the plot
of the timeseries above. However, since the series are so similar and this is
one of the defining differences of the series, I think it would be unwise to
remove this outlier.

### Baseline Model
Since the data has a clear trend component but no seasonality component, a
simple random walk with drift will serve as a good baseline model.

```{r s3var1-drift, echo = F}
s3v1 <- S3[,1]
s3v2 <- S3[,2]

rwf(s3v1, h = 140, drift = T) %>% autoplot() + ylab('Var05')
rwf(s3v2, h = 140, drift = T) %>% autoplot() + ylab('Var07')
```

The cumulative error for our baseline model is 20.09, which is meaningless in and
of itself but will be an excellent comparison point for other models.

### Decomposition

```{r}
s3v1 <- ts(s3v1, frequency = 365)

s3v1 %>% stl(s.window = 'periodic') %>% autoplot()
```

The data may have some seasonal aspect to it on the daily time frame. Since we are unsure how the data was collected and what it represents we cannot confirm this. We chose not to model with the seasonal aspect since the there is no correlation between the data and itself 365 timesteps ago, indicating that last year's data is not predictive of next years.

### Exponential Smoothing model

Due the the nature of the data, a linear trend model is a natural choice as an
attempt to model the data.

```{r, echo = F}
fit <- ets(s3v1, model = 'AAN', damped = F)
fit %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

fit2 <- ets(s3v2, model = 'AAN', damped = F)
fit2 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

Exponential Smoothing models achieved an error of 20.05, a slight improvement on
the baseline model.

### Differencing Data

The data is clearly non stationary which means it is not appropriate for ARIMA
modeling. We can solve this by taking the first (or potentially second)
difference between observations.

The new differenced data is stationary and therefore appropriate for ARIMA
modeling. Diagnostic tests suggest either an ARIMA(1,1,0) model or an
ARIMA(2,1,0) model.

### ARIMA Model

The most appropriate ARIMA model is an ARIMA(1,1,0) model as the data is clearly
not stationary and needs to be differenced. This choice of model parameters can
be confirmed both by data visualization and by the auto arima function in R.

```{r, echo = F}
# data indicates an AR 1 model from ACF and PACF
arima_s3v1 <- Arima(s3v1, order = c(1,1,0), include.drift = T)
arima_s3v1 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

arima_s3v2 <- Arima(s3v2, order = c(1,1,0), include.drift = T)
arima_s3v2 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

The ARIMA model has an error of 19.98, the lowest of the 3 models. All 3 models
make very similar looking positions so we will select the ARIMA model as it has
the lowest error across the validation period.



## Series 4

### Series Visualization

```{r echo=FALSE}
S4ts <- ts(S04[-futureobs, 3:4])
plot(S4ts)
```

The graphs above show both series plotted next to each other. According to the graphs, the series look quite different from each other. The first series appears to be gradually increasing upward as time increases and then begins to trend downward after a certain point. This series is not stationary, so some differencing will be used to create a stationary series. Beyond the non-stationarity, there does not seem to be any unusual values in the first series.

The second series appears to be stationary with many large spikes at somewhat random intervals. These large spikes will most likely have an effect on the forecasting and should be adjusted for more accurate predictions.

```{r echo=FALSE, fig.width=8L, fig.height=6L}
V1 <- scale(S04$Var01[-futureobs])
V2 <- scale(S04$Var02[-futureobs])
plot(V2, type = 'l', col = 'green4', main = "Series 01 Variables",
     xlab = "Time", ylab = "Scaled Values", ylim = c(-2, 8))
lines(V1, col = 'blue')
legend('topright', legend = c('V1', 'V2'), col = c('blue', 'green4'), lty = 1L)
```

The graph above shows both series overlayed. There does not appear to be an obvious correlation or pattern between these two series.


### Data Transformation

#### Missing Values

The first series exhibits a couple missing values. The `na.interp` times series interpolation function replaces the missing values with numbers that match the pattern of the overall series.

The second series had no missing values.

#### Outliers

The first series exhibited a few outliers, but they were not significant enough to warrant replacement; so no outlier transformations were performed.

The second series exhibited many outliers that were quite extreme. These outliers were replaced with more reasonable values using the `tsclean` function.

#### Variance Stabilization

Both series did not exhibit a changes in variance, so a Box-Cox transformation was deemed unnecessary.

#### Stationarity

A first order difference was applied to both series to induce stationarity. The differencing significantly improved the stationarity for both series to a satifactory level so no further differencing was applied.

#### Seasonality

After differencing both series, neither series exhibited seasonality.

```{r include=FALSE}
sum(is.na(S4ts[,1])) # 2 missing values
sum(is.na(S4ts[,2])) # no missing values

S4V1 = na.interp(S4ts[,1]) #interpolate missing values
S4V1_outliers = tsoutliers(S4V1) #identify outliers

S4V2 = S4ts[,2]
S4V2_outliers = tsoutliers(S4V2) #identify outliers
S4V2_clean = tsclean(S4V2) #replace outliers with more reasonable values
```

```{r include=FALSE}
S4V1_autoarima = auto.arima(S4V1, ic = 'aicc', stepwise = F, approximation = F, allowdrift = F, seasonal = F, parallel = T)
S4V1_boxed_autoarima = auto.arima(S4V1, ic = 'aicc', stepwise = F, approximation = F, allowdrift = F, seasonal = F, parallel = T, lambda = 'auto')

S4V2_autoarima = auto.arima(S4V2, ic = 'aicc', stepwise = F, approximation = F, allowdrift = F, seasonal = F, parallel = T)
S4V2_outliers_autoarima = auto.arima(S4V2_clean, ic = 'aicc', stepwise = F, approximation = F, allowdrift = F, seasonal = F, parallel = T)
S4V2_boxed_autoarima = auto.arima(S4V2, ic = 'aicc', stepwise = F, approximation = F, allowdrift = F, seasonal = F, parallel = T, lambda = 'auto')
S4V2_boxed_outliers_autoarima = auto.arima(S4V2_clean, ic = 'aicc', stepwise = F, approximation = F, allowdrift = F, seasonal = F, parallel = T, lambda = 'auto')
```

### Forecasting

The forecast for both series can be seen below.

```{r, echo=FALSE}
plot(forecast(S4V1_boxed_autoarima, h = 140), main = "Forecast for V1")
plot(forecast(S4V2_boxed_outliers_autoarima, h = 140, biasadj = TRUE), main = "Forecast for V2")
```

## Series 6

### Series Visualization

```{r}
S6 <- ts(S06[-futureobs, 6:7])
autoplot(S6) + ylab('Value') + xlab('Time') + ggtitle('Var05 vs Var07')
```

Both series have an extremely clear outlier that will need to be adressed. Both series also appear to be almost 100% correlated. The data has a clear trend but no visibly clear seasonal pattern.

### Missing Values

As per most other series, missing values were imputed linearly to maintain a consistent time series.

```{r s6NA, echo=F, include=F}
sum(is.na(S6))

sum(is.na(S6[,1]))
sum(is.na(S6[,2]))

S6 <- ts(sapply(S6, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S6))
```

### Outliers

Before doing any outlier analysis it is very clear that there is a large outlier that will need to be fixed in order to create stable predictions for this timeseries.

Here we can see the repaired timeseries with outliers removed.

```{r}
S6_orig <- S6
S6 <- ts(sapply(S6, tsclean))
S6 %>% autoplot()
```

We get a much better view of the time series as a whole, not obscured by the huge outlier.

### Baseline Model

Once again, since there is no clear seasonal component to the time series, we are choosing to use a simple random walk with drift model as a baseline.

```{r S6DriftGraph}
s6v5 <- S6[,1]
s6v7 <- S6[,2]

rwf(s6v5, h = 140, drift = T) %>% autoplot() + ylab('Var05')
rwf(s6v7, h = 140, drift = T) %>% autoplot() + ylab('Var07')
```

The baseline model has an error of 16.72. This is not meaningful in and of itself but rather as a comparison to other models.

### Decomposition

```{r S6Decomposition}
s6v5 <- ts(s6v5, frequency = 365)

s6v5 %>% stl(s.window = 'periodic') %>% autoplot()
```

It is unclear what the correct time period for this data should be as we have no knowledge of what the data is modeling, so we have tested annual, quarterly, monthly, weekly, and daily seasonality to see if the data fits any of these models. Although there is some seasonal pattern on the daily timeframe, the data is not predictive of itself 1 year in advance so we will not model using seasonality.

### Exponential Smoothing

```{r, echo = F}
ets_s6v5 <- ets(s6v5, model = 'AAN', damped = F)
ets_s6v5 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

ets_s6v7 <- ets(s6v7, model = 'AAN', damped = F)
ets_s6v7 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

This model has an error of 16.69, slightly lower than the baseline.

### ARIMA Model

```{r, echo = F}
# data indicates an AR 1 model from ACF and PACF
arima_s6v5 <- Arima(s6v5, order = c(2,1,2), include.drift = T)
arima_s6v5 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

arima_s6v7 <- Arima(s6v7, order = c(2,1,2), include.drift = T)
arima_s6v7 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')

arima_s6v5
```

The ARIMA model has an error of 16.74, worse than the exponential smoothing model and even underperforming the baseline. It also has a higher level of complexity so it makes sense to select the simpler and more accurate model.

# Analytical Appendix
## Data Preprocessing
The data was converted to CSV, read into R, and then split into separate files
by group.

Each of the separate series was investigated for correlation, autocorrelation,
and stationarity. For each data series, the last 140 periods are those needing
the forecast. The first 1622 periods are the data.

There are missing data elements in the series. Excluding the last 140 entries,
the respective series are missing the following data elements:
```{r missingData, echo=FALSE}
knitr::kable(DT[, lapply(.SD, function(x) sum(is.na(x)) - 140),
   .SDcols = c(3:7), keyby = group])
```

These will be addressed in the individual series discussions.

## Series 01
### Variable 01
As mentioned in the text, originally the entire series was fit to ARIMA models.
As becomes clear in the discussion below, this has problems. The technical
narrative will retain the discussion of the original analysis for verification
purposes, but the last section will just describe the training models, the
best-fitting to the test set, and the selected model for forecasting.

#### Missing Data
With no apparent seasonality, the missing 2 variables for out of 1662 values for
V1 were replaced by linear interpolation.

#### Outliers
For V1, five points were identified as outliers using the `tsoutliers` function
in the `forecast` package.
```{r, S1V1Out}
plot(S1V1, main = "Outliers for V1", ylab = "V1")
points(V1O$index, V1O$replacements, col = 'red', pch = 16L)
points(V1O$index, S1V1[V1O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

As seen in the plot above, the blue points are the actual values and the maroon
points are the suggested replacements. The outliers and their replacements are
not deemed different enough to substitute interpolated data for real.

#### Stationarity
It is clear that there is non-stationarity in V1 when looking at autocorrelation
plots.

```{r S1V1acf, echo=FALSE, fig.width=10L}
ggAcf(S1V1, type = 'correlation') + ggtitle("V1 Autocorrelation")
```

The autocorrelation plot shows slowly decreasing but continually positive
values, a clear sign of the existence of non-stationarity. Using the
*Kwiatkowski-Phillips-Schmidt-Shin* test as suggested by the text implies that a
difference of one lag will be sufficient, as the test statistic falls below the
critical values for any sane level of confidence after
a difference of one lag.

```{r S1V1station, echo=TRUE}
summary(ur.kpss(S1V1))
summary(ur.kpss(diff(S1V1)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S1V1lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S1V1), set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 1")
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V1.
```{r S1V1aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S1V1))
```

The positive ACF lag 1 with cutoff after lag 2 implies that V1 will require a MA
component of at least 2. It's more difficult to immediately identify an AR
component, so an exhaustive search using `auto.arima` will be performed on a
set of \(ARIMA(p, 1, q)\) models, using AICc as our goodness-of-fit measure.
While \(d\) has been robustly estimated as 1, it too will be left blank for use
in the automatic trial-and-error fitting procedures. Lastly, as the entirety of
V1 demonstrates a clear upward trend, fitting will be with drift allowed.

For V1, the best model using maximum likelihood is estimated as an
ARIMA(`r V1p`, `r V1d`, `r V1q`). However, looking at the characteristic roots
gives some pause as the majority of its roots lie near the boundary of the unit
circle. This is despite the author's claim:
"*The `auto.arima()` function is even stricter, and will not select a model*
*with roots close to the unit circle*"
([Section 8.7](https://otexts.com/fpp2/arima-r.html),
Hyndman & Athanasopoulos 2018).

```{r S1V1unitC1}
autoplot(AAV1)
```

A number of other models were investigated, but the ones which brought their
roots well inside the circle suffered from severe deterioration in goodness of
fit (Burnham \& Anderson 2012). Therefore, the original model is retained.

The residuals for this model visually exhibit very little heteroskedasticity, 
and are particularly Gaussian in nature.
```{r S1V1resid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAV1)
```

#### Exponential Smoothing State Space
An exponential smoothing state-space model was fit to the data as well and
returned a `r ETSV1$method` model. The accuracy of the models are compared
below, first the ARIMA model and then the ETS model.
```{r V1wAcc}
accuracy(AAV1)
accuracy(ETSV1)
```

More importantly, since they are fit on the exact same data, their AICc may
be compared. For the ARIMA model it is `r AAV1$aicc` and for the ETS model, it
is `r ETSV1$aicc`. The choice is clear, that the ARIMA model is superior.

#### Selection
The ARIMA(`r V1p`, `r V1d`, `r V1q`) model will be used to forecast Variable 01
for Series 01.

### Variable 02
#### Missing Data
There are no missing values for V2.

#### Outliers
Using the same technique, there are `r length(V2O$index)` outliers identified
for V2.
```{r, S1V2aOut}
plot(S1V2, main = "Outliers for V1", ylab = "V1")
points(V2O$index, V2O$replacements, col = 'red', pch = 16L)
points(V2O$index, S1V2[V2O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

In this case, the outliers are **clearly** extremes. Replacing the outliers with
the suggestions would result in the series in black becoming the one in green.
```{r, S2V2cOut}
plot(S1V2)
lines(S1V2c, col = 'green4')
```

Consideration was given throughout the process to replacing outliers, but
doing so did not result in any significant improvement.

#### Stationarity
It is clear that there is non-stationarity in V2 when looking at autocorrelation
plots.

```{r S1V2acf, echo=FALSE, fig.width=10L}
ggAcf(S1V2, type = 'correlation') + ggtitle("V2 Autocorrelation")
```

The same tests as performed on V1 indicate that a difference of one lag will be
sufficient here as well.

```{r S1V2station, echo=TRUE}
summary(ur.kpss(S1V2))
summary(ur.kpss(diff(S1V2)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S1V2lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S1V2),
          set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 2") +
  scale_x_continuous(breaks = breaks_extended(n = 3L),
                     labels = label_scientific(digits = 1L)) 
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V2.
```{r S1V2aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S1V2))
```

Here, there is certainly more uncertainty as to what parameters should be
selected. Therefore, the best option is once again an exhaustive search.

Using the same exhaustive technique as for V1, the best selected model for V2 is
an ARIMA(`r V2p`, `r V2d`, `r V2q`) model. The autoregressive component for V2
makes sense in the context of the partial autocorrelation plot above; the fourth
partial autocorrelation is larger in magnitude than all that follow. 

However, once again, the characteristic roots raise the specter of instability.
```{r S1V2unitC1, echo=FALSE}
autoplot(AAV2) + ggtitle("Inverse Roots for V2")
```

The residuals look poor as well.
```{r S1V2Resid, echo=FALSE}
checkresiduals(AAV2)
```

Similar to V1, multiple models were investigated including limiting the order of
the ARIMA and replacing the outliers discussed above. However, any model which
showed a meaningful improvement in its roots demonstrated a significant
reduction in goodness of fit.

#### Applying Box-Cox
At this point, despite the initial reluctance to transform the raw data, it
became prudent to investigate a Box-Cox transform. Similar to the cases above,
the important findings and graphics are reprised below.

The best model under order 20 for V2 transformed via Box-Cox (\(\lambda = \)
`r AAV2l$lambda[[1]]`) is once again an ARIMA(`r V2lp`, `r V2ld`, `r V2lq`).
It too cannot be directly compared to the earlier two models, as it is based on
different data. Its characteristic roots are not much better than the original.

```{r S1V2l_unitC2, echo=FALSE}
autoplot(AAV2l)
```

However, its residuals show clear improvement.
```{r S1V2lresid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAV2l)
```

#### Exponential Smoothing State Space
An ETS model with and without Box-Cox was also fit. These returned an
`r ETSV2$method` and `r ETSV2l$method` respectively. The accuracy and AICc are
shown below in the following order: ARIMA without Box-Cox, ARIMA with Box-Cox,
ETS without Box-Cox, and lastly ETS without Box-Cox.
```{r ETSAcc, echo=FALSE}
accuracy(AAV2)
accuracy(AAV2l)
accuracy(ETSV2)
accuracy(ETSV2l)
```

The AICc values are also shown below for comparable models.
```{r ETSAICc, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(data.frame(Name = c('ARIMA', ETSV2$method),
           AICc = c(AAV2$aicc, ETSV2$aicc)))

knitr::kable(data.frame(Name = c('ARIMA+BoxCox', ETSV2l$method),
           AICc = c(AAV2l$aicc, ETSV2l$aicc)))
```

In both cases, the ARIMA variants outperform their exponentially smoothed
counterparts. Moreover, in comparable metrics (e.g. RMSE), the Box-Cox
transformation shows better results (and better residuals).

#### Selection
The ARIMA(`r V2lp`, `r V2ld`, `r V2lq`) with lambda `r AAV2l$lambda[[1]]` model
will be used to forecast Variable 02 for Series 01.

## Series 3
### Missing Value Imputation
Below is the code used to linearly impute the missing values.
```{r s3NA, include = T, echo=TRUE}
sum(is.na(S3))

sum(is.na(S3[,1]))
sum(is.na(S3[,2]))

S3 <- ts(sapply(S3, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S3))
```

### Outliers
```{r S3Outliers}
s31out <- tsoutliers(S3[,1])
s32out <- tsoutliers(S3[,2])

data.frame(S3) %>% ggplot() +
  geom_line(aes(x = 1:length(S3[,1]), y = Var05), color = 'green4') +
  geom_point(data = data.frame(s31out), aes(x = index, y = replacements),
             color = 'blue', size = 2) +
  geom_point(aes(x = s31out$index, y = Var05[s31out$index]),
             color = 'red', size = 2) +
  xlab('Time') + ylab('Values') +
  ggtitle('Var05 With Outlier and Replacement Shown')
```

Due to this outlier being one of the defining differences between the two
timeseries we will not be adjusting the outlier to its recommended replacement.

### Calculating Error
To calculate the error for each model, we created a model at each timestep using
previous data and predict one timestep ahead. We then compare the predicted
value to the actual value to generate an error at this time step. The overall
error is the sum of squared errors.

### Drift Model Error
```{r S3Drift, echo=TRUE}
drift_sse_v1 <- 0
for(i in 100:(length(s3v1)-1)){
  pred <- rwf(s3v1[1:i], h = 1, drift = T)
  error <- abs((s3v1[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  drift_sse_v1 <- drift_sse_v1 + error
}
drift_sse_v1
```

### Exponential Smoothing Error
```{r S3ExpSmthErr, echo=TRUE}
ets_sse <- 0
for(i in 100:(length(s3v1)-1)){
  model <- ets(s3v1[1:i], model = 'AAN')
  pred <- model %>% forecast(h = 1)
  error <- abs((s3v1[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  ets_sse <- ets_sse + error
}
ets_sse
```

### ARIMA Parameter Selection
The ACF plot indicates a strong trend component to the data. the PACF plot
indicates that there is little to no seasonality as entries far in the past are
not predictive of entries in the future. I also used a PACF plot of length 300
to check for any daily seasonality but it appears there is no strong daily
seasonality on the yearly scale either.

```{r S3ARIMA_1, warning=FALSE}
library(tseries)

kpss.test(s3v1)
kpss.test(diff(s3v1))

diff(s3v1) %>% autoplot()
acf(s3v1)
Pacf(s3v1, 370)
```

The decreasing ACF values coupled with the large spike in the PACF values at 1
is indicative of an AR(1) model, leading us to believe that the best ARIMA model
would be an ARIMA(1,1,0) model. Also, the PACF values discount the daily seasonality we found as there is no spike in the PACF graph near 365.

### ARIMA Error
```{r S3ARIMA_2, echo=TRUE}
arima_sse <- 0
for(i in 100:(length(s3v1)-1)){
  model <- Arima(s3v1[1:i], order = c(1,1,0), include.drift = T)
  pred <- model %>% forecast(h = 1)
  error <- abs((s3v1[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  arima_sse <- arima_sse + error
}
arima_sse
```

### Model Residuals
```{r S3Residuals}
checkresiduals(arima_s3v1)
checkresiduals(fit)
```

Both sets of residuals appear to resemble a normal distribution with some strong
outliers due to the volatile nature of the series. However the exponential
smoothing model does not pass  the Ljung-Box test with a p-value of .03,
indicating that some of the residuals from the exponential smoothing model may
be correlated and that the model could be improved. As such, this further
confirms our decision to use the ARIMA model for this series.


## Series 4

### S4V1

#### Outlier Replacement

```{r}
#outliers do not look significant enough to be replaced

plot(S4V1, main = "Outliers for V1", ylab = "V1")
points(S4V1_outliers$index, S4V1_outliers$replacements, col = 'red', pch = 16L)
points(S4V1_outliers$index, S4V1[S4V1_outliers$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

The plot above shows the outliers for the first series in blue and their replacements in red. There does not appear to be a significant difference between the outliers and the replacements given how close they are to each other. As a result, we did not choose to use the replacements for the first series.

#### Stationarity

```{r}
#nonstationary

ggAcf(S4V1, type = 'correlation') + ggtitle("V1 Autocorrelation")
```

The ACF plot of the first series above shows a very clear indication of autocorrelation due to significant spikes across the board for every lag. As further evidence, we conducted a KPSS Unit Root Test on both the standard series and a first order difference of the series to see if differencing would improve the stationarity. According to the tests, differencing appears to improve the stationarity significantly as seen by the value of the test statistic falling under the 1 percent critical value.

```{r}
#one lag is good enough because test statistic falls below 1 percent critical value

summary(ur.kpss(S4V1))
summary(ur.kpss(diff(S4V1)))
```

After taking the first difference, we can see a significant improvement in the stationarity as shown in the lag plots below. None of the lags appear to exhibit a linear pattern, which means that the data is more stationary now.

```{r}
#after differencing, autocorrelation is gone as seen by random pattern
gglagplot(diff(S4V1), set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 1")
```

#### Picking ARIMA Parameters

Looking at the ACF and PACF plots, we can see that there is not a clear indication that an AR or MA model should be used. The only root that is statistically significant is at lag 17 for both plots, but even then, it is not significant by a large margin. Also, there appears to be increasing variance as time increases, which indicates that a Box-Cox transformation may be necessary.

At this point, the candidate model appears to be a Box-Cox transformed ARIMA(0,1,0) model.

```{r}
#lag at 17 may indicate something
ggtsdisplay(diff(S4V1))
```

#### Residuals

After fitting the model, we found that an ARIMA(0,1,1) model had a lower AICc compared to the ARIMA(0,1,0) model. This ARIMA(0,1,1) model exhibited good residuals with no discernable pattern, constant variance, and a normal distribution. In other words, the residuals appear to be Gaussian Noise.

```{r}
#residuals looking good. no discernable patterns, gaussian noise.
checkresiduals(S4V1_boxed_autoarima)
```

### S4V2

#### Outlier Replacement

The plot below shows the outliers for the second series in blue and their replacements in red. As we can see, there are many outliers in this series and that their replacements are much more reasonable values. For this series, replacing the outliers would create better forecasts.

```{r}
#there are many outliers that need to be replaced

plot(S4V2, main = "Outliers for V2", ylab = "V2")
points(S4V2_outliers$index, S4V2_outliers$replacements, col = 'red', pch = 16L)
points(S4V2_outliers$index, S4V2[S4V2_outliers$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

#### Stationarity

According to the ACF plot below, the data exhibits very clear non-stationarity due to the spikes across all lags. Differencing will be necessary to make the series stationary.

```{r}
#nonstationary

ggAcf(S4V2_clean, type = 'correlation') + ggtitle("V2 Autocorrelation")
```

Using a KPSS Unit Root Test, we show that one degree of differencing is sufficient for creating a stationary series. The value of the test statistic falls below the critical value at 1 percent, which indicates that the series is stationary.

```{r}
#one lag is good enough because test statistic falls below 1 percent critical value

summary(ur.kpss(S4V2_clean))
summary(ur.kpss(diff(S4V2_clean)))
```

The lag plot below further reinforces the idea that the once differenced series is stationary given that there is no linear pattern for any lag.


```{r}
#lags look more random now after differencing
gglagplot(diff(S4V2_clean),
          set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 2") +
  scale_x_continuous(breaks = breaks_extended(n = 3L),
                     labels = label_scientific(digits = 1L)) 
```

#### Picking ARIMA Parameters

The ACF graph shows two spikes at lag 1 and lag 2. This would suggest that an ARIMA(p,1,2) model would be appropriate for this series. The PACF graph shows spikes from lag 1 to lag 9. This would suggest that `q` could be any value between 3 and 9. Though, most likely, a value of 3 or 4 would be sufficient for the AR component of this model.

The variance appears to be constant, however, the large spikes in the first half might indicate that a Box-Cox transformation could improve the model.

Given that there are many moving parts for this series, we decided to use the `auto.arima` function to fit a variety of different models at various `p` and `q` values with and without a Box-Cox transformation.

```{r}
#PACF would suggest an ARIMA(3,1,q) or ARIMA(4,1,q) model
#ACF would suggest an ARIMA(p,1,1) or ARIMA(p,1,2) model

ggtsdisplay(diff(S4V2_clean))
```

#### Roots Analysis

The characteristic roots for our best model are pretty good overall given that most of the roots are not close to the unit circle. However, there is one MA root that is close to the circle, which could indicate that the forecast may not be as accurate as we hope.

```{r}
# AR roots look fine, MA root is too close to the unit circle, which indicates that the model might not be good for forecasting
autoplot(S4V2_boxed_outliers_autoarima)
```

#### Residuals

After running the `auto.arima` function, we found that an ARIMA(3,1,1) model with smoothed outliers and a Box-Cox transformation gave the best looking results. As seen in the residual plots below, the variance of the noise is constant, there is no indication of autocorrelation of the residuals, and the residuals are normally distributed. All these factors would indicate that the resdiuals are white noise.

```{r}
#the outlier replacement model appears to be the best compared to the other models

checkresiduals(S4V2_boxed_outliers_autoarima)
```

## Series 6

### Missing Value Imputation

Below is the code for imputing the missing values. Missing values are imputed linearly as an average between the last two known values.

```{r s6NA-exec, echo=T, output = F}
sum(is.na(S6))

sum(is.na(S6[,1]))
sum(is.na(S6[,2]))

S6 <- ts(sapply(S6, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S6))
```

### Outliers

Here is the code used to clean the outlier.

```{r}
s61out <- tsoutliers(S6_orig[,1])
s62out <- tsoutliers(S6_orig[,2])

data.frame(S6_orig) %>% ggplot() +
  geom_line(aes(x = 1:length(S6_orig[,1]), y = Var05), color = 'green4') +
  geom_point(data = data.frame(s61out), aes(x = index, y = replacements),
             color = 'blue', size = 2) +
  geom_point(aes(x = s61out$index, y = Var05[s61out$index]),
             color = 'red', size = 2) +
  xlab('Time') + ylab('Values') +
  ggtitle('Var05 With Outlier and Replacement Shown')
```

### Calculating Error

Error is calculated using hold one out validation where a model is build on all previous data and then forecasts the next point. Error statistic reported is the sum of squared errors across all data points.

### Drift Model Error

```{r S6DriftError, echo=TRUE}
drift_sse_v1 <- 0
for(i in 100:(length(s6v5)-1)){
  pred <- rwf(s6v5[1:i], h = 1, drift = T)
  error <- abs((s6v5[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  drift_sse_v1 <- drift_sse_v1 + error
}
drift_sse_v1
```

### Exponential Smoothing Error

```{r S6ExpSmthErr, echo=TRUE}
ets_sse <- 0
for(i in 100:(length(s6v5)-1)){
  model <- ets(s6v5[1:i], model = 'AAN')
  pred <- model %>% forecast(h = 1)
  error <- abs((s6v5[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  ets_sse <- ets_sse + error
}
ets_sse
```

### ARIMA Parameter Selection

```{r S6ACFDiag}
library(tseries)

kpss.test(s6v5)
kpss.test(diff(s6v5))

diff(s6v5) %>% autoplot()
acf(s6v5)
Pacf(s6v5, 20)
Pacf(s6v5, 370)
```

The spike in the PACF at 2 may indicate an AR component of 2 in the ARIMA model. There also seems to be some alternating nature to the PACF graph with small spikes at 4 and 6, which may indicate an oscillating nature to the data which would be well modeled by an MA 2 component.

The data needs a single differencing to become a stationary series.

### ARIMA Model Error

```{r S6ARIMA_2, echo=TRUE}
arima_sse <- 0
for(i in 100:(length(s6v5)-1)){
  model <- Arima(s6v5[1:i], order = c(2,1,2), method = 'CSS', include.drift = T)
  pred <- model %>% forecast(h = 1)
  error <- abs((s6v5[i+1] - as.numeric(pred$mean))/as.numeric(pred$mean))
  arima_sse <- arima_sse + error
}
arima_sse
```

### Residuals

```{r S6Residuals}
checkresiduals(arima_s6v5)
checkresiduals(ets_s6v5)
```

The residuals confirm our suspicions: The exponential smoothing model is likely a better fit as the residuals from the ARIMA have a significant level of correlation with each other, meaning the ARIMA model has not fully modeled the data. The exponential smoothing model however has non correlated residuals and is therefore the better model choice.


# References
 * Hyndman, R.J., & Athanasopoulos, G. (2018)
*Forecasting: principles and practice*, 2nd edition, OTexts: Melbourne,
Australia. OTexts.com/fpp2. Accessed on 2020-06-21

 * Burnham, Kenneth P., and Anderson, David R. (2002).
 *Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach*.
 Second. New York: Springer Science+Business Media, Inc.