---
title: "CUNY DT 624"
subtitle: "Project 1"
author: "Group 1: Avraham Adler, Vishal Arora, Samuel Bellows, Austin Chan"
date: "Summer 2020"
output:
  word_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readxl)
library(fpp2)
library(urca)
library(ggplot2)
library(scales)
library(data.table)

# Read in Data
DT <- read_xls('./project1data/Data Set for class.xls')
# Set as data table
setDT(DT)
# Seperate out groups
n <- length(unique(DT$group))
for (i in seq_len(n)) {
  gName <- paste0('S0', i)
  assign(gName, DT[group == gName])
}

# Following is useful to quickly remove "future" periods from groups
futureobs <- 1623:1762
```

# Introduction
The purpose of this report is to describe the analysis performed in order to
forecast the following series, each for 140 periods.

 * S01: Forecast Var01 and Var02
 * S02: Forecast Var02 and Var03
 * S03: Forecast Var05 and Var07
 * S04: Forecast Var01 and Var02
 * S05: Forecast Var02 and Var03
 * S06: Forecast Var05 and Var07
 
The forecasts will be provided in a separate Excel document. The code for the
analysis can be found in the attached R markdown document.

The **executive summmary** will contain the key findings. The more detailed
analysis may be found in the **analyitical appendix** for those interested in
the findings and decisions applied.

# Executive Summary
## Series 01
### Observations
Variables 01 and 02 (V1 & V2 from now on) of Series 01 (S1 from now on) can each
be considered a time series. As few transformations as possible will be applied
to the data to maximize fidelity to the original source.
```{r S1ts, echo=FALSE}
S1ts <- ts(S01[-futureobs, 3:4])
plot(S1ts)
```

Clearly V1 and V2 are negatively correlated with each other, as V1 has an
increasing secular trend whereas V2 has an overall decreasing one. This is even
more apparent when they are plotted after center-scaling and normalization.
```{r S1scaleV, echo=FALSE, fig.width=8L, fig.height=6L}
V1 <- scale(S01$Var01[-futureobs])
V2 <- scale(S01$Var02[-futureobs])
plot(V2, type = 'l', col = 'green4', main = "Series 01 Variables",
     xlab = "Time", ylab = "Scaled Values", ylim = c(-2, 8))
lines(V1, col = 'blue')
legend('topright', legend = c('V1', 'V2'), col = c('blue', 'green4'), lty = 1L)
```
```{r ARIMACalcs, include=FALSE}
S1V1 <- na.interp(S1ts[, 1])
S1V2 <- S1ts[, 2]
V1O <- tsoutliers(S1V1)
V2O <- tsoutliers(S1V2)
S1V2c <- tsclean(S1V2)
AAV1 <- auto.arima(S1V1, max.order = 20L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, allowdrift = FALSE, seasonal = FALSE,
                   parallel = TRUE)
AAV2 <- auto.arima(S1V2, max.order = 20L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, allowdrift = FALSE, seasonal = FALSE,
                   parallel = TRUE)
V1p <- AAV1$arma[[1]]
V1q <- AAV1$arma[[2]]
V1d <- AAV1$arma[[5]]
V2p <- AAV2$arma[[1]]
V2q <- AAV2$arma[[2]]
V2d <- AAV2$arma[[5]]
AAV1b <- auto.arima(S1V1, max.order = 6L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, allowdrift = FALSE, seasonal = FALSE,
                   parallel = TRUE)
AAV2b <- auto.arima(S1V2, max.order = 6L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, allowdrift = FALSE, seasonal = FALSE,
                   parallel = TRUE)
V1bp <- AAV1b$arma[[1]]
V1bq <- AAV1b$arma[[2]]
V1bd <- AAV1b$arma[[5]]
V2bp <- AAV2b$arma[[1]]
V2bq <- AAV2b$arma[[2]]
V2bd <- AAV2b$arma[[5]]

AAV2c <- auto.arima(S1V2c, max.order = 20L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, allowdrift = FALSE, seasonal = FALSE,
                   parallel = TRUE)
V2cp <- AAV2c$arma[[1]]
V2cq <- AAV2c$arma[[2]]
V2cd <- AAV2c$arma[[5]]
AAV2d <- auto.arima(S1V2, max.order = 20L, ic = 'aicc', stepwise = FALSE,
                   approximation = FALSE, allowdrift = FALSE, seasonal = FALSE,
                   parallel = TRUE, lambda = 'auto')
V2dp <- AAV2d$arma[[1]]
V2dq <- AAV2d$arma[[2]]
V2dd <- AAV2d$arma[[5]]
```

### Stationarity
Both variables, as they stand are non-stationary. Statistical tests indicate
that a difference of lag 1 is sufficient to induce stationarity.

### Seasonality
Once differenced, neither variable exhibits seasonality.

### Missing Data
With no apparent seasonality, the missing 2 variables for out of 1662 values for
V1 were imputed by linear interpolation. There are no missing values for V2.

### Transformations
After much analsyis, V1 was modeled with neither outlier replacement nor
transform and V2 was modeled without explicit outlier replacement but with a
Box-Cox transform to reduce variability.

### ARIMA parameters
The selected model for V1 is an ARIMA(`r V1bp`, `r V1bd`, `r V1bq`) for 1 and
for V2 it is an ARIMA(`r V2p`, `r V2d`, `r V2q`) with a Box-Cox \(lambda\) of
`r AAV2d$lambda[[1]]`

### Forecasts
The mean forcast values will be supplied in the attached Excel workbook. The
mean forecasts with error bands are plotted below
```{r V1plot, echo=FALSE}
plot(forecast(AAV1b, h = 140), main = "Forecast for V1")
plot(forecast(AAV2d, h = 140, biasadj = TRUE), main = "Forecast for V2")
```

## Series 3

### Series Visualization
```{r s3}
S3 <- ts(S03[-futureobs, 6:7])
autoplot(S3) + ylab('Value') + xlab('Time') + ggtitle('Var05 vs Var07')
```

The two series of interest appear to be extremely similar, although one of the series appears to be slightly more volatile than the other. Likely the forecasts for these two series will be almost identical. We can also see on the graph what appears to be missing values towards the very end of the series.

The data appear to have a strong trend but no clear seasonal pattern, which will aid us in our modeling in the future.

### Missing Values

missing values were imputed linearly to create a smooth timeseries.

```{r s3NA-exec, echo = F, include = F}
sum(is.na(S3))

sum(is.na(S3[,1]))
sum(is.na(S3[,2]))

S3 <- ts(sapply(S3, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S3))
```

### Outliers
The series have a single outlier between them which is also apparent in the plot of the timeseries above. However, since the series are so similar and this is one of the defining differences of the series, I think it would be unwise to remove this outlier.

### Baseline Model

Since the data has a clear trend component but no seasonality component, a simple random walk with drift will serve as a good baseline model.

```{r s3var1-drift, echo = F}
s3v1 <- S3[,1]
s3v2 <- S3[,2]

rwf(s3v1, h = 140, drift = T) %>% autoplot() + ylab('Var05')
rwf(s3v2, h = 140, drift = T) %>% autoplot() + ylab('Var07')
```

The cumulative error for our baseline model is 3700, which is meaningless in and of itself but will be an excellent comparison point for other models.

### Exponential Smoothing model

Due the the nature of the data, a linear trend model is a natural choice as an attempt to model the data.

```{r, echo = F}
fit <- ets(s3v1, model = 'AAN', damped = F)
fit %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

fit2 <- ets(s3v2, model = 'AAN', damped = F)
fit2 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

Exponential Smoothing models achieved an error of 3650, a slight improvement on the baseline model.

### Differencing Data

The data is clearly non stationary which means it is not appropriate for ARIMA modeling. We can solve this by taking the first (or potentially second) difference between observations.

The new differnced data is stationary and therefore appropriate for ARIMA modeling. Diagnostic tests suggest either an ARIMA(1,1,0) model or an ARIMA(2,1,0) model.

### ARIMA Model

The most appropriate ARIMA model is an ARIMA(1,1,0) model as the data is clearly not stationary and needs to be differenced. This choice of model parameters can be confirmed both by data visualization and by the auto arima function in R.

```{r, echo = F}
# data indicates an AR 1 model from ACF and PACF
arima_s3v1 <- Arima(s3v1, order = c(1,1,0), include.drift = T)
arima_s3v1 %>% forecast(h = 140) %>% autoplot() + ylab('Var05')

arima_s3v2 <- Arima(s3v2, order = c(1,1,0), include.drift = T)
arima_s3v2 %>% forecast(h = 140) %>% autoplot() + ylab('Var07')
```

The ARIMA model has an error of 3630, the lowest of the 3 models. All 3 models make very similar looking positions so we will select the ARIMA model as it has the lowest error across the validation period.

# Analytical Appendix
## Data Preprocessing
The data was converted to CSV, read into R, and then split into separate files
by group.

Each of the separate series was investigated for correlation, autocorrelation,
and stationarity. For each data series, the last 140 periods are those needing
the forecast. The first 1622 periods are the data.

There are missing data elements in the series. Excluding the last 140 entries,
the respective series are missing the following data elements:
```{r missingData, echo=FALSE}
knitr::kable(DT[, lapply(.SD, function(x) sum(is.na(x)) - 140),
   .SDcols = c(3:7), keyby = group])
```

These will be addressed in the individual series discussions.


## Series 01
### Variable 01
#### Missing Data
With no apparent seasonality, the missing 2 variables for out of 1662 values for
V1 were replaced by linear interpolation.

#### Outliers
For V1, five points were identified as outliers using the `tsoutliers` function
in the `forecast` package.
```{r, S1V1Out}
plot(S1V1, main = "Outliers for V1", ylab = "V1")
points(V1O$index, V1O$replacements, col = 'red', pch = 16L)
points(V1O$index, S1V1[V1O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

As seen in the plot above, the blue points are the actual values and the maroon
points are the suggested replacements. The outliers and their replacements are
not deemed different enought to substitute interpolated data for real.

#### Stationarity
It is clear that there is non-stationarity in V1 when looking at autocorrelation
plots.

```{r S1V1acf, echo=FALSE, fig.width=10L}
ggAcf(S1V1, type = 'correlation') + ggtitle("V1 Autocorrelation")
```

The autocorrelation plot shows slowly decreasing but continually positive
values, a clear sign of the existence of non-stationarity. Using the
*Kwiatkowski-Phillips-Schmidt-Shin* test as suggested by the text implies that a
difference of one lag will be sufficient, as the test statistic falls below the
critical values for any sane level of confidence after
a difference of one lag.

```{r S1V1station, echo=TRUE}
summary(ur.kpss(S1V1))
summary(ur.kpss(diff(S1V1)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S1V1lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S1V1), set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 1")
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V1.
```{r S1V1aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S1V1))
```

The positive ACF lag 1 with cutoff after lag 2 implies that V1 will require a MA
component of at least 2. It's more difficult to immediately identify an AR
componenent, so an exhaustive search using `auto.arima` will be performed on a
set of \(ARIMA(p, 1, q)\) models, using AICc as our goodness-of-fit measure.
While \(d\) has been robsutly estimated as , it too will be left blank for use
in the automatic trial-and-error fitting procedures

For V1, the best model using maximum likelihood is estimated as an
ARIMA(`r V1p`, `r V1d`, `r V1q`). However, looking at the characteristic roots
implies that this models may be unstable, as the majority of its roots lie on
the boundary of the unit circle. This is despite the author's claim:
"*The `auto.arima()` function is even stricter, and will not select a model*
*with roots close to the unit circle*"
([Section 8.7](https://otexts.com/fpp2/arima-r.html),
Hyndman & Athanasopoulos 2018).

```{r S1V1unitC1}
autoplot(AAV1)
```

As models with too many roots near the unit circle are not good for forecasting,
the total order of the ARIMA model will be constrained to 6, and tried again.
This time, the selected model is an ARIMA(`r V1bp`, `r V1bd`, `r V1bq`) whose
characteristic roots are well within the unit circle.

```{r S1V1unitC2}
autoplot(AAV1b)
```

Furthermore, the difference in AICc between this and the original model is
minuscule: `r AAV1b$aicc - AAV1$aicc`. According to the rules of thumb of
Burnham and Anderson (2012), that implies functionally no difference between the
models.

#### Residuals
The residuals for this model visually exhibit very little heteroskedasticity, 
and are particularly Gaussian in nature.
```{r S1V1resid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAV1b)
```

#### Selection
The ARIMA(`r V1bp`, `r V1bd`, `r V1bq`)model will be used to forecast Variable
01 for Series 01.

### Variable 02
#### Missing Data
There are no missing values for V2.

#### Outliers
Using the same technique, there are `r length(V2O$index)` outliers identified
for V2.
```{r, S1V2aOut}
plot(S1V2, main = "Outliers for V1", ylab = "V1")
points(V2O$index, V2O$replacements, col = 'red', pch = 16L)
points(V2O$index, S1V2[V2O$index], col = 'blue', pch = 16L)
legend('topleft', c("Outlier", "Replacement"), pch = 16L,
       col = c('blue', 'red'))
```

In this case, the outliers are **clearly** extremes. Replacing the ouliers with
the suggestions would result in the series in black becoming the one in green.
```{r, S2V2cOut}
plot(S1V2)
lines(S1V2c, col = 'green4')
```

At this point, the analysis proceeded **without** replacing the outliers, but
that will change once the model is fit below for reasons that will be explained.

#### Stationarity
It is clear that there is non-stationarity in V2 when looking at autocorrelation
plots.

```{r S1V2acf, echo=FALSE, fig.width=10L}
ggAcf(S1V2, type = 'correlation') + ggtitle("V2 Autocorrelation")
```

The same tests as performed on V1 indicate that a difference of one lag will be
sufficient here as well.

```{r S1V2station, echo=TRUE}
summary(ur.kpss(S1V2))
summary(ur.kpss(diff(S1V2)))
```

#### Seasonality
Once differenced, there is no apparent seasonality at any reasonable lag value.
```{r S1V2lagplots, echo=FALSE, fig.width=10L}
gglagplot(diff(S1V2),
          set.lags = c(1, 2, 3, 4, 5, 6, 7, 12, 30, 91, 182, 365)) +
  ggtitle("Lag Plots for Differenced Variable 2") +
  scale_x_continuous(breaks = breaks_extended(n = 3L),
                     labels = label_scientific(digits = 1L)) 
```

#### ARIMA parameters
Below are auto and partial autocorrelation plots for of the differenced values
of V2.
```{r S1V2aP, echo=FALSE, fig.width=10L}
ggtsdisplay(diff(S1V2))
```

Here, there is cerainly more uncertainty as to what parameters should be
selected. Therefore, the best option is once again an exhaustive search.

Using the same exhaustive technique as for V1, the best selected model for V2 is
an ARIMA(`r V2p`, `r V2d`, `r V2q`) model. The autoregressive component for V2
makes sense in the context of the partial autocorrelation plot above; the fourth
partial autocorrelation is larger in magnitude than all that follow. 

However, once again, the characteristic roots imply instability.
```{r S1V2unitC1, echo=FALSE}
autoplot(AAV2) + ggtitle("Inverse Roots for V2")
```

Limiting the order to a maximum of 6 returns an
ARIMA(`r V2bp`, `r V2bd`, `r V2bq`) model. Unfortunately, this model too
exhibits instability.

```{r S1V2unitC2, echo=FALSE}
autoplot(AAV2b) + ggtitle("Inverse Roots for V2")
```

Moreover, the loss in AICc is `r AAV2b$aicc - AAV2$aicc`, implying very little
support for the second model. Even relaxing the order to 7 does not help. The
next "best" model is the order 8 ARIMA(4, 1, 4) found earlier. This is also the
best model for orders up to 50 using `auto.arima` with its exhaustive search
parameters set.

#### Replacing the Outliers
At this point, consideration was given to replacing the outliers found above.
Quickly reprising the key graphics, this is a better behaved series.
```{r S1V2c_1, echo=FALSE}
ggAcf(S1V2c, type = 'correlation') + ggtitle("V2(repaired) Autocorrelation")
```
```{r S1V2c_2, echo=TRUE}
summary(ur.kpss(S1V2c))
summary(ur.kpss(diff(S1V2c)))
```
```{r S1V2c_3, echo=TRUE}
ggtsdisplay(diff(S1V2c))
```

The best model under order 20 for the repaired version of Variable two is an
ARIMA(`r V2cp`, `r V2cd`, `r V2cq`). It cannot be directly compared to the
earlier two models, as it is based on different data. However, its
characteristic roots are safer (not by much, though).

```{r S1V2c_unitC2, echo=FALSE}
autoplot(AAV2c)
```

#### Applying Box-Cox
Given that there are transformations being performed to V2, it is prudent to
investigate a Box-Cox transform too. Similar to the cases above, the important
findings and graphics are reprised below.

The best model under order 20 for V2 transformed via Box-Cox with
\(\lambda = \)`r AAV2d$lambda[[1]]` is once again an
ARIMA(`r V2dp`, `r V2dd`, `r V2dq`). It too cannot be directly compared to the
earlier two models, as it is based on different data. Its characteristic roots
not much better than the original either.

```{r S1V2d_unitC2, echo=FALSE}
autoplot(AAV2d)
```

#### Residuals
At this point, it pays to compare the residuals of the three models for V2, in
order.
```{r S1V2dresid, echo=FALSE, fig.width=10L, fig.height=8L}
checkresiduals(AAV2, main = "Residuals for Raw V2")
checkresiduals(AAV2c, main = "Residuals for outlier-replaced V2")
checkresiduals(AAV2d, main = "Residuals for Box-Cox transformed V2")
```

Clearly, the best residuals belong to the Box-Cox transformed model.

#### Selection
The ARIMA(`r V2dp`, `r V2dd`, `r V2dq`) with lambda `r AAV2d$lambda[[1]]` model
will be used to forecast Variable 02 for Series 01.

## Series 3
### Missing Value Imputation
Below is the code used to linearly impute the missing values.
```{r s3NA, include = T}
sum(is.na(S3))

sum(is.na(S3[,1]))
sum(is.na(S3[,2]))

S3 <- ts(sapply(S3, function(X) approxfun(seq_along(X), X)(seq_along(X))))

sum(is.na(S3))
```

### Outliers
```{r S3Outliers}
s31out <- tsoutliers(S3[,1])
s32out <- tsoutliers(S3[,2])

data.frame(S3) %>% ggplot() +
  geom_line(aes(x = 1:length(S3[,1]), y = Var05), color = 'green4') +
  geom_point(data = data.frame(s31out), aes(x = index, y = replacements),
             color = 'blue', size = 2) +
  geom_point(aes(x = s31out$index, y = Var05[s31out$index]),
             color = 'red', size = 2) +
  xlab('Time') + ylab('Values') +
  ggtitle('Var05 With Outlier and Replacement Shown')
```

Due to this outlier being one of the defining differences between the two
timeseries we will not be adjusting the outlier to its recommended replacement.

### Calculating Error
To calculate the error for each model, we created a model at each timestep using
previous data and predict one timestep ahead. We then compare the predicted
value to the actual value to generate an error at this time step. The overall
error is the sum of squared errors.

### Drift Model Error
```{r S3Drift, echo=TRUE}
drift_sse_v1 <- 0
for(i in 100:(length(s3v1)-1)){
  pred <- rwf(s3v1[1:i], h = 1, drift = T)
  error <- as.numeric(pred$mean) - s3v1[i+1]
  drift_sse_v1 <- drift_sse_v1 + error**2
}
drift_sse_v1
```

### Exponential Smoothing Error
```{r S3ExpSmthErr, echo=TRUE}
ets_sse <- 0
for(i in 100:(length(s3v1)-1)){
  model <- ets(s3v1[1:i], model = 'AAN')
  pred <- model %>% forecast(h = 1)
  error <- as.numeric(pred$mean) - s3v1[i+1]
  ets_sse <- ets_sse + error**2
}
ets_sse
```

### ARIMA Parameter Selection
The ACF plot indicates a strong trend component to the data. the PACF plot
indicates that there is little to no seasonality as entries far in the past are
not predictive of entries in the future. I also used a PACF plot of length 300
to check for any daily seasonality but it appears there is no strong daily
seasonality on the yearly scale either.

```{r S3ARIMA_1, warning=FALSE}
library(tseries)

kpss.test(s3v1)
kpss.test(diff(s3v1))

diff(s3v1) %>% autoplot()
acf(s3v1)
Pacf(s3v1)
Pacf(s3v1, 370)
```

The decreasing ACF values coupled with the large spike in the PACF values at 1
is indicative of an AR(1) model, leading us to believe that the best ARIMA model
would be an ARIMA(1,1,0) model.

### ARIMA Error
```{r S3ARIMA_2, echo=TRUE}
arima_sse <- 0
for(i in 100:(length(s3v1)-1)){
  model <- Arima(s3v1[1:i], order = c(1,1,0), include.drift = T)
  pred <- model %>% forecast(h = 1)
  error <- as.numeric(pred$mean) - s3v1[i+1]
  arima_sse <- arima_sse + error**2
}
```

### Model Residuals
```{r S3Residuals}
checkresiduals(arima_s3v1)
checkresiduals(fit)
```

Both sets of residuals appear to resemble a normal distribution with some strong
outliers due to the volatile nature of the series. However the exponential
smoothing model does not pass  the Ljung-Box test with a p-value of .03,
indicating that some of the residuals from the exponential smoothing model may
be correlated and that the model could be improved. As such, this further
confirms our decision to use the ARIMA model for this series.

# References
 * Hyndman, R.J., & Athanasopoulos, G. (2018)
*Forecasting: principles and practice*, 2nd edition, OTexts: Melbourne,
Australia. OTexts.com/fpp2. Accessed on 2020-06-21

 * Burnham, Kenneth P., and Anderson, David R. (2002).
 *Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach*.
 Second. New York: Springer Science+Business Media, Inc.