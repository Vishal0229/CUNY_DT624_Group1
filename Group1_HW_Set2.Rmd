---
title: "CUNY DT 624"
subtitle: "Homework Set 2"
author: "Group 1: Avraham Adler, Vishal Arora, Samuel Bellows, Austin Chan"
date: "Summer 2020"
output:
  word_document:
  toc: true
toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(caret)
library(doParallel)
library(RANN)
library(elasticnet)
library(brnn)
library(earth)
library(kernlab)
library(data.table)
cl <- parallel::makePSOCKcluster((parallel::detectCores() * 0.5))
registerDoParallel(cl)
```
# KJ Question 6.3
## Question
A chemical manufacturing process for a pharmaceutical product was
discussed in Sect. 1.4. In this problem, the objective is to understand the
relationship between biological measurements of the raw materials (predictors),
measurements of the manufacturing process (predictors), and the response of
product yield. Biological predictors cannot be changed but can be used to assess
the quality of the raw material before processing. On the other hand,
manufacturing process predictors can be changed in the manufacturing process.
Improving product yield by 1\% will boost revenue by approximately one hundred
thousand dollars per batch:

  (a) Start R and use these commands to load the data:

```
library(AppliedPredictiveModeling)
data(chemicalManufacturingProcess)
```

  The matrix `processPredictors` contains the 57 predictors (12 describing the
  input biological material and 45 describing the process predictors) for the
  176 manufacturing runs. yield contains the percent yield for each run.

  (b) A small percentage of cells in the predictor set contain missing values.
  Use an imputation function to fill in these missing values (e.g., see Sect.
  3.8).

  (c) Split the data into a training and a test set, pre-process the data, and
  tune a model of your choice from this chapter. What is the optimal value of
  the performance metric?

  (d) Predict the response for the test set. What is the value of the performance
  metric and how does this compare with the resampled performance metric on the
  training set?

  (e) Which predictors are most important in the model you have trained? Do
  either the biological or process predictors dominate the list?

  (f) Explore the relationships between each of the top predictors and the
  response. How could this information be helpful in improving yield in future
  runs of the manufacturing process?

## Answers
The excellent `caret` package will be used to coordinate the supervised
learning, together with any necessary packages. For convenience, data frames may
be turned into `data.tables`.

In general, it is considered optimal to split the data into three pieces:
training, validation, and testing. Where the training set is used to train a
model of a given type, the validation set is used to estimate the prediction
error for the model, and the test set only brought out at the end to estimate
general error (Hastie et al 2001, p. 222). However, as we are only supposed to
pick one model for this exercise, the data will be broken into two pieces.

### Part (a)
```{r KJ63a}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
CMP <- as.data.table(ChemicalManufacturingProcess)
```

### Part (b)
There are data fields with missing observations.

```{r KJ63b}
knitr::kable(unlist(CMP[, lapply(.SD, function (x) {sum(is.na(x))})])[
  which(CMP[, lapply(.SD, function (x) {sum(is.na(x))})
  ] > 0)],
  col.names = "NumMissing", format = 'pandoc')
```

The values will be imputed using K-nearest neighbors which also automatically
implies a centering and scaling of the data. If imputation is desired without
centering and scaling, one may use a bagged-tree imputation method instead.

### Part (c)
The data will be divided equally, and randomly, into a training and test set.
For repeatability, a random seed will be set.

```{r KJ63c1}
set.seed(674835)
trainObs <- sample.int(n = dim(CMP)[[1]], size = dim(CMP)[[1]] %/% 2)
trainCMPX <- as.matrix(CMP[trainObs, -1])
trainCMPY <- unlist(CMP[trainObs, 1])
testCMPX <- as.matrix(CMP[-trainObs, -1])
testCMPY <- unlist(CMP[-trainObs, 1])
```

The model selected from the chapter will be an elastic net, combining the (best)
features of ridge regression and the feature selection of lasso. The tuning will
be based on a five-time repeated 11-fold cross validation, as both the training
and testing data have 88 observations. The tuning will use
RMSE as the performance metric to optimize.

Furthermore, initial training showed that there were columns with zero or
near-zero variance. These will be pre-processed as well. Lastly, the default
in caret is to provide a grid of three values for the ridge lambda and three for
the \(L_1\) fraction used for the lasso component. For this problem, a large
number of very fine mesh grids were tried, with one zoomed in on the best result
supplied below.

```{r KJ63c2}
fitControl <- trainControl(method = "repeatedcv", number = 11L, repeats = 5L)
tGrid <- expand.grid(fraction = seq(0.028, 0.029, 1e-4),
                     lambda = c(0, 1e-4, 1e-3))
elastiFit <- train(x = trainCMPX, y = trainCMPY, method = 'enet',
                   preProcess = c('zv', 'nzv', 'knnImpute'),
                   trControl = fitControl, tuneGrid = tGrid)
```

The value of the performance at the selected optimal point is an RMSE of
`r min(elastiFit$results$RMSE)` at an \(L_1 \) fraction of
`r elastiFit$bestTune[[1]]` and a ridge lambda of `r elastiFit$bestTune[[2]]`.
Note that this is thus actually a true lasso fit!

### Part (d)
```{r KJ63d}
elastiPred <- predict(elastiFit, testCMPX)
testResult <- defaultSummary(data.frame(obs = testCMPY,  pred = elastiPred))
```

The RMSE for the test data set is `r testResult[[1]]` which is greater than that
for the training set. This is to be expected, as there is bound to be be some
overfitting on the training set, despite the use of cross-validation.

### Part (e)
The top 10 variables by importance are shown below:
```{r KJ63e}
elastiVarImp <- varImp(elastiFit)
nI <- names(CMP)[-1]
t10E <- data.frame(nI, elastiVarImp$importance)[
  order(elastiVarImp$importance, decreasing = TRUE)[1:10], ]
t10EN <- t10E$nI
knitr::kable(t10E, row.names = FALSE, format = 'pandoc', digits = 2)
```

For this fit, the **Manufacturing** components completely dominate the list.

### Part (f)
```{r KJ63f, fig.height=8L, fig.width=10L}
par(mfrow = c(4, 3))
for (i in seq_along(t10N)) {
  plot(x = unlist(CMP[, .SD, .SDcols = t10N[i]]), CMP$Yield,
       xlab = t10N[i], ylab = "Yield")
}
par(mfrow = c(1, 1))
```

`r t10N[1]`, `r t10N[7]`, and to a lesser extent `r t10N[5]` demonstrate a
positive relation with yield. Conversely, `r t10N[2]` and `r t10N[3]` appear to
be negatively correlated with yield. The remaining variables could almost be
outlier examples from Anscombe's famous quartet.

At this point, I would suggest that another model family should be investigated
and test-set behavior should be compared.

# KJ Question 7.2

# KJ Question 7.5
## Question
Exercise 6.3 describes data for a chemical manufacturing process. Use the same
data imputation, data splitting, and pre-processing steps as before and train
several nonlinear regression models.

  (a) Which nonlinear regression model gives the optimal resampling and test
  set performance?
  (b) Which predictors are most important in the optimal nonlinear regression
  model? Do either the biological or process variables dominate the list? How do
  the top ten important predictors compare to the top ten predictors from the
  optimal linear model?
  (c) Explore the relationships between the top predictors and the response for
  the predictors that are unique to the optimal nonlinear regression model. Do
  these plots reveal intuition about the biological or process predictors and
  their relationship with yield?

## Answers
### Part a
As several models are requested, yet the same train/test split are mandated, the
"best" model will have to be selected using cross-validation on the training set
instead of having a tripartite training/testing/validation schema as per ESL.

Various models from chapter 7 will be investigated.[^kj75a] Similar to question
6.3, many versions of each model were fit. The one which is displayed is usually
a model whose tuning grid is focused on the area of best behavior.[^kj75b] Seeds
will be set before each call to ensure the repeatability of the results. The
metric used for tuning and comparison will be **RMSE**.

[^kj75a]: My hunch is that this data will respond best to a random forest or
boosted/bagged tree approach, but that is chapter 8.
[^kj75b]: This is a practical accommodation necessary to reduce the load on both
the machines and the times of the group members.

```{r KJ75preprocess}
set.seed(674835)
trainObs <- sample.int(n = dim(CMP)[[1]], size = dim(CMP)[[1]] %/% 2)
trainCMPX <- as.matrix(CMP[trainObs, -1])
trainCMPY <- unlist(CMP[trainObs, 1])
testCMPX <- as.matrix(CMP[-trainObs, -1])
testCMPY <- unlist(CMP[-trainObs, 1])
modelC <- data.frame(Model = character(0), TrnErr = double(0),
                     TstErr = double(0))
```

#### Simple Neural Network
```{r KJ75aNN}
fitControl <- trainControl(method = "repeatedcv", number = 11L, repeats = 5L,
                           allowParallel = TRUE)
nnGrid <- expand.grid(decay = c(0, 1e-2, 1e-1), size = 1:4,
                      bag = c(FALSE, TRUE))
set.seed(12)
nnFit <- train(x = trainCMPX, y = trainCMPY, method = 'avNNet',
               preProcess = c('zv', 'nzv', 'knnImpute'), trControl = fitControl,
               tuneGrid = nnGrid, trace = FALSE)
modelC[1, ] <- data.frame(model = "NeuralNet",
                          TrnErr = RMSE(predict(nnFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(nnFit, testCMPX), testCMPY))
```

#### Bayesian Neural Network
```{r KJ75aBNN}
set.seed(12)
bnnFit <- train(x = trainCMPX, y = trainCMPY, method = 'brnn',
                preProcess = c('zv', 'nzv', 'knnImpute'),
                trControl = fitControl, tuneLength = 3)
modelC[2, ] <- data.frame(model = "Bayesian NeuralNet",
                          TrnErr = RMSE(predict(bnnFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(bnnFit, testCMPX), testCMPY))
```

#### Multivariate Adaptive Regression Splines
```{r KJ75aMARS}
marsGrid <- expand.grid(degree = 1:2, nprune = 1:6)
set.seed(12)
marsFit <- train(x = trainCMPX, y = trainCMPY, method = 'earth',
                 preProcess = c('zv', 'nzv', 'knnImpute'),
                 trControl = fitControl, tuneGrid = marsGrid)
modelC[3, ] <- data.frame(model = "MARS",
                          TrnErr = RMSE(predict(marsFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(marsFit, testCMPX), testCMPY))
```

#### Support Vector Machine   
```{r KJ75aSVM}
svmRGrid <- expand.grid(C = seq(124.60, 124.85, 0.01),
                        sigma = seq(4.39e-4, 4.41e-4, 1e-6))
set.seed(12)
svmFit <- train(x = trainCMPX, y = trainCMPY, method = 'svmRadialSigma',
                preProcess = c('zv', 'nzv', 'knnImpute'),
                trControl = fitControl, tuneGrid = svmRGrid)
modelC[4, ] <- data.frame(model = "Support Vector Machine",
                          TrnErr = RMSE(predict(svmFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(svmFit, testCMPX), testCMPY))
```

#### Relevance Vector Machine   
```{r KJ75aRVM}
rvmRGrid <- expand.grid(sigma = seq(6e-6, 6.5e-6, 5e-8))
set.seed(12)
rvmFit <- train(x = trainCMPX, y = trainCMPY, method = 'rvmRadial',
                preProcess = c('zv', 'nzv', 'knnImpute'),
                trControl = fitControl, tuneGrid = rvmRGrid)
modelC[5, ] <- data.frame(model = "Relevance Vector Machine",
                          TrnErr = RMSE(predict(rvmFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(rvmFit, testCMPX), testCMPY))
```

#### K-Nearest Neighbors
```{r KJ75aKNN}
knnGrid <- expand.grid(k = 5:10)
set.seed(12)
knnFit <- train(x = trainCMPX, y = trainCMPY, method = 'knn',
                preProcess = c('zv', 'nzv', 'knnImpute'),
                trControl = fitControl, tuneGrid = knnGrid)
modelC[6, ] <- data.frame(model = "KNN Regression",
                          TrnErr = RMSE(predict(knnFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(knnFit, testCMPX), testCMPY))
modelC[7, ] <- data.frame(model = "Elastic Net",
                          TrnErr = RMSE(predict(elastiFit, trainCMPX), trainCMPY),
                          TstErr = RMSE(predict(elastiFit, testCMPX), testCMPY))
```

#### Model Comparison
Below is a table of all the selected models, including the elastic net from
question 6.3.

```{r KJ75aComp}
knitr::kable(modelC[order(modelC$TstErr), ], format = 'pandoc', digits = 3L)
```

Not unexpectedly, there are models which did significantly better on the
training set than on the testing set. This exemplifies the concept of
overfitting, which can occur even with the robust levels of repeated
cross-validation performed. Interestingly, the elastic net model performed
very well, comparatively. The simple neural network failed utterly.

### Part (b)
```{r KJ75b1}
rvmImp <- varImp(rvmFit)
t10NL <- data.frame(nI, rvmImp$importance)[
  order(rvmImp$importance, decreasing = TRUE)[1:10], ]
t10NLN <- t10NL$nI
knitr::kable(t10NL, row.names = FALSE, format = 'pandoc', digits = 2)
```

The variable importance metric is exactly the same. Looking in to the code,
neither `rvm` nor `elasticnet` have special methods for calculating variable
importance, so they both rely on the default method which uses a loess smoother
fit between the outcome and the predictor. The importance is a scaled value of
the \(R^2\) for the loess model with the parameter versus the intercept-only
null model. Note this different from the best-fitting MARS model which has a
mars-specific method for calculating variable importance.

```{r KJ75b2}
head(varImp(marsFit)$importance, 10)
```

# KJ Question 8.1

# KJ Question 8.2

# KJ Question 8.3

# KJ Question 8.7

```{r stopCluster, include=FALSE}
parallel::stopCluster(cl)
```
# References
* Hastie, T.; Tibshirani, R. & Friedman, J. (2001),
*The Elements of Statistical Learning*, Springer New York Inc.,
New York, NY, USA.