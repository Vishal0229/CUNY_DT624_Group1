---
title: "CUNY DT 624"
author: 'Group 1: Avraham Adler, Vishal Arora, Samuel Bellows, Austin Chan'
date: "Summer 2020"
output:
  word_document:
    toc: yes
    toc_depth: 4
subtitle: Project 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE)
library(openxlsx)
library(caret)
library(data.table)
ncl <- parallel::detectCores() * 0.75
```

# Agenda
This report will discuss our team's building a model to predict the PH of a
formulation given the presence and ratio of key ingredients. We understand that
there will be a varied audience for this report. Therefore, the first section of
the report will contain a high-level overview of the modeling process and
decisions resulting in the findings. There will be a more technical appendix
with a deeper dive into the process. Computer code will be made available upon
request.

```{r dataIntake, include=FALSE}
trainData <- read.xlsx('./project2data/StudentData - TO MODEL.xlsx',
                       colNames = TRUE, sep.names = ' ')
predictData <- read.xlsx('./project2data/StudentEvaluation- TO PREDICT.xlsx',
                         colNames = TRUE, sep.names = ' ')
setDT(trainData)
setDT(predictData)
trainY <- trainData$PH
trainDummy <- dummyVars(PH ~ ., data = trainData, fullRank = TRUE)
trainX <- as.data.table(predict(trainDummy, newdata = trainData))
predX <- as.data.table(predict(trainDummy, newdata = predictData))
numObs <- length(trainY)
numVar <- dim(trainX)[[2]]
```

# Research Problem
The company has provided data on `r numObs` formulations of `r numVar`
components and their resulting PH values. From this data, the research team was
asked to build a model allowing PH prediction based on the formulation *prior*
to actually implementing the recipe.

## Vocabulary
In the ensuing report, we will refer to the formulations *with* their applicable
PH values as the **training** set, for it is on this data we intend to *train*
the model.

The term **features** is used to describe the elements of the formulation which
will be used in prediction. In statistical terms these are also known as the
*independent variables*.

The term **target** is used to describe the element for which the model is being
built---in this case, the `PH`. In statistical terms this is also known as the
*dependent variables*.

All the features but one are numerical. `Brand Code`, however, can take one of
four character values when it is not missing. As it is somewhat hard to do math
on the alphabet, the standard way of dealing with categories is to use
**dummy variables** or *one-hot encoding*. The unique instances of the feature
are replaced by yes/no questions. In this case, the data may be `A`, `B`, `C`,
or `D`. The default is assumed to be `A`, and the `Brand Code` field is replaced
by three new variables which are basically `IsB`, `IsC`, and `IsD`. If the
original value was `A`, all three are 0. Otherwise, the column representing the
value gets a 1.

## Model Families
There are various families of models which can be built. While it is out of
scope of this report to give a detailed analysis of each, following will be a
brief synopsis for the benefit of those unfamiliar.

 * **Regression**
   * These are models which rely on some linear relationship between the target
   and either the features themselves or some transform of them. They also may
   include some form of *regularization*, which means a penalty to reflect that
   adding more features adds more **parameter risk** at the expense of
   **process risk**.
 * **Neural Networks**
   * These are mathematical models which allow for a "mixing" of different
   linear functions to arrive at a non-linear relationship. The are often
   considered the epitome of the "black-box" model.
 * **Support Vector Machines (SVM)**
   * Although it may appear that the relationship between a target and its
   features are non-linear in the two or three dimensions that we can easily
   envision, it may be that in high-enough dimensions there is a "separating
   plane" which can successfully partition the data into distinct groups. The
   mathematics behind SVMs searches for those high-dimensional "fences" and uses
   those to build a model.
 * **Regression Trees**
   * Instead of a single mathematical formula, tree-based models ask a yes-no
   question at various levels, and partition the data points based on those
   yes-no questions. The number of levels of questions and the number of points
   allowed in each final bucket, affect the complexity of the tree. There are
   also methods which create many trees and use those results to arrive at a
   final answer.

# Data Preparation
## Missing Data
### Missing Targets
```{r missingD, include=FALSE}
missDat <- trainData[, lapply(.SD, function(x) sum(is.na(x)))]
missTgt <- missDat$PH
missTgtLoc <- which(is.na(trainY))
trainX <- trainX[-missTgtLoc, ]
trainY <- trainY[-missTgtLoc]
nZV <- nzv(trainX)
```
Unfortunately, there were some issues with measurements. The most important of
these are the `r missTgt` missing target variables. Since for those entries
there is nothing to which to calibrate, those entries will be removed from the
training data.

### Missing Features
There remains the issue of the many missing features, a table of which can be
found in the technical appendix. Generally, so long as the proportion of the
missing data is not a sizable percentage of the training set, one *imputes* the
value of the missing data. This means finding a substitute for the missing value.
For numeric features, the simplest imputation substitutes the missing values
with their mean or median. On the one hand, this engenders the least distortion
due to the missing data. However, this comes at the cost of the least predictive
value as well.

A more sophisticated technique would be to impute the missing value based on
similar observations. One method, called *k-nearest-neighbors* (knn) does so by
finding those similar observations, taking the mean value of that feature, and
using that as the imputation. That has the benefit of the imputed value being
more predictive than the global mean or median. There are other even more
sophisticated imputation methods, but knn should work here.

Another benefit of using knn imputation is that it automatically applies
*centering* and *scaling* which gives every variable a mean of 0 and a standard
deviation of 1. This allows for modeling to be performed on a data set which is
on a consistent unit basis. Of course, for predictions, the data will be
returned to its native scale.

## Near-Zero Variance
Sometimes a predictor is constant, are almost constant. In these cases, it
doesn't have much predictive power as it is the same or almost the same for
almost all cases. It is prudent to not even include those in the modeling
process as they will tend to consume resources for little gain. For the
beverage data, there was only `r length(nZV)` such variable:
`r names(trainX)[nZV]`.

# Analysis
Now that the data has been prepared with dummy variables, knn imputation, and
near-zero variance removals, the next step was to fit various models. 

## Model Selection Criteria
In order to select the best model, a measure of its accuracy is needed. There
are two main methods (Hastie et al 2001, ch. 7). The first is to manually split
the training data into a "training" and "validation" set, fit the model to the
"training" subset, and compare the results of different models' scores on the
"validation" subset.

The second is to only hold out a small subset of the training set, use that to
test, but do it multiple times over a number of holdouts. These holdouts are
called **folds** and this process is called **cross-validation** (cv). So a
five-fold cv would train on 80\% of the data and test against the remaining 20\%
five different times on five disjoint holdouts. 

For the purposes of this report, 25\% of the data will be randomly extracted to
act as the final test set, and three-times repeated five-fold cross validation
will be used to select the best parameters for a model on the remaining 75\%.
This means three separate runs of five-fold cv, each time with different 20\%
partitions.

The metric used for comparison will be **RMSE** which is the square root of the
average of the squared residuals. The lower the value on the test set the
better.

## Modeling
```{r modelSetup}
set.seed(1864)
tSplits <- createDataPartition(trainY, p = 0.75)
trnX <- trainX[tSplits$Resample1, ]
tstX <- trainX[-tSplits$Resample1, ]
trnY <- trainY[tSplits$Resample1]
tstY <- trainY[-tSplits$Resample1]
mdCmp <- data.frame(Model = character(0), TrnErr = double(0),
                    TstErr = double(0))
fitControl <- trainControl(method = "repeatedcv", number = 5L, repeats = 3L,
                           allowParallel = TRUE)
```
```{r elastiNet, cache=2L}
cl <- parallel::makePSOCKcluster(ncl)
doParallel::registerDoParallel(cl)
set.seed(9614)
elFit <- train(x = trnX, y = trnY, method = 'enet',
               preProcess = c('nzv', 'knnImpute'),
               trControl = fitControl, tuneLength = 10L)
mdCmp <- rbind(mdCmp, data.frame(Model = "ElasticNet",
                                 TrnErr = getTrainPerf(elFit)$TrainRMSE,
                                 TstErr = RMSE(predict(elFit, tstX), tstY)))
parallel::stopCluster(cl)
```
```{r randomF, cache=2L}
cl <- parallel::makePSOCKcluster(ncl)
doParallel::registerDoParallel(cl)
rfGrid <- expand.grid(mtry = 31:34,
                      splitrule = 'extratrees',
                      min.node.size = 5L)
set.seed(9614)
rfFit <- train(x = trnX, y = trnY, method = 'ranger',
               preProcess = c('nzv', 'knnImpute'), trControl = fitControl,
               tuneGrid = rfGrid, importance = 'impurity')
mdCmp <- rbind(mdCmp, data.frame(Model = "Random Forest",
                                 TrnErr = getTrainPerf(rfFit)$TrainRMSE,
                                 TstErr = RMSE(predict(rfFit, tstX), tstY)))
parallel::stopCluster(cl)
```
```{r svmF, cache=2L}
cl <- parallel::makePSOCKcluster(ncl)
doParallel::registerDoParallel(cl)
svmGrid <- expand.grid(C = seq(2.78, 2.82, 0.01),
                       sigma = seq(0.049, 0.051, 0.001))
set.seed(9614)
svmFit <- train(x = trnX, y = trnY, method = 'svmRadial',
               preProcess = c('nzv', 'knnImpute'), trControl = fitControl,
               tuneGrid = svmGrid)
mdCmp <- rbind(mdCmp, data.frame(Model = "Support Vector Machine",
                                 TrnErr = getTrainPerf(svmFit)$TrainRMSE,
                                 TstErr = RMSE(predict(svmFit, tstX), tstY)))
parallel::stopCluster(cl)
```
```{r rvmF, cache=2L}
cl <- parallel::makePSOCKcluster(ncl)
doParallel::registerDoParallel(cl)
rvmGrid <- expand.grid(sigma = seq(1.7e-4, 1.9e-4, 1e-5))
set.seed(9614)
rvmFit <- train(x = trnX, y = trnY, method = 'rvmRadial',
               preProcess = c('nzv', 'knnImpute'), trControl = fitControl,
               tuneGrid = rvmGrid)
mdCmp <- rbind(mdCmp, data.frame(Model = "Relevance Vector Machine",
                                 TrnErr = getTrainPerf(rvmFit)$TrainRMSE,
                                 TstErr = RMSE(predict(rvmFit, tstX), tstY)))
parallel::stopCluster(cl)
```
```{r knnF, cache=2L}
cl <- parallel::makePSOCKcluster(ncl)
doParallel::registerDoParallel(cl)
knnGrid <- expand.grid(k = 7:9)
set.seed(9614)
knnFit <- train(x = trnX, y = trnY, method = 'knn',
               preProcess = c('nzv', 'knnImpute'), trControl = fitControl,
               tuneGrid = knnGrid)
mdCmp <- rbind(mdCmp, data.frame(Model = "K-Nearest Neighbors",
                                 TrnErr = getTrainPerf(knnFit)$TrainRMSE,
                                 TstErr = RMSE(predict(knnFit, tstX), tstY)))
parallel::stopCluster(cl)
```
```{r xgbTF, cache=2L}
cl <- parallel::makePSOCKcluster(ncl)
doParallel::registerDoParallel(cl)
xgbTGrid <- expand.grid(max_depth = 8:9, nrounds = 430,
                        eta = 0.082,
                        colsample_bytree = 0.826,
                        min_child_weight = 1,
                        subsample = 1,
                        gamma = 0)
set.seed(9614)
xgbTFit <- train(x = trnX, y = trnY, method = 'xgbTree',
               preProcess = c('nzv', 'knnImpute'), trControl = fitControl,
               tuneGrid = xgbTGrid)
mdCmp <- rbind(mdCmp, data.frame(Model = "XGBoost: Tree",
                                 TrnErr = getTrainPerf(xgbTFit)$TrainRMSE,
                                 TstErr = RMSE(predict(xgbTFit, tstX), tstY)))
parallel::stopCluster(cl)
```
## Results
```{r mResults}
knitr::kable(mdCmp[order(mdCmp$TstErr), ], format = 'pandoc', digits = 4L)
```

# Technical Appendix
## Table of Missing Data
The magnitude of the missing features and target are shown in the table below:
```{r missTable}
knitr::kable(t(missDat), format = 'pandoc')
```

## Near-Zero Variance
For this analysis, a variable is considered to have near-zero variance if the
ratio between the most common and second most common instance is 19:1 or greater
**and** the number of unique values is no more than 10\% of the total dataset.
```{r echo=FALSE}
knitr::kable(nzv(trainX, saveMetrics = TRUE), format = 'pandoc', digits = 3L)
```

# References
  * Hastie, T.; Tibshirani, R. & Friedman, J. (2001),
  *The Elements of Statistical Learning*, Springer New York Inc.,
  New York, NY, USA.
