---
title: "CUNY DT 624---Summer 2020"
subtitle: "Homework Set 1"
author: "Group 1: Avraham Adler, Vishal Arora, Gabrielle Bartomeo, Samuel Bellows, Austin Chan"
date: "Summer 2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question HA 2.1
## Questions
Use the help function to explore what the series gold, woolyrnq and gas
represent.
 a. Use `autoplot()` to plot each of these in separate plots.
 b. What is the frequency of each series? Hint: apply the `frequency()` function.
 c. Use `which.max()` to spot the outlier in the gold series. Which observation
 was it?
 
## Answers
### Help Functions
```{r Q1setup, echo=TRUE, warning=FALSE, out.width="100%", message=FALSE}
library(fpp2)
library(httr)
```

Using the help function to know more about the various datasets like a short
description about the dataset, the format of the dataset---which in this case is
Time Series data for all three, what is the source of the data, and an example
of how to display the data to learn about it using the `tsdisplay` function.           

```{r Q1help, eval=FALSE}
help(gold)
help("woolyrnq")
help("gas")
```

According to the help function, the datasets are described as so:

gold - This dataset represents the daily morning gold prices in US dollars from
January 1st, 1985 to March 31st, 1989.

woolyrnq - This dataset represents the quarterly production of woollen yarn in
Australia in tons from March 1965 to September 1994.

gas - This dataset represents the Australian monthly gas prodcution from 1956 to
1995.

### Part a
Using the `autoplot` function can give us a graphic disply of `ts` datasets.
Looking at the graphs we can see if there is any trend, seasonality, or other
cyclical patterns in the datasets which can help us in selecting our model for
prediction.                              

```{r Q1autoplot}
autoplot(gold)
autoplot(woolyrnq)
autoplot(gas)
```

### Part b
Using the `frequency` function, we find that Gold has annual frequency, Woolrnq
has quarterly frequency, and Gas has monthly frequency.
```{r Q1freuency}
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```

### Part c

Using the `which.max()` function gives us the index of the outlier with maximum
value. It is important to note that the `which.max()` function does not return
the actual max value, but the index in the vector where the max value is
located. After plugging in the `which.max()` index value into the time series
vector, we get the true maximum value, which is 593.7.

```{r Q1whichmax}
which.max(gold)
gold[which.max(gold)]
```

# Question HA 2.3
## Questions
Download some monthly Australian retail data from the
[book website](http://otexts.com/fpp2/extrafiles/tute1.csv). These represent
retail sales in various categories for different Australian states, and are
stored in a MS-Excel file.
 
 a. You can read the data into R with the following script:
 ```
 retaildata <- readxl::read_excel("retail.xlsx", skip=1)
 ```
 The second argument (skip=1) is required because the Excel sheet has two header
 rows.

 b. Select one of the time series as follows (but replace the column name with
 your own chosen column):
 ```
 myts <- ts(retaildata[,"A3349873A"], frequency=12, start=c(1982,4))
 ```
 
 c. Explore your chosen retail time series using the following functions:
     ```autoplot(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf()```
    Can you spot any seasonality, cyclicity and trend? What do you learn about
    the series?
    
## Answers
### Part a
Using the `httr` package's `GET` function to download the retail.xlsx file to
a local directory and then using the `read_xlsx` function to read the xlsx
skipping the first row into a R Dataframe object.
```{r Q2a}
url <- "https://otexts.com/fpp2/extrafiles/retail.xlsx"
GET(url, write_disk("retail.xlsx", overwrite=TRUE))
retaildata <- readxl::read_xlsx("retail.xlsx", skip=1)
```

### Part b
```{r Q2b}
# choosing column 'A3349721R'
myts <- ts(retaildata[, "A3349721R"], frequency = 12, start = c(1982, 1),
           end = c(2005, 6))
head(myts)
tail(myts, 6)
```

### Part c
Using the `autoplot` function on a `ts` dataset, we can see that there is indeed
a trend and seasonality in the graph.                                            

```{r Q2c1}
autoplot(myts)
```

Using the `ggseasonplot` function, one can confirm what we deduced from the
above plot that there is slight seasonlity pattern also in the data. Thus
September is the peak and March is the trough season.
```{r Q2c2}
ggseasonplot((myts))
```

Using `subseriesplot` we can see the seasonality for each month for the time
span of time series. We can clearly see that September is the peak and March and
November are the troughs.           
```{r Q2c3}
ggsubseriesplot(myts)
```

A lag plot shows the scatter plots for each month in which all the graphs show
linearity. This suggests that a strong autocorrelations exits.                                 
```{r Q2c4}
gglagplot(myts)
```

Using the `ggACF` function to plot ACF graphs with differrent lag periods shows
that there exists strong autocorrelations. We can also see that with the
increase in lag period the graphs show decreasing positive values. With
increasing lag periods in the second graph we can also see a slight seasonal
pattern.                         

```{r Q2c5}
ggAcf(myts, lag = 12)

ggAcf(myts, lag = 30)

```

### Summary
As we can clearly see that out data in second questions has a positive trend
with a slight seasonality and with no cyclic patterns.
This can be corroborated by plotting using the decompose function and then
autoplotting.              

```{r Q2Summary}
decmyts <-decompose(myts)
autoplot(decmyts, type="multiplicative")
```

# Question HA 6.2
## Question
The `plastics` data set consists of the monthly sales (in thousands) of product
A for a plastics manufacturer for five years.

 a. Plot the time series of sales of product A. Can you identify seasonal
 fluctuations and/or a trend-cycle?
 b. Use a classical multiplicative decomposition to calculate the trend-cycle
 and seasonal indices.
 c. Do the results support the graphical interpretation from part a?
 d. Compute and plot the seasonally adjusted data.
 e. Change one observation to be an outlier (e.g., add 500 to one observation),
 and recompute the seasonally adjusted data. What is the effect of the outlier?
 f. Does it make any difference if the outlier is near the end rather than in
 the middle of the time series?
 
## Answers
### Part a
```{r Q62a}
autoplot(plastics)
```

The graphs shows clear seasonality. The winter months have low values, which
climb in the spring and summer, and then fall during fall. There is also a
long-term increasing secular trend in the sales figures. There is too little
data to determine if this increasing trend is part of a larger cycle. The peak
in the last year (the years are denoted 1--5 in the dataset) is relatively
lower than prior peak increases have been, which may indicate the turning of
a cycle. However, one to two more years of data would be necessary to make a
specific determination.

### Part b
This data has monthly frequency, so \(m = 12\) and \(\hat{T}_n\) should be
calculated as a 2x12-period moving average.
```{r 62b_1}
Tn <- ma(plastics, order = 12, centre = TRUE)
```

Classical multiplicative decomposition detrends by dividing out the MA component.
```{r 62b_2}
detrendPlastics <- plastics / Tn
```

Classical decomposition adjusts for seasonality by calculating \(\hat{S}_n\),
the average value per cycle---here monthly---and then subtracting it for
additive, or dividing it out for multiplicative, from the detrended series.
```{r 62b_3}
# Preallocate your data structures when you can: RInferno
Sn <- double(12)
# This can be vectorized using data.table or dplyr, but is short enough that
# the simple for loop is easiest to understand and implement.
for (i in seq_len(12)) {
    Sn[i] <- mean(detrendPlastics[cycle(detrendPlastics) == i], na.rm = TRUE)
}
```

The raw values need to be adjusted so that their sum equals \(m\), which is 12
in this case.
``` {r 62b_4}
Sn <- 12 * Sn / sum(Sn)
```

The random component of the times series, \(\hat{R}_n\) is calculated in 
classical multiplicative decomposition as the quotient of the raw values with
the product of the trend-cycle and seasonal components.
```{r 62b_5}
Rn <- plastics / (Tn * Sn)
```

We can check these results by comparing them to the output of the `decompose`
function from the `stats` package, which does all of this automatically.
```{r 62b_6}
DecomPlastics <- decompose(plastics, type = 'multiplicative')
all.equal(DecomPlastics$trend, Tn)
all.equal(DecomPlastics$figure, Sn)
all.equal(DecomPlastics$random, Rn)
```

While graphical output will wait until part d, below are \(\hat{S}_n,
\hat{T}_n\), and \(\hat{R}_n\).
```{r 62b_7}
Sn
Tn
```

### Part c.
As surmised above, there is unmistakeably clear seasonality shown in
\(\hat{S}_n\). Moreover, there is also a clear increasing trend shown in
\(\hat{T}_n\). Lastly, the trend element shows a peak around February \& March
of year 5. This implies that instead of a pure upwards trend there may be a
longer-term cycle. However, whether that is noise or signal requires more data.
Regardless, the multiplicative decomposition in part b clearly **supports** the
graphical interpretation of part a.

### Part d.
The results will be graphically displayed using `autoplot` from `ggplot2`.
```{r Q62d_1}
autoplot(DecomPlastics)
```

The seasonally-adjusted data is \(\hat{T}_n\hat{R}_n\) which is plotted below.
```{r Q62d_2}
plot(Tn * Rn, ylab = "Seasonally-Adjusted")
```

### Part e.
```{r Q62e_1}
plastics2 <- plastics
plastics2[[26]] <- plastics2[[26]] + 500
Tn2 <- ma(plastics2, order = 12, centre = TRUE)
detrendPlastics2 <- plastics2 / Tn2
Sn2 <- double(12)
for (i in seq_len(12)) {
    Sn2[i] <- mean(detrendPlastics2[cycle(detrendPlastics2) == i], na.rm = TRUE)
}
Sn2 <- 12 * Sn2 / sum(Sn2)
plot(Sn, type = 'l')
lines(Sn2, col = 'blue')
autoplot(decompose(plastics2, type = 'multiplicative'))
```

As can be seen from the plots above, the adding of an outlier to February in the
middle of the time series gives a "bump" to that period's entry in the
seasonality index and has an effect on the moving average so long as it is
within the window. However, the overall shape and scale of the decompositions
are very similar, indicating some level of robustness to a one-time outlier

### Part f.
```{r Q62f_1}
plastics3 <- plastics
plastics3[[2]] <- plastics2[[2]] + 500
Tn3 <- ma(plastics3, order = 12, centre = TRUE)
detrendPlastics3 <- plastics3 / Tn3
Sn3 <- double(12)
for (i in seq_len(12)) {
    Sn3[i] <- mean(detrendPlastics3[cycle(detrendPlastics3) == i], na.rm = TRUE)
}
Sn3 <- 12 * Sn3 / sum(Sn3)
plot(Sn, type = 'l')
lines(Sn2, col = 'blue')
lines(Sn3, col = 'purple')
autoplot(decompose(plastics3, type = 'multiplicative'))
```

Here, the outlier was added to the first February. In this case it has almost
no effect at all, as that is outside the acceptable window for a 2x12 moving
average. In this case, interestingly, one of the known shortcomings of
classical decomposition---its inability to process the ends of the time
series---here becomes valuable!

# Question KJ 3.1
## Question
The UC Irvine Machine Learning Repository[^a] contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.
The data can be accessed via:
```{r QKJ31}
library(mlbench)
data(Glass)
str(Glass)
```

 (a) Using visualizations, explore the predictor variables to understand their
 distributions as well as the relationships between predictors.
 (b) Do there appear to be any outliers in the data? Are any predictors skewed?
 (c) Are there any relevant transformations of one or more predictors that might
 improve the classification model?
 
[^a]: http://archive.ics.uci.edu/ml/index.html
 
## Answer
The answer will combine parts (a), (b), and (c) into one narrative as it
progresses.

### Variable Empirical Distributions
The very first step is to understand the data. The call to `str` in the question
is very valuable. It tells us that we can naïvely expect that each of the values
comes from a continuous distribution, since they are coded as floats/doubles.
The exception is the `Type` variable which is a factor which takes 6 discrete
values. If we wanted to use this as a predictor in a regression we would have to
consider what we called **dummy variables** back in twentieth-century linear
programming, and what the cool kids of today call **one-hot encoding**.

The first visualization step is usually to plot the empirical distribution of
each of the variables. Below are histograms and kernel density plots for each of
the 10 variables. To make life a bit simpler, the factor `Type` will be
converted to the eponynous integer. Also, since there will be a few calls to a
set of scatterplots, a helper function `histbox` will be defined.
```{r Q31Hist, fig.width = 9L, fig.height = 10L}
Glass$Type <- as.integer(levels(Glass$Type))[Glass$Type]
histbox <- function(df, box, useLast = TRUE) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]] # Thanks for nothing, scale!!
    if (!useLast) ndf <- ndf[-length(ndf)]
    for (i in seq_along(ndf)) {
        data <- unlist(df[, i])
        hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
             xlab = ndf[i], freq = FALSE)
        lines(density(data, kernel = "ep"), col = 'blue')
    }
    par(mfrow = c(1, 1))
}
histbox(Glass, c(4, 3))
```

The plots show that some of the variable such as Potassium (**K**), Barium
(**Ba**), and Iron (**Fe**) have classic very right-tailed skew distributions.
Others, such as Magnesium (**Mg**), Calcium (**Ca**), and the refractive index
(**RI**) have slightly right-tailed distributions with noticeable observations
below the mode. Still others, such as Silicon (**Si**), Sodium (**Na**), and
Aluminum (**Al**) are borderline symmetrical. Lastly, Magnesium (**Mg**) doesn't
fit into any category. Rather it has peaks at the ends and dips in the
middle. It almost looks like some kind of beta distribution would be appropriate.
**Type** doesn't have an immediately recognizable shape either, but as it is a
categorical variable, that is less concerning. We can check for skewness using
the sample skew parameter.[^Sk]

[^Sk]: There are multiple definitions of the empirical skew function. Joanes &
Gill (1997) define three of them, referred to as \(\mathbf{g_1}\),
\(\mathbf{G_1}\), and \(\mathbf{b_1}\). The `skewness` function in the `e1071`
package allows for choosing which type should be used. Here, Kuhn & Johnson use
a formula which is none of those three, but is actually
\(\frac{\mathbf{b_1}\cdot n}{n-1}\).

    Joanes, D. N. and Gill, C. A. (1998) Comparing Measures of Sample Skewness
    and Kurtosis. *Journal of the Royal Statistical Society.*
    *Series D (The Statistician)* **47**(1), 183--189.
    https://www.jstor.org/stable/2988433
    
```{r Q31Skew}
KJskew <- function(x) {
    sum((x - mean(x, na.rm = TRUE)) ^ 3, na.rm = TRUE) /
        (sd(x, na.rm = TRUE) ^ 3 * (length(x) - 1))
}
apply(Glass, 2L, KJskew)
```

As seen in the histograms, `Ca`, `K`, and `Ba` show the heaviest right skew.

### Transforms
#### Scaling and Centering
The next step, with continuous data at least, is to look at the data after
transformations. The simplest is the Z-scaling of the data, which subtracts the
mean and divides by the standard deviation of each variable. This sets all means
to 0 and puts the variability on the same scale. This which will obviously not
be applied to the `Type` variable.
```{r Q31Scale, fig.width = 9L, fig.height = 8L}
histbox(scale(Glass), box = c(3, 3), useLast = FALSE)
apply(scale(Glass), 2L, KJskew)
```

Unfortuately, this did not help matters much and the skew not at all, which
stands to reason, as centering and scaling do not materially address skew.

#### Logarithmic Transformation
A transformation often used to address skew in particular is to log the data.
Again, this is inappropriate for the `Type` variable.
```{r Q31Log, fig.width = 9L, fig.height = 8L}
histbox(log(Glass), box = c(3, 3), useLast = FALSE)
apply(log(Glass), 2L, KJskew)
```

Now, some of the distributions begin to look more symmetrical, which should put
us in mind of the *lognormal* distribution. Magnesium, however, becomes
heavily **left**-tailed. This phenomenon is often seen in *gamma* distributed
variables and is very clear in *beta* distributed random variables where both
shape parameters are near 0.5. This fits very nicely with our observation above.

Note that many of the variables had observations of 0, which returns `-Inf` when
logged. This is the reasons for the `Nan`s in the skew calculations. This also
indicates that a log-transform may not be the best for those variables.

#### Other Transformations
Other transformations include power transformations and the Box-Cox family of
transforms with parameter \(\lambda\):
\[
x^* =
\begin{cases}
\frac{x^\lambda - 1}{\lambda}\qquad &\textrm{if }\lambda \neq 0\\
\log(x) &\textrm{if }\lambda = 0
\end{cases}
\]

This transformation encompasses many power transformations depending on the
value of \(\lambda\). An example of the transform with \(\lambda = 0.4\) is
shown below.
```{r Q31bxcx, fig.width = 9L, fig.height = 8L}
bxcx <- function(x, l) {
    if (l == 0) {
        log(x)
    } else {
        (x ^ l - 1) / l
    }
}
histbox(bxcx(Glass, 0.4), box = c(3, 3), useLast = FALSE)
apply(bxcx(Glass, 0.4), 2L, KJskew)
```

Here, the Box-Cox transform did next to nothing for the skew of `RI` and `Mg`,
but the skew of `K` and `Al` are greatly reduced. The effect on the other
variables lies somewhere in between. There are packages in `R`, such as the
`car` package, which will use maximum likelihood to fit \(\lambda_i\) for each
variable \(i\) in the dataset.

### Variable Correlations
To visually inspect correlation, the simplest approach is to create scatterplots
of each variable against each other.
```{r Q31Scat, fig.width = 9L, fig.height = 8L}
plot(Glass, pch = ".")
```

With 10 variables there are 45 possible pairs, most of which don't show
overwhelming correlations. However, there are some visually apparent
relationships. For example,  `Refractive Index` seems to be highly positively
correlated with `Ca` and somewhat negatively correlated with `Si`.

### Outliers
The scatterplot doesn't show strong evidence of outliers. However, boxplots or
violin plots are often useful in identifying outliers.

```{r Q31bxplt, fig.width = 9L, fig.height = 8L}
boxplot(Glass, pch = 16)
```

There are clearly a few observations for each variable which exceed 1.5 times
the IQR, the majority of which are in Calcium.

The book discusses the spatial sign transform, which is probably not appropriate
for this data, but it can be tried for fun.
```{r Q31aSpS, fig.width = 9L, fig.height = 8L}
SpatSignT <- function(df) {
  dfS <- as.data.frame(scale(df))
  return(dfS / sqrt(rowSums(dfS ^ 2)))
}
```

For this example, the focus will be on the relationship between `Ca` and `Ba`
which shows two "clusters" of outliers.

```{r Q31SpSEx, fig.width = 4L, fig.height = 4.5}
BaCa <- data.frame(Ba = Glass$Ba, Ca = Glass$Ca)
outliers1 <- which(BaCa$Ca > 8 & BaCa$Ba < 2 & BaCa$Ba > 0)
outliers2 <- c(which(BaCa$Ca > 12 & BaCa$Ba > 2.5),
              which(BaCa$Ca < 8 & BaCa$Ba > 1))
plot(BaCa[-c(outliers1, outliers1), ], xlim = c(0, 3.5), ylim = c(5, 17))
points(BaCa[outliers1, ], col = 'blue', pch = 16)
points(BaCa[outliers2, ], col = 'red', pch = 16)
BaCaSS <- SpatSignT(BaCa)
plot(BaCaSS[-c(outliers1, outliers1), ], xlim = c(-1, 1), y = c(-1, 1))
points(BaCaSS[outliers1, ], col = 'blue', pch = 16)
points(BaCaSS[outliers2, ], col = 'red', pch = 16)
```

The spatial sign transformation does indicate that the clusters in normal space
do tend to inhabit different areas of the unit hypersphere. How that can be
leveraged for analysis is somewhat more complicated.

### Summary
This writeup has investigated the `Glass` data from the `mlbench` package. The
native variables visually exhibit a range of distributions from the highly
skewed to the nearly symmetrical, with the standard complement of outliers. A
sequence of transformations were applied to address some of these issues and
the results visualized as well. Further areas for investigation would include
dimensional reduction techniques, especiallt PCA, which should help to reduce
the feature set from 10 to something more manageable which still captures the
predominance of the variability in the data.

# Question KJ 3.2
## Question
The soybean data can also be found at the UC Irvine Machine Learning Repository.
Data were collected to predict disease in 683 soybeans. The 35 predictors are
mostly categorical and include information on the environmental conditions
(e.g., temperature, precipitation) and plant conditions (e.g., left spots,
mold growth). The outcome labels consist of 19 distinct classes. The data can be
loaded via:
```{r QKJ32q}
library(mlbench)
data(Soybean)
## See ?Soybean for details
```

  (a) Investigate the frequency distributions for the categorical predictors.
  Are any of the distributions degenerate in the ways discussed earlier in this
  chapter?
  (b) Roughly 18\% of the data are missing. Are there particular predictors that
  are more likely to be missing? Is the pattern of missing data related to the
  classes?
  (c) Develop a strategy for handling missing data, either by eliminating
  predictors or imputation.

## Answer
### Part (a)
```{r QKJ32barbox}
barbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]] # Thanks for nothing, scale!!
    for (i in seq_along(ndf)) {
        data <- unlist(df[, i])
        plot(data, main = paste("PseudoHistogram of", ndf[i]), xlab = ndf[i])
    }
    par(mfrow = c(1, 1))
}
```

The frequency of the categorical is shown below, both visually and tabularly:
```{r QKJ32a1, fig.width = 9L, fig.height = 10L}
for (i in seq_len(3)) {
  barbox(Soybean[, seq_len(12) * i], c(4, 3))
}
apply(Soybean, 2, function(x) length(unique(x)))
```

From the above, we see that there is no mathematically degenerate distribution,
in that every catgeory has at least one instance of more than one variable type.
Furthermore, the number of unique values being small in respect of the number of
samples is not a significant issue, as most of these are dummy/indicator
variables for the categories. The key test will be the ratio of most frequent to
the second most frequent observation in each category.
```{r QKJ32a2}
nS <- names(Soybean)
soyRatios <- data.frame(Variable = character(36), Ratio = double(36))
for (i in seq_len(dim(Soybean)[[2]])) {
  soyRatios[i, 1] <- nS[i]
  soyRatios[i, 2] <- max(table(Soybean[, i])) /
    max(table(Soybean[, i])[-which.max(table(Soybean[, i]))])
}
soyRatios[order(soyRatios$Ratio, decreasing = TRUE), ]
soyRatios[which(soyRatios$Ratio > 20), ]
```

Here we see three categories which have a most-to-second most ratio of greater
than 20, the value suggested by Kuhn & Johnson. These would be categories for
consideration of removal or some other transform due to their imbalance.

### Part (b)
There clearly is a correlation between the Class and the "missingness" of data.
```{r QKJ32b1, fig.width = 9L, fig.height = 10L}
numClass <- length(levels(Soybean$Class))
nameClass <- levels(Soybean$Class)
NAList <- data.frame(Class = character(0), NACount = integer(0))
par(mfrow = c(5, 4))
for (i in seq_len(numClass)) {
  NAList[i, ] <- c(nameClass[i],
                   sum(is.na(Soybean[Soybean$Class == nameClass[i], ])))
  barplot(apply(Soybean[Soybean$Class == nameClass[i], ], 2L,
                function(x) sum(is.na(x)) / length(x)),
          xlab = nameClass[i])
}
par(mfrow = c(1, 1))
NAList
```

It's clear that classes 1, 9, 10, 14, and 16 have significant missing values
where the other 14 classes have none. As an aside, this kind of analysis would
be done more easily using a package like `data.table` where operations could be
done by group.
```{r QKJ32b2}
library(data.table)
Sb <- as.data.table(Soybean)
Sb[, .(NACount = sum(is.na(.SD))), keyby = Class]
```

Which gives us the same information as does base R but with less typing.

### Part (c)
