---
title: "CUNY DT 624"
subtitle: "Homework Set 1"
author: "Group 1: Avraham Adler, Vishal Arora, Samuel Bellows, Austin Chan"
date: "Summer 2020"
output:
  word_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question HA 2.1
## Questions
Use the help function to explore what the series gold, woolyrnq and gas
represent.
 a. Use `autoplot()` to plot each of these in separate plots.
 b. What is the frequency of each series? Hint: apply the `frequency()` function.
 c. Use `which.max()` to spot the outlier in the gold series. Which observation
 was it?
 
## Answers
### Help Functions
```{r Q1setup, echo=TRUE, warning=FALSE, out.width="100%", message=FALSE}
library(fpp2)
library(httr)
```

Using the help function to know more about the various datasets like a short
description about the dataset, the format of the dataset---which in this case is
Time Series data for all three, what is the source of the data, and an example
of how to display the data to learn about it using the `tsdisplay` function.           

```{r Q1help, eval=FALSE}
help(gold)
help("woolyrnq")
help("gas")
```

According to the help function, the datasets are described as so:

gold - This dataset represents the daily morning gold prices in US dollars from
January 1st, 1985 to March 31st, 1989.

woolyrnq - This dataset represents the quarterly production of woollen yarn in
Australia in tons from March 1965 to September 1994.

gas - This dataset represents the Australian monthly gas prodcution from 1956 to
1995.

### Part a
Using the `autoplot` function can give us a graphic disply of `ts` datasets.
Looking at the graphs we can see if there is any trend, seasonality, or other
cyclical patterns in the datasets which can help us in selecting our model for
prediction.                              

```{r Q1autoplot}
autoplot(gold)
autoplot(woolyrnq)
autoplot(gas)
```

### Part b
Using the `frequency` function, we find that Gold has annual frequency, Woolrnq
has quarterly frequency, and Gas has monthly frequency.
```{r Q1freuency}
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```

### Part c

Using the `which.max()` function gives us the index of the outlier with maximum
value. It is important to note that the `which.max()` function does not return
the actual max value, but the index in the vector where the max value is
located. After plugging in the `which.max()` index value into the time series
vector, we get the true maximum value, which is 593.7.

```{r Q1whichmax}
which.max(gold)
gold[which.max(gold)]
```

# Question HA 2.3
## Questions
Download some monthly Australian retail data from the
[book website](http://otexts.com/fpp2/extrafiles/tute1.csv). These represent
retail sales in various categories for different Australian states, and are
stored in a MS-Excel file.
 
 a. You can read the data into R with the following script:
 ```
 retaildata <- readxl::read_excel("retail.xlsx", skip=1)
 ```
 The second argument (skip=1) is required because the Excel sheet has two header
 rows.

 b. Select one of the time series as follows (but replace the column name with
 your own chosen column):
 ```
 myts <- ts(retaildata[,"A3349873A"], frequency=12, start=c(1982,4))
 ```
 
 c. Explore your chosen retail time series using the following functions:
     ```autoplot(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf()```
    Can you spot any seasonality, cyclicity and trend? What do you learn about
    the series?
    
## Answers
### Part a
Using the `httr` package's `GET` function to download the retail.xlsx file to
a local directory and then using the `read_xlsx` function to read the xlsx
skipping the first row into a R Dataframe object.
```{r Q2a}
url <- "https://otexts.com/fpp2/extrafiles/retail.xlsx"
GET(url, write_disk("retail.xlsx", overwrite=TRUE))
retaildata <- readxl::read_xlsx("retail.xlsx", skip=1)
```

### Part b
```{r Q2b}
# choosing column 'A3349721R'
myts <- ts(retaildata[, "A3349721R"], frequency = 12, start = c(1982, 1),
           end = c(2005, 6))
head(myts)
tail(myts, 6)
```

### Part c
Using the `autoplot` function on a `ts` dataset, we can see that there is indeed
a trend and seasonality in the graph.                                            

```{r Q2c1}
autoplot(myts)
```

Using the `ggseasonplot` function, one can confirm what we deduced from the
above plot that there is slight seasonlity pattern also in the data. Thus
September is the peak and March is the trough season.
```{r Q2c2}
ggseasonplot((myts))
```

Using `subseriesplot` we can see the seasonality for each month for the time
span of time series. We can clearly see that September is the peak and March and
November are the troughs.           
```{r Q2c3}
ggsubseriesplot(myts)
```

A lag plot shows the scatter plots for each month in which all the graphs show
linearity. This suggests that a strong autocorrelations exits.                                 
```{r Q2c4}
gglagplot(myts)
```

Using the `ggACF` function to plot ACF graphs with differrent lag periods shows
that there exists strong autocorrelations. We can also see that with the
increase in lag period the graphs show decreasing positive values. With
increasing lag periods in the second graph we can also see a slight seasonal
pattern.                         

```{r Q2c5}
ggAcf(myts, lag = 12)

ggAcf(myts, lag = 30)

```

### Summary
As we can clearly see that out data in second questions has a positive trend
with a slight seasonality and with no cyclic patterns.
This can be corroborated by plotting using the decompose function and then
autoplotting.              

```{r Q2Summary}
decmyts <-decompose(myts)
autoplot(decmyts, type="multiplicative")
```

# Question HA 6.2
## Question
The `plastics` data set consists of the monthly sales (in thousands) of product
A for a plastics manufacturer for five years.

 a. Plot the time series of sales of product A. Can you identify seasonal
 fluctuations and/or a trend-cycle?
 b. Use a classical multiplicative decomposition to calculate the trend-cycle
 and seasonal indices.
 c. Do the results support the graphical interpretation from part a?
 d. Compute and plot the seasonally adjusted data.
 e. Change one observation to be an outlier (e.g., add 500 to one observation),
 and recompute the seasonally adjusted data. What is the effect of the outlier?
 f. Does it make any difference if the outlier is near the end rather than in
 the middle of the time series?
 
## Answers
### Part a
```{r Q62a}
autoplot(plastics)
```

The graphs shows clear seasonality. The winter months have low values, which
climb in the spring and summer, and then fall during fall. There is also a
long-term increasing secular trend in the sales figures. There is too little
data to determine if this increasing trend is part of a larger cycle. The peak
in the last year (the years are denoted 1--5 in the dataset) is relatively
lower than prior peak increases have been, which may indicate the turning of
a cycle. However, one to two more years of data would be necessary to make a
specific determination.

### Part b
This data has monthly frequency, so \(m = 12\) and \(\hat{T}_n\) should be
calculated as a 2x12-period moving average.
```{r 62b_1}
Tn <- ma(plastics, order = 12, centre = TRUE)
```

Classical multiplicative decomposition detrends by dividing out the MA component.
```{r 62b_2}
detrendPlastics <- plastics / Tn
```

Classical decomposition adjusts for seasonality by calculating \(\hat{S}_n\),
the average value per cycle---here monthly---and then subtracting it for
additive, or dividing it out for multiplicative, from the detrended series.
```{r 62b_3}
# Preallocate your data structures when you can: RInferno
Sn <- double(12)
# This can be vectorized using data.table or dplyr, but is short enough that
# the simple for loop is easiest to understand and implement.
for (i in seq_len(12)) {
    Sn[i] <- mean(detrendPlastics[cycle(detrendPlastics) == i], na.rm = TRUE)
}
```

The raw values need to be adjusted so that their sum equals \(m\), which is 12
in this case.
``` {r 62b_4}
Sn <- 12 * Sn / sum(Sn)
```

The random component of the times series, \(\hat{R}_n\) is calculated in 
classical multiplicative decomposition as the quotient of the raw values with
the product of the trend-cycle and seasonal components.
```{r 62b_5}
Rn <- plastics / (Tn * Sn)
```

We can check these results by comparing them to the output of the `decompose`
function from the `stats` package, which does all of this automatically.
```{r 62b_6}
DecomPlastics <- decompose(plastics, type = 'multiplicative')
all.equal(DecomPlastics$trend, Tn)
all.equal(DecomPlastics$figure, Sn)
all.equal(DecomPlastics$random, Rn)
```

While graphical output will wait until part d, below are \(\hat{S}_n,
\hat{T}_n\), and \(\hat{R}_n\).
```{r 62b_7}
Sn
Tn
```

### Part c.
As surmised above, there is unmistakeably clear seasonality shown in
\(\hat{S}_n\). Moreover, there is also a clear increasing trend shown in
\(\hat{T}_n\). Lastly, the trend element shows a peak around February \& March
of year 5. This implies that instead of a pure upwards trend there may be a
longer-term cycle. However, whether that is noise or signal requires more data.
Regardless, the multiplicative decomposition in part b clearly **supports** the
graphical interpretation of part a.

### Part d.
The results will be graphically displayed using `autoplot` from `ggplot2`.
```{r Q62d_1}
autoplot(DecomPlastics)
```

The seasonally-adjusted data is \(\hat{T}_n\hat{R}_n\) which is plotted below.
```{r Q62d_2}
plot(Tn * Rn, ylab = "Seasonally-Adjusted")
```

### Part e.
```{r Q62e_1}
plastics2 <- plastics
plastics2[[26]] <- plastics2[[26]] + 500
Tn2 <- ma(plastics2, order = 12, centre = TRUE)
detrendPlastics2 <- plastics2 / Tn2
Sn2 <- double(12)
for (i in seq_len(12)) {
    Sn2[i] <- mean(detrendPlastics2[cycle(detrendPlastics2) == i], na.rm = TRUE)
}
Sn2 <- 12 * Sn2 / sum(Sn2)
plot(Sn, type = 'l')
lines(Sn2, col = 'blue')
autoplot(decompose(plastics2, type = 'multiplicative'))
```

As can be seen from the plots above, the adding of an outlier to February in the
middle of the time series gives a "bump" to that period's entry in the
seasonality index and has an effect on the moving average so long as it is
within the window. However, the overall shape and scale of the decompositions
are very similar, indicating some level of robustness to a one-time outlier

### Part f.
```{r Q62f_1}
plastics3 <- plastics
plastics3[[2]] <- plastics2[[2]] + 500
Tn3 <- ma(plastics3, order = 12, centre = TRUE)
detrendPlastics3 <- plastics3 / Tn3
Sn3 <- double(12)
for (i in seq_len(12)) {
    Sn3[i] <- mean(detrendPlastics3[cycle(detrendPlastics3) == i], na.rm = TRUE)
}
Sn3 <- 12 * Sn3 / sum(Sn3)
plot(Sn, type = 'l')
lines(Sn2, col = 'blue')
lines(Sn3, col = 'purple')
autoplot(decompose(plastics3, type = 'multiplicative'))
```

Here, the outlier was added to the first February. In this case it has almost
no effect at all, as that is outside the acceptable window for a 2x12 moving
average. In this case, interestingly, one of the known shortcomings of
classical decomposition---its inability to process the ends of the time
series---here becomes valuable!

# Question KJ 3.1
## Question
The UC Irvine Machine Learning Repository[^a] contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.
The data can be accessed via:
```{r QKJ31}
library(mlbench)
data(Glass)
str(Glass)
```

 (a) Using visualizations, explore the predictor variables to understand their
 distributions as well as the relationships between predictors.
 (b) Do there appear to be any outliers in the data? Are any predictors skewed?
 (c) Are there any relevant transformations of one or more predictors that might
 improve the classification model?
 
[^a]: http://archive.ics.uci.edu/ml/index.html
 
## Answer
The answer will combine parts (a), (b), and (c) into one narrative as it
progresses.

### Variable Empirical Distributions
The very first step is to understand the data. The call to `str` in the question
is very valuable. It tells us that we can naïvely expect that each of the values
comes from a continuous distribution, since they are coded as floats/doubles.
The exception is the `Type` variable which is a factor which takes 6 discrete
values. If we wanted to use this as a predictor in a regression we would have to
consider what we called **dummy variables** back in twentieth-century linear
programming, and what the cool kids of today call **one-hot encoding**.

The first visualization step is usually to plot the empirical distribution of
each of the variables. Below are histograms and kernel density plots for each of
the 10 variables. To make life a bit simpler, the factor `Type` will be
converted to the eponynous integer. Also, since there will be a few calls to a
set of scatterplots, a helper function `histbox` will be defined.
```{r Q31Hist, fig.width = 9L, fig.height = 10L}
Glass$Type <- as.integer(levels(Glass$Type))[Glass$Type]
histbox <- function(df, box, useLast = TRUE) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]] # Thanks for nothing, scale!!
    if (!useLast) ndf <- ndf[-length(ndf)]
    for (i in seq_along(ndf)) {
        data <- unlist(df[, i])
        hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
             xlab = ndf[i], freq = FALSE)
        lines(density(data, kernel = "ep"), col = 'blue')
    }
    par(mfrow = c(1, 1))
}
histbox(Glass, c(4, 3))
```

The plots show that some of the variable such as Potassium (**K**), Barium
(**Ba**), and Iron (**Fe**) have classic very right-tailed skew distributions.
Others, such as Magnesium (**Mg**), Calcium (**Ca**), and the refractive index
(**RI**) have slightly right-tailed distributions with noticeable observations
below the mode. Still others, such as Silicon (**Si**), Sodium (**Na**), and
Aluminum (**Al**) are borderline symmetrical. Lastly, Magnesium (**Mg**) doesn't
fit into any category. Rather it has peaks at the ends and dips in the
middle. It almost looks like some kind of beta distribution would be appropriate.
**Type** doesn't have an immediately recognizable shape either, but as it is a
categorical variable, that is less concerning. We can check for skewness using
the sample skew parameter.[^Sk]

[^Sk]: There are multiple definitions of the empirical skew function. Joanes &
Gill (1997) define three of them, referred to as \(\mathbf{g_1}\),
\(\mathbf{G_1}\), and \(\mathbf{b_1}\). The `skewness` function in the `e1071`
package allows for choosing which type should be used. Here, Kuhn & Johnson use
a formula which is none of those three, but is actually
\(\frac{\mathbf{b_1}\cdot n}{n-1}\).

    Joanes, D. N. and Gill, C. A. (1998) Comparing Measures of Sample Skewness
    and Kurtosis. *Journal of the Royal Statistical Society.*
    *Series D (The Statistician)* **47**(1), 183--189.
    https://www.jstor.org/stable/2988433
    
```{r Q31Skew}
KJskew <- function(x) {
    sum((x - mean(x, na.rm = TRUE)) ^ 3, na.rm = TRUE) /
        (sd(x, na.rm = TRUE) ^ 3 * (length(x) - 1))
}
apply(Glass, 2L, KJskew)
```

As seen in the histograms, `Ca`, `K`, and `Ba` show the heaviest right skew.

### Transforms
#### Scaling and Centering
The next step, with continuous data at least, is to look at the data after
transformations. The simplest is the Z-scaling of the data, which subtracts the
mean and divides by the standard deviation of each variable. This sets all means
to 0 and puts the variability on the same scale. This which will obviously not
be applied to the `Type` variable.
```{r Q31Scale, fig.width = 9L, fig.height = 8L}
histbox(scale(Glass), box = c(3, 3), useLast = FALSE)
apply(scale(Glass), 2L, KJskew)
```

Unfortuately, this did not help matters much and the skew not at all, which
stands to reason, as centering and scaling do not materially address skew.

#### Logarithmic Transformation
A transformation often used to address skew in particular is to log the data.
Again, this is inappropriate for the `Type` variable.
```{r Q31Log, fig.width = 9L, fig.height = 8L}
histbox(log(Glass), box = c(3, 3), useLast = FALSE)
apply(log(Glass), 2L, KJskew)
```

Now, some of the distributions begin to look more symmetrical, which should put
us in mind of the *lognormal* distribution. Magnesium, however, becomes
heavily **left**-tailed. This phenomenon is often seen in *gamma* distributed
variables and is very clear in *beta* distributed random variables where both
shape parameters are near 0.5. This fits very nicely with our observation above.

Note that many of the variables had observations of 0, which returns `-Inf` when
logged. This is the reasons for the `Nan`s in the skew calculations. This also
indicates that a log-transform may not be the best for those variables.

#### Other Transformations
Other transformations include power transformations and the Box-Cox family of
transforms with parameter \(\lambda\):
\[
x^* =
\begin{cases}
\frac{x^\lambda - 1}{\lambda}\qquad &\textrm{if }\lambda \neq 0\\
\log(x) &\textrm{if }\lambda = 0
\end{cases}
\]

This transformation encompasses many power transformations depending on the
value of \(\lambda\). An example of the transform with \(\lambda = 0.4\) is
shown below.
```{r Q31bxcx, fig.width = 9L, fig.height = 8L}
bxcx <- function(x, l) {
    if (l == 0) {
        log(x)
    } else {
        (x ^ l - 1) / l
    }
}
histbox(bxcx(Glass, 0.4), box = c(3, 3), useLast = FALSE)
apply(bxcx(Glass, 0.4), 2L, KJskew)
```

Here, the Box-Cox transform did next to nothing for the skew of `RI` and `Mg`,
but the skew of `K` and `Al` are greatly reduced. The effect on the other
variables lies somewhere in between. There are packages in `R`, such as the
`car` package, which will use maximum likelihood to fit \(\lambda_i\) for each
variable \(i\) in the dataset.

### Variable Correlations
To visually inspect correlation, the simplest approach is to create scatterplots
of each variable against each other.
```{r Q31Scat, fig.width = 9L, fig.height = 8L}
plot(Glass, pch = ".")
```

With 10 variables there are 45 possible pairs, most of which don't show
overwhelming correlations. However, there are some visually apparent
relationships. For example,  `Refractive Index` seems to be highly positively
correlated with `Ca` and somewhat negatively correlated with `Si`.

### Outliers
The scatterplot doesn't show strong evidence of outliers. However, boxplots or
violin plots are often useful in identifying outliers.

```{r Q31bxplt, fig.width = 9L, fig.height = 8L}
boxplot(Glass, pch = 16)
```

There are clearly a few observations for each variable which exceed 1.5 times
the IQR, the majority of which are in Calcium.

The book discusses the spatial sign transform, which is probably not appropriate
for this data, but it can be tried for fun.
```{r Q31aSpS, fig.width = 9L, fig.height = 8L}
SpatSignT <- function(df) {
  dfS <- as.data.frame(scale(df))
  return(dfS / sqrt(rowSums(dfS ^ 2)))
}
```

For this example, the focus will be on the relationship between `Ca` and `Ba`
which shows two "clusters" of outliers.

```{r Q31SpSEx, fig.width = 4L, fig.height = 4.5}
BaCa <- data.frame(Ba = Glass$Ba, Ca = Glass$Ca)
outliers1 <- which(BaCa$Ca > 8 & BaCa$Ba < 2 & BaCa$Ba > 0)
outliers2 <- c(which(BaCa$Ca > 12 & BaCa$Ba > 2.5),
              which(BaCa$Ca < 8 & BaCa$Ba > 1))
plot(BaCa[-c(outliers1, outliers1), ], xlim = c(0, 3.5), ylim = c(5, 17))
points(BaCa[outliers1, ], col = 'blue', pch = 16)
points(BaCa[outliers2, ], col = 'red', pch = 16)
BaCaSS <- SpatSignT(BaCa)
plot(BaCaSS[-c(outliers1, outliers1), ], xlim = c(-1, 1), y = c(-1, 1))
points(BaCaSS[outliers1, ], col = 'blue', pch = 16)
points(BaCaSS[outliers2, ], col = 'red', pch = 16)
```

The spatial sign transformation does indicate that the clusters in normal space
do tend to inhabit different areas of the unit hypersphere. How that can be
leveraged for analysis is somewhat more complicated.

### Summary
This writeup has investigated the `Glass` data from the `mlbench` package. The
native variables visually exhibit a range of distributions from the highly
skewed to the nearly symmetrical, with the standard complement of outliers. A
sequence of transformations were applied to address some of these issues and
the results visualized as well. Further areas for investigation would include
dimensional reduction techniques, especiallt PCA, which should help to reduce
the feature set from 10 to something more manageable which still captures the
predominance of the variability in the data.

# Question KJ 3.2
## Question
The soybean data can also be found at the UC Irvine Machine Learning Repository.
Data were collected to predict disease in 683 soybeans. The 35 predictors are
mostly categorical and include information on the environmental conditions
(e.g., temperature, precipitation) and plant conditions (e.g., left spots,
mold growth). The outcome labels consist of 19 distinct classes. The data can be
loaded via:
```{r QKJ32q}
library(mlbench)
data(Soybean)
## See ?Soybean for details
```

  (a) Investigate the frequency distributions for the categorical predictors.
  Are any of the distributions degenerate in the ways discussed earlier in this
  chapter?
  (b) Roughly 18\% of the data are missing. Are there particular predictors that
  are more likely to be missing? Is the pattern of missing data related to the
  classes?
  (c) Develop a strategy for handling missing data, either by eliminating
  predictors or imputation.

## Answer
### Part (a)
```{r QKJ32barbox}
barbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]] # Thanks for nothing, scale!!
    for (i in seq_along(ndf)) {
        data <- unlist(df[, i])
        plot(data, main = paste("PseudoHistogram of", ndf[i]), xlab = ndf[i])
    }
    par(mfrow = c(1, 1))
}
```

The frequency of the categorical is shown below, both visually and tabularly:
```{r QKJ32a1, fig.width = 9L, fig.height = 10L}
for (i in seq_len(3)) {
  barbox(Soybean[, seq_len(12) * i], c(4, 3))
}
apply(Soybean, 2, function(x) length(unique(x)))
```

From the above, we see that there is no mathematically degenerate distribution,
in that every catgeory has at least one instance of more than one variable type.
Furthermore, the number of unique values being small in respect of the number of
samples is not a significant issue, as most of these are dummy/indicator
variables for the categories. The key test will be the ratio of most frequent to
the second most frequent observation in each category.
```{r QKJ32a2}
nS <- names(Soybean)
soyRatios <- data.frame(Variable = character(36), Ratio = double(36))
for (i in seq_len(dim(Soybean)[[2]])) {
  soyRatios[i, 1] <- nS[i]
  soyRatios[i, 2] <- max(table(Soybean[, i])) /
    max(table(Soybean[, i])[-which.max(table(Soybean[, i]))])
}
soyRatios[order(soyRatios$Ratio, decreasing = TRUE), ]
soyRatios[which(soyRatios$Ratio > 20), ]
```

Here we see three categories which have a most-to-second most ratio of greater
than 20, the value suggested by Kuhn & Johnson. These would be categories for
consideration of removal or some other transform due to their imbalance.

### Part (b)
There clearly is a correlation between the Class and the "missingness" of data.
```{r QKJ32b1, fig.width = 9L, fig.height = 10L}
numClass <- length(levels(Soybean$Class))
nameClass <- levels(Soybean$Class)
NAList <- data.frame(Class = character(0), NACount = integer(0))
par(mfrow = c(5, 4))
for (i in seq_len(numClass)) {
  NAList[i, ] <- c(nameClass[i],
                   sum(is.na(Soybean[Soybean$Class == nameClass[i], ])))
  barplot(apply(Soybean[Soybean$Class == nameClass[i], ], 2L,
                function(x) sum(is.na(x)) / length(x)),
          xlab = nameClass[i])
}
par(mfrow = c(1, 1))
NAList
```

It's clear that classes 1, 9, 10, 14, and 16 have significant missing values
where the other 14 classes have none. Note that the values are not the counts
of records with missing values, but are indications of many missing values per
record. As an aside, this kind of analysis would be done more easily using a
package like `data.table` where operations could be done by group.
```{r QKJ32b2}
library(data.table)
Sb <- as.data.table(Soybean)
Sb[, .(NACount = sum(is.na(.SD))), keyby = Class]
```

Which gives us the same information as does base R but with less typing.

### Part (c)
Using `data.table` we can more easily get a better picture of the number of
missing values and consider approaches to handling them. First, a list of the
toal number of records for classes with missing data and then the distribution
of the missing data by variable.
missing data.
```{r QKJ32c1}
SbM <- Sb[Class %in% levels(Sb$Class)[c(1, 9, 10, 14, 16)]]
SbM[, .N, keyby = Class]
SbM[, lapply(.SD, function(x) sum(is.na(x))), keyby = Class]
```

When a variable is missing, it tends to be missing for the entirety of its
class, as shown below:

```{r QKJ32c2}
SbM[, lapply(.SD, function(x) sum(is.na(x)) / length(x)), keyby = Class]
```

Lastly, of the 35 explanatory variables (not counting class itself), these
missing classes tend to be missing a lot of them

```{r QKJ32c3}
SbM[, lapply(.SD, function(x) sum(is.na(x))), keyby = Class
    ][, .(VarMissing = sum(.SD > 0) / length(.SD)), keyby = Class
      ][order(-VarMissing)]
```

All the classes with missing data are missing more than 50\% of their data with
the exception of the `diaporthe-pod-&-stem-blight` class. As such, unless we
were using tree-based methods, I would suggest that the data observations of the
first four classes which are missing be removed, as I would be loathe to impute
more than half of the data points for that class. Especially if there will
eventually be training-validation-testing splits.

As for `diaporthe-pod-&-stem-blight`, on the one hand, there are only 15
observations in the entire data set. On the other, it has around 2/3 of the
features. A first strategy would be to see just how predictive the missing
features are in the remaining data; perhaps they aren't of much value.
Otherwise, I would have to weigh the time constraints of the project against the
time needed to impute the 15 sets of 11 and 2 more sets of 6 missing data
elements, and how much predictive power they will end up demonstrating.

Lastly, using a dimensional reduction approach, such as PCA, on the non-missing
data may be informative in that the variability reduction of a missing variable
may be correlated enough with a non-missing variable to allow a focused model to
be of value.

# Question HA 7.1

Consider the `pigs` series -- the number of pigs slaughtered in Victoria each
month.

## Answers

### Part a

Use the `ses()` function in R to find the optimal values of alpha and l, and
generate forecastss for the next four months.

```{r Q71a_1}
pigs_output = ses(pigs, h = 4)

summary(pigs_output$model)
```

After running the `ses()` function on the `pigs` data, we can see that the
optimal value for alpha is 0.2971 and the optimal value for l is 77260.0561.

The forecast for the next four months is shown below:

```{r Q71a_2}
pigs_output
```

### Part b

Compute a 95% prediction interval for the first forecast using predicted value
+/- 1.96 * s, where s is the standard deviation of the residuals. Compare your
interval with the interval produced by R.

```{r Q71b_1}
pigs_sd_resid = sd(pigs_output$residuals)

pigs_lb = pigs_output$mean - (1.96 * pigs_sd_resid)
pigs_ub = pigs_output$mean + (1.96 * pigs_sd_resid)

pigs_lb
pigs_ub
```

The interval that I created does not get wider as time increases, whereas the
interval produced by R gradually gets wider as time increases. Using the basic
confidence interval calculation with a simple point forecast that does not
adjust for increasing uncertainty as time increases creates this static
rectangle forecast. The output from R appears to adjust for this uncertainty.

# Question HA 7.3

Modify your function from the previous exercise t o return the sum of squared
errors rather than the forecast of the next observation. Then use the `optim()`
function to find the optimal values of alpha and l. Do you get the same values
as the `ses()` function?

## Answers

The function below calculates the SSE from a simple exponential smoothing model
after inputing a specific alpha, level, and time series. The function takes the
component form of the smoothing equation of the SES model and uses it to
calculate the SSE.

```{r Q73_1}
my_ses = function(y,pars = c(alpha,level)){
  
  alpha = pars[1]
  level = pars[2]
  
  l_t = vector(mode = "numeric",length = length(y) + 1)
  l_t[1] = level
  
  for (item in seq(2, length(y) + 1)) {
    
      l_t[item] = (alpha * y[item - 1]) + ((1 - alpha) * l_t[item - 1])
    
  }
  
  
  SSE = sum((y - l_t[1:length(l_t) - 1]) ^ 2)
  
  
  return(SSE)
}


my_ses(pigs,c(0.2971,77260.0561))
```

The code below finds the optimal values of alpha and level using the `optim()`
function. We can see that the values for alpha and level are nearly identical to
the same parameters calculated using the `ses()` function.

```{r Q73_2}
optimal_values = optim(par = c(0.5, pigs[1]), y = pigs, fn = my_ses)

optimal_values$par[1]
optimal_values$par[2]
```
# Question HA 8.1
## Question
Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 
1,000 random numbers.

### Part A
Explain the differences among these figures. Do they all indicate that the data 
are white noise?

![Fig 8.31 ACF for a white noise series of 36 numbers. Middle: ACF for a white noise
series of 360 numbers. Right: ACF for a white noise series of 1,000 numbers.](wnacfplus-1.png)

**Ans* :- Data in all the 3 figures resemble white noise.The difference between all 
these figures is the critical values for each dataset, as the dataset is small the
critical values are larger and as the dataset increases the critical values are smaller.

### Part B
Why are the critical values at different distances from the mean of zero? Why are the 
autocorrelations different in each figure when they each refer to white noise?

**Ans* :- Critical values are at different distances from the mean of zero , because
the formula to calculate critical values is $$\pm 2/\sqrt { N }$$ where N=length of TS.
Hence as the length of TS increases or decreases the critical values come closer or move
further away from mean of zero.                          
The autocorrelations are different in each figure because of the different length of each 
timeseries.


# Question HA 8.2
## Question

A classic example of a non-stationary series is the daily closing IBM stock
price series (data set `ibmclose`). Use R to plot the daily closing prices for
IBM stock and the ACF and PACF. Explain how each plot shows that the series is
non-stationary and should be differenced.

```{r 82Q1}
ibm <- ibmclose
autoplot(ibm)
```

The timeseries appears to have some trend component, which would make the data
non-stationary. Taking either the first or second difference should remove this
trend and make the data stationary.

```{r 82Q2}
ggAcf(ibm)
```

Here we see an autocorrelation plot that agrees with our above assumptions; the
decaying values are a clear sign of trend, and every possible lag has a
significant autocorrelation, indicating non-stationary data.

```{r 82Q3}
ggPacf(ibm)
```

All of the autocorrelations are explainable by the first lag, the other
correlations being significant are a product of this. Again, this correlation
indicates non-stationary data which can likely be fixed by a first difference.

# Question HA 8.6
## Question
Use R to simulate and plot some data from simple ARIMA models.

### Part A
Use the following R code to generate data from an AR(1) model with
\(\phi_1 = 0.6\) and \(\sigma^2 = 1\). The process starts with \(y_1 = 0\).

```{r 86a1}
y <- ts(numeric(100))
e <- rnorm(100)
for (i in 2:100)
  y[i] <- 0.6*y[i - 1] + e[i]
```

### Part B
Produce a time plot for the series. How does the plot change as you change
\(\phi_1\)?

```{r 86b1}
for (i in c(-0.8, -0.6, -0.4, -0.2, 0.2, 0.4, 0.6, 0.8)) {
    y <- ts(numeric(100))
    e <- rnorm(100)
    for (j in 2:100) {
        y[j] <- i * y[j - 1] + e[j]
    }
    plot1 <- autoplot(y) + xlab(i)
    print(plot1)
}
```

The value chosen for \(\phi\) affects how volatile the series is. Negative
values close to -1 force the series to oscillate across the mean nearly every
observation, whereas positive values for \(\phi\) close to 1 show much less
volatility and are smoother overall.

### Part C
Write your own code to generate data from an MA(1) model with \(\theta_1 = 0.6\)
and \(\sigma^2 = 1\).

```{r 86c1}
set.seed(42)
for (theta in c(-1, -0.6, -0.3, 0, 0.3, 0.6, 1)) {
  y <- ts(numeric(100))
  e <- rnorm(100)
  for (i in 2:100) {
    y[i] <- theta * e[i - 1] + e[i]
  }
  print(autoplot(y) + xlab(theta))
}
```

### Part D
Produce a time plot for the series. How does the plot change as you change
\(\theta_1\)?

As \(\theta\) approaches 1, the time series appears to resemble a random walk.
However, as \(\theta\) approaches -1, the time series oscillates rapidly across
the mean with almost every step in the series.

### Part E
Generate data from an ARMA(1,1) model with \(\phi_1 = 0.6, \theta_1 = 0.6\), and
\(\sigma^2 = 1\).

```{r 86e1}
phi <- 0.6
theta <- 0.6
y_arma <- ts(numeric(100))
e <- rnorm(100)
for (i in 2:100) {
  y_arma[i] <- phi * y_arma[i - 1] + theta * e[i - 1] + e[i]
}
```

### Part F
Generate data from an AR(2) model with \(\phi_1 = −0.8\), \(\phi_1 = 0.3\),
and \(\sigma^2 = 1\). (Note that these parameters will give a non-stationary
series.)

```{r 86f1}
phi1 <- -0.8
phi2 <- 0.3
y_ar <- ts(numeric(100))
e <- rnorm(100)
y_ar[2] <- y_ar[1] * phi1 + e[2]
for (i in 3:100) {
  y_ar[i] <- phi1 * y_ar[i - 1] + phi2 * y_ar[i - 2] + e[i]
}
```

### Part G
Graph the latter two series and compare them.

```{r 86g1}
autoplot(y_arma) + xlab('ARMA Model')
autoplot(y_ar) + xlab('AR 2 Model')
```

The time series are extremely different as the AR 2 model is divergent. This is
because The \(\phi_1\) parameter makes the series oscillate, but the \(\phi_2\)
parameter has a positive sign and increases the size of the oscillations. Since
\(\phi_1\) and \(\phi_2\) add to greater than 1, the oscillations diverge rather
than converge. This leads to a time series with ever increasing amplitudes
rather than the stationary time series we observe with the ARMA model.

# Question HA 8.8
## Question

Consider `austa`, the total international visitors to Australia (in millions)
for the period 1980--2015.

### Part A
Use `auto.arima()` to find an appropriate ARIMA model. What model was selected.
Check that the residuals look like white noise. Plot forecasts for the next 10
periods.

```{r 88a1}
arima_model <- auto.arima(austa)
arima_model
```

Auto Arima has selected an ARIMA(0, 1, 1) with drift.

```{r 88a2}
checkresiduals(arima_model)
autoplot(forecast(arima_model, h = 10))
```

The residuals appear to be vaguely normally distributed and pass the Ljung Box
test with a p-value of .8.

### Part B
Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these to
part a. Remove the MA term and plot again.

```{r 88b1}
arima_no_drift <- Arima(austa, c(0, 1, 1), include.drift = F)
autoplot(forecast(arima_no_drift), h = 10)

arima_no_ma <- Arima(austa, c(0, 1, 0), include.drift = F)

autoplot(forecast(arima_no_ma), h = 10)
```

Both the forecasts without drift fail to capture the trending nature of the
previous data, providing constant forecasts rather than upward trending
forecasts. 

### Part C
Plot forecasts from an ARIMA(2,1,3) model with drift. Remove the constant and
see what happens.

```{r 88c1}
fit <- Arima(austa, c(2, 1, 3), include.drift = T)
autoplot(forecast(fit, h = 10))

fit_no_c <- Arima(austa, c(2,1,3), include.drift = T, include.constant = F,
                  method = 'ML')
autoplot(forecast(fit_no_c, h = 10))
```

The two models give somewhat similar forecasts, but the prediction interval for
the model without a constant is much larger than the prediction interval for the
model with a constant. The model with a constant also appears to be slightly
damped compared to the model without a constant.

### Part D
Plot forecasts from an ARIMA(0,0,1) model with a constant. Remove the MA term
and plot again.

```{r 88d1}
fit <- Arima(austa, c(0, 0, 1), include.constant = T)
autoplot(forecast(fit, h = 10))

fit_no_ma <- Arima(austa, c(0, 0, 0), include.constant = T)
autoplot(forecast(fit_no_ma, h = 10))
```

The model with the MA term regresses to the mean after 1 timestep, the length of
the MA term. The model with no MA term simply forecasts the mean value for all
future timesteps. Neither of these models seem appropriate as they fail to
capture the trending nature of the data.

### Part E
Plot forecasts from an ARIMA(0,2,1) model with no constant.

```{r 88e1}
fit <- Arima(austa, c(0, 2, 1), include.constant = F)
autoplot(forecast(fit, h = 10))
```

This model appears to be a reasonable fit to the data, capturing the upward
trend and without a huge prediction interval.
